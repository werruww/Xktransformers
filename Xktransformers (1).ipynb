{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKbDAPyTRH2V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n4-zmxEATht7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kvcache-ai/ktransformers.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-h7kIJ_Thxq",
        "outputId": "3893e188-dcdc-4518-f288-44b3bfd0d8c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ktransformers'...\n",
            "remote: Enumerating objects: 5717, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 5717 (delta 35), reused 27 (delta 27), pack-reused 5660 (from 3)\u001b[K\n",
            "Receiving objects: 100% (5717/5717), 15.34 MiB | 12.23 MiB/s, done.\n",
            "Resolving deltas: 100% (3362/3362), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ktransformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iQopzgDTj3i",
        "outputId": "f6b899b8-8e1b-4bca-a04e-414a46f0b756"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ktransformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements-local_chat.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUIQEO3JTsmy",
        "outputId": "2b3790f1-c242-496e-b282-0d1d1f1a8020"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignoring cpufeature: markers 'sys_platform == \"win32\" or sys_platform == \"Windows\"' don't match your environment\n",
            "Collecting fire (from -r requirements-local_chat.txt (line 1))\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers==4.51.3 (from -r requirements-local_chat.txt (line 2))\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 4)) (2.6.0+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 7)) (5.29.5)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: blobfile in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 9)) (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (0.33.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->-r requirements-local_chat.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.3.0->-r requirements-local_chat.txt (line 4))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements-local_chat.txt (line 9)) (3.23.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements-local_chat.txt (line 9)) (2.4.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements-local_chat.txt (line 9)) (5.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.0.2)\n",
            "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=258234ffe576263b1d97597e1a42ae6d4a366aee63b461365a2079e29a30ddd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.4\n",
            "    Uninstalling transformers-4.52.4:\n",
            "      Successfully uninstalled transformers-4.52.4\n",
            "Successfully installed fire-0.7.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 transformers-4.51.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x /content/ktransformers/install.sh\n",
        "!/content/ktransformers/install.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTWLZQ09TvJz",
        "outputId": "0db92083-6978-4aad-c1ea-cc9b170e6fe9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected backend: cuda\n",
            "Installing python dependencies from requirements.txt\n",
            "Ignoring cpufeature: markers 'sys_platform == \"win32\" or sys_platform == \"Windows\"' don't match your environment\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 2)) (4.51.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 4)) (2.6.0+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 7)) (5.29.5)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: blobfile in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 9)) (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (0.33.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->-r requirements-local_chat.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements-local_chat.txt (line 9)) (3.23.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements-local_chat.txt (line 9)) (2.4.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements-local_chat.txt (line 9)) (5.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 2)) (4.51.3)\n",
            "Requirement already satisfied: fastapi>=0.111.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 3)) (0.115.12)\n",
            "Requirement already satisfied: langchain>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 4)) (0.3.25)\n",
            "Collecting blessed>=1.20.0 (from -r ktransformers/server/requirements.txt (line 5))\n",
            "  Downloading blessed-1.21.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 6)) (1.7.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 8)) (1.86.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 9)) (75.2.0)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 10)) (1.2.2.post1)\n",
            "Collecting ninja (from -r ktransformers/server/requirements.txt (line 11))\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 12)) (0.45.1)\n",
            "Collecting colorlog (from -r ktransformers/server/requirements.txt (line 13))\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 14)) (0.7.0)\n",
            "Collecting zmq (from -r ktransformers/server/requirements.txt (line 15))\n",
            "  Downloading zmq-0.0.0.zip (2.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 16)) (5.9.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (2.11.7)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.3.45)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (2.0.41)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.20.0->-r ktransformers/server/requirements.txt (line 5)) (0.2.13)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build->-r ktransformers/server/requirements.txt (line 10)) (1.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->-r ktransformers/server/requirements.txt (line 14)) (3.1.0)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from zmq->-r ktransformers/server/requirements.txt (line 15)) (24.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai->-r ktransformers/server/requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r ktransformers/server/requirements.txt (line 8)) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r ktransformers/server/requirements.txt (line 8)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r ktransformers/server/requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (3.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (3.0.0)\n",
            "Downloading blessed-1.21.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: zmq\n",
            "  Building wheel for zmq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zmq: filename=zmq-0.0.0-py3-none-any.whl size=1265 sha256=8e92c0aca4dfdd89139c12c3a24a84d71dec299a19c01a3073fd210b94a7311a\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/9e/c7/d497825227491fa00cca08b88ae37f9bcc14809233db76342c\n",
            "Successfully built zmq\n",
            "Installing collected packages: zmq, ninja, colorlog, blessed\n",
            "Successfully installed blessed-1.21.0 colorlog-6.9.0 ninja-1.11.1.4 zmq-0.0.0\n",
            "Installing ktransformers\n",
            "Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
            "Processing /content/ktransformers\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "  /usr/local/lib/python3.11/dist-packages/setuptools/config/_apply_pyprojecttoml.py:78: SetuptoolsWarning: `install_requires` overwritten in `pyproject.toml` (dependencies)\n",
            "    corresp(dist, value, root_dir)\n",
            "  cpuinfer_ext /content/ktransformers/csrc/ktransformers_ext\n",
            "  Using native cpu instruct\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-w4xevtlh/ktransformers.egg-info\n",
            "  writing /tmp/pip-modern-metadata-w4xevtlh/ktransformers.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-w4xevtlh/ktransformers.egg-info/dependency_links.txt\n",
            "  writing entry points to /tmp/pip-modern-metadata-w4xevtlh/ktransformers.egg-info/entry_points.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-w4xevtlh/ktransformers.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-w4xevtlh/ktransformers.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-w4xevtlh/ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-w4xevtlh/ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  warning: no directories found matching 'local_chat.py'\n",
            "  no previously-included directories found matching 'ktransformers/logs'\n",
            "  no previously-included directories found matching 'ktransformers.egg-info'\n",
            "  no previously-included directories found matching 'third_party/llama.cpp/models'\n",
            "  warning: no directories found matching 'ktransformers/website/dist'\n",
            "  warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
            "  warning: no files found matching 'KTransformersOps.*.so'\n",
            "  warning: no files found matching 'cpuinfer_ext.*.so'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-w4xevtlh/ktransformers.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-w4xevtlh/ktransformers-0.3.1+cu125torch26fancy.dist-info'\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (4.51.3)\n",
            "Requirement already satisfied: fastapi>=0.111.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.115.12)\n",
            "Requirement already satisfied: uvicorn>=0.30.1 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.34.3)\n",
            "Requirement already satisfied: langchain>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.3.25)\n",
            "Requirement already satisfied: blessed>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (1.21.0)\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (1.7.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (75.2.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (1.11.1.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.45.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (6.9.0)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (1.2.2.post1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.7.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.31.0->ktransformers==0.3.1+cu125torch26fancy) (5.9.5)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.20.0->ktransformers==0.3.1+cu125torch26fancy) (0.2.13)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (4.14.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (0.3.45)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (2.0.41)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers==0.3.1+cu125torch26fancy) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers==0.3.1+cu125torch26fancy) (0.16.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build->ktransformers==0.3.1+cu125torch26fancy) (1.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->ktransformers==0.3.1+cu125torch26fancy) (3.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (1.1.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (3.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (1.0.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (3.0.0)\n",
            "Building wheels for collected packages: ktransformers\n",
            "  Running command Building wheel for ktransformers (pyproject.toml)\n",
            "  /usr/local/lib/python3.11/dist-packages/setuptools/config/_apply_pyprojecttoml.py:78: SetuptoolsWarning: `install_requires` overwritten in `pyproject.toml` (dependencies)\n",
            "    corresp(dist, value, root_dir)\n",
            "  cpuinfer_ext /content/ktransformers/csrc/ktransformers_ext\n",
            "  Using native cpu instruct\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  copying ktransformers/local_chat.py -> build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  copying ktransformers/local_chat_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  copying ktransformers/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/triton_fp8gemm_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/mmlu_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/mmlu_pro_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/dequant_gpu_t.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/score.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/test_client.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/test_pytorch_q8.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/mmlu_test_multi.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/dequant_gpu.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/test_speed.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/function_call_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/exceptions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/args.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/main.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/gate.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/RoPE.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/flashinfer_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/base_operator.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/mlp.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/experts.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/linear.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/dynamic_attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/triton_attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/triton_attention_prefill.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/cpuinfer.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/flashinfer_batch_prefill_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/layernorm.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/balance_serve_attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/models.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/custom_gguf.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/modeling_rope_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/cuda_graph_runner.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/vendors.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/weight_loader.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/custom_loader.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/textstream.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_deepseek_v2.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_deepseek_v3.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_deepseek_v3.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_qwen3_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_qwen3_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_qwen2_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_qwen3_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_deepseek.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_cache.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_llama.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_llama.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_qwen2_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_mixtral.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_deepseek_v3.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_deepseek.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_qwen2_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize\n",
            "  copying ktransformers/optimize/optimize.py -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  copying ktransformers/tests/AIME_2024/evaluation.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  copying ktransformers/tests/AIME_2024/eval_api.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  copying ktransformers/tests/AIME_2024/prompts.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  copying ktransformers/tests/humaneval/evaluation.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  copying ktransformers/tests/humaneval/eval_api.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  copying ktransformers/tests/humaneval/prompts.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/base.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/context_manager.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/args.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  copying ktransformers/server/schemas/base.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  copying ktransformers/server/schemas/conversation.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  copying ktransformers/server/schemas/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/crud\n",
            "  copying ktransformers/server/crud/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api\n",
            "  copying ktransformers/server/api/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/sql_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/create_interface.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/multi_timer.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/models\n",
            "  copying ktransformers/server/models/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  copying ktransformers/server/config/config.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  copying ktransformers/server/config/log.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  copying ktransformers/server/config/singleton.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve\n",
            "  copying ktransformers/server/balance_serve/settings.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve\n",
            "  copying ktransformers/server/balance_serve/sched_rpc.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/exllamav2.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/ktransformers.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/transformers.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/balance_serve.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy\n",
            "  copying ktransformers/server/schemas/legacy/completions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy\n",
            "  copying ktransformers/server/schemas/legacy/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/endpoints\n",
            "  copying ktransformers/server/schemas/endpoints/chat.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/endpoints\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/streaming.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/tool.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai\n",
            "  copying ktransformers/server/api/openai/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web\n",
            "  copying ktransformers/server/api/web/system.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web\n",
            "  copying ktransformers/server/api/web/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama\n",
            "  copying ktransformers/server/api/ollama/completions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama\n",
            "  copying ktransformers/server/api/ollama/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy\n",
            "  copying ktransformers/server/api/openai/legacy/completions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy\n",
            "  copying ktransformers/server/api/openai/legacy/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints\n",
            "  copying ktransformers/server/api/openai/endpoints/chat.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints\n",
            "  copying ktransformers/server/api/openai/endpoints/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/run_steps.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/model_runner.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/forward_batch.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/config.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/query_manager.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/sampler.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/custom_all_reduce_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/pynccl_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/pynccl.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/communication_op.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/cuda_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/custom_all_reduce.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/parallel_state.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/orchestrator.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/min_new_tokens.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/presence_penalty.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/repetition_penalty.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/frequency_penalty.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/triton\n",
            "  copying ktransformers/ktransformers_ext/triton/fp8gemm.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/triton\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_perms.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_24_perms.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/quant_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/format_24.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  running egg_info\n",
            "  creating ktransformers.egg-info\n",
            "  writing ktransformers.egg-info/PKG-INFO\n",
            "  writing dependency_links to ktransformers.egg-info/dependency_links.txt\n",
            "  writing entry points to ktransformers.egg-info/entry_points.txt\n",
            "  writing requirements to ktransformers.egg-info/requires.txt\n",
            "  writing top-level names to ktransformers.egg-info/top_level.txt\n",
            "  writing manifest file 'ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest file 'ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  warning: no directories found matching 'local_chat.py'\n",
            "  no previously-included directories found matching 'ktransformers/logs'\n",
            "  no previously-included directories found matching 'third_party/llama.cpp/models'\n",
            "  warning: no directories found matching 'ktransformers/website/dist'\n",
            "  warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
            "  warning: no files found matching 'KTransformersOps.*.so'\n",
            "  warning: no files found matching 'cpuinfer_ext.*.so'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'ktransformers.egg-info/SOURCES.txt'\n",
            "  copying ktransformers/tests/.gitignore -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/server/requirements.txt -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/configs\n",
            "  copying ktransformers/configs/config.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/configs\n",
            "  copying ktransformers/configs/log_config.ini -> build/lib.linux-x86_64-cpython-311/ktransformers/configs\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu-4.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-4.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-8.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-fp8-linear-ggml-experts.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-marlin.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Internlm2_5-7b-Chat-1m.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Mixtral.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Moonlight-16B-A3B-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Moonlight-16B-A3B.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-serve-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen3Moe-serve-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen3Moe-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  copying ktransformers/optimize/optimize_rules/xpu/DeepSeek-V2-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  copying ktransformers/optimize/optimize_rules/xpu/DeepSeek-V3-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  copying ktransformers/optimize/optimize_rules/xpu/Qwen3Moe-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/rocm\n",
            "  copying ktransformers/optimize/optimize_rules/rocm/DeepSeek-V3-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/rocm\n",
            "  running build_ext\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n",
            "    warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n",
            "    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  -- Compiler and CPU support AVX512F (tested by compiling a program)\n",
            "  -- Compiler does NOT support AMX\n",
            "  CMake Error at CMakeLists.txt:275 (add_subdirectory):\n",
            "    The source directory\n",
            "\n",
            "      /content/ktransformers/third_party/pybind11\n",
            "\n",
            "    does not contain a CMakeLists.txt file.\n",
            "\n",
            "\n",
            "  CMake Error at CMakeLists.txt:276 (add_subdirectory):\n",
            "    The source directory\n",
            "\n",
            "      /content/ktransformers/third_party/llama.cpp\n",
            "\n",
            "    does not contain a CMakeLists.txt file.\n",
            "\n",
            "\n",
            "  CMake Warning (dev) at CMakeLists.txt:310 (find_package):\n",
            "    Policy CMP0146 is not set: The FindCUDA module is removed.  Run \"cmake\n",
            "    --help-policy CMP0146\" for policy details.  Use the cmake_policy command to\n",
            "    set the policy and suppress this warning.\n",
            "\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  -- Found CUDA: /usr/local/cuda (found version \"12.5\")\n",
            "  -- Looking for a CUDA compiler\n",
            "  -- Looking for a CUDA compiler - /usr/local/cuda/bin/nvcc\n",
            "  -- CUDA detected\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.5.82\")\n",
            "  -- enabling CUDA\n",
            "  -- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- CMAKE_CXX_FLAGS:  -O3 -ffast-math -fopenmp\n",
            "  -- ARCH_FLAGS: -mfma;-mavx;-mavx2;-march=native\n",
            "  CMake Error at CMakeLists.txt:354 (pybind11_add_module):\n",
            "    Unknown CMake command \"pybind11_add_module\".\n",
            "\n",
            "\n",
            "  -- Configuring incomplete, errors occurred!\n",
            "  CMake args: ['-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/content/ktransformers/build/lib.linux-x86_64-cpython-311/', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DCMAKE_BUILD_TYPE=Release', '-DKTRANSFORMERS_USE_CUDA=ON', '-D_GLIBCXX_USE_CXX11_ABI=0']\n",
            "  CMake args: ['-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/content/ktransformers/build/lib.linux-x86_64-cpython-311/', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DCMAKE_BUILD_TYPE=Release', '-DKTRANSFORMERS_USE_CUDA=ON', '-D_GLIBCXX_USE_CXX11_ABI=0', '-DLLAMA_NATIVE=ON', '-DEXAMPLE_VERSION_INFO=0.3.1+cu125torch26fancy']\n",
            "  build_temp: /content/ktransformers/csrc/ktransformers_ext/build\n",
            "  Traceback (most recent call last):\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
            "      main()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
            "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 251, in build_wheel\n",
            "      return _build_backend().build_wheel(wheel_directory, config_settings,\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/build_meta.py\", line 434, in build_wheel\n",
            "      return _build(['bdist_wheel', '--dist-info-dir', metadata_directory])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/build_meta.py\", line 422, in _build\n",
            "      return self._build_with_temp_dir(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/build_meta.py\", line 403, in _build_with_temp_dir\n",
            "      self.run_setup()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/build_meta.py\", line 318, in run_setup\n",
            "      exec(code, locals())\n",
            "    File \"<string>\", line 668, in <module>\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/__init__.py\", line 117, in setup\n",
            "      return distutils.core.setup(**attrs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/core.py\", line 183, in setup\n",
            "      return run_commands(dist)\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/core.py\", line 199, in run_commands\n",
            "      dist.run_commands()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py\", line 954, in run_commands\n",
            "      self.run_command(cmd)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/dist.py\", line 991, in run_command\n",
            "      super().run_command(command)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
            "      cmd_obj.run()\n",
            "    File \"<string>\", line 263, in run\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/command/bdist_wheel.py\", line 407, in run\n",
            "      self.run_command(\"build\")\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "      self.distribution.run_command(command)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/dist.py\", line 991, in run_command\n",
            "      super().run_command(command)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
            "      cmd_obj.run()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/command/build.py\", line 135, in run\n",
            "      self.run_command(cmd_name)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "      self.distribution.run_command(command)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/dist.py\", line 991, in run_command\n",
            "      super().run_command(command)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
            "      cmd_obj.run()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/command/build_ext.py\", line 98, in run\n",
            "      _build_ext.run(self)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\n",
            "      self.build_extensions()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py\", line 900, in build_extensions\n",
            "      build_ext.build_extensions(self)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/command/build_ext.py\", line 476, in build_extensions\n",
            "      self._build_extensions_serial()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/command/build_ext.py\", line 502, in _build_extensions_serial\n",
            "      self.build_extension(ext)\n",
            "    File \"<string>\", line 591, in build_extension\n",
            "    File \"<string>\", line 370, in run_command_with_live_tail\n",
            "    File \"/usr/lib/python3.11/subprocess.py\", line 571, in run\n",
            "      raise CalledProcessError(retcode, process.args,\n",
            "  subprocess.CalledProcessError: Command '['cmake', '/content/ktransformers/csrc/ktransformers_ext', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/content/ktransformers/build/lib.linux-x86_64-cpython-311/', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DCMAKE_BUILD_TYPE=Release', '-DKTRANSFORMERS_USE_CUDA=ON', '-D_GLIBCXX_USE_CXX11_ABI=0', '-DLLAMA_NATIVE=ON', '-DEXAMPLE_VERSION_INFO=0.3.1+cu125torch26fancy']' returned non-zero exit status 1.\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for ktransformers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  \u001b[1;35mfull command\u001b[0m: \u001b[34m/usr/bin/python3 /usr/local/lib/python3.11/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_wheel /tmp/tmpc5gl81o2\u001b[0m\n",
            "  \u001b[1;35mcwd\u001b[0m: /content/ktransformers\n",
            "  Building wheel for ktransformers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for ktransformers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build ktransformers\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (ktransformers)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eBkwkLAZUSuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33e4b1ef",
        "outputId": "35a65763-ed37-4f0e-d8f7-176f1be2c6aa"
      },
      "source": [
        "!apt-get update && apt-get install -y build-essential"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,798 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,051 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,994 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,747 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,561 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.2 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,340 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,253 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,703 kB]\n",
            "Fetched 27.9 MB in 4s (7,734 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x /content/ktransformers/install.sh\n",
        "!/content/ktransformers/install.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPo8cBEKU2SL",
        "outputId": "c8d5dcc7-6e36-4dbe-b147-24fe7316705e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected backend: cuda\n",
            "Installing python dependencies from requirements.txt\n",
            "Ignoring cpufeature: markers 'sys_platform == \"win32\" or sys_platform == \"Windows\"' don't match your environment\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 2)) (4.51.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 4)) (2.6.0+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 7)) (5.29.5)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: blobfile in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 9)) (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (0.33.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->-r requirements-local_chat.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements-local_chat.txt (line 9)) (3.23.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements-local_chat.txt (line 9)) (2.4.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements-local_chat.txt (line 9)) (5.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 2)) (4.51.3)\n",
            "Requirement already satisfied: fastapi>=0.111.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 3)) (0.115.12)\n",
            "Requirement already satisfied: langchain>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 4)) (0.3.25)\n",
            "Requirement already satisfied: blessed>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 5)) (1.21.0)\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 6)) (1.7.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 8)) (1.86.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 9)) (75.2.0)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 10)) (1.2.2.post1)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 11)) (1.11.1.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 12)) (0.45.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 13)) (6.9.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 14)) (0.7.0)\n",
            "Requirement already satisfied: zmq in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 15)) (0.0.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 16)) (5.9.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (2.11.7)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.3.45)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (2.0.41)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.20.0->-r ktransformers/server/requirements.txt (line 5)) (0.2.13)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build->-r ktransformers/server/requirements.txt (line 10)) (1.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->-r ktransformers/server/requirements.txt (line 14)) (3.1.0)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from zmq->-r ktransformers/server/requirements.txt (line 15)) (24.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai->-r ktransformers/server/requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r ktransformers/server/requirements.txt (line 8)) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r ktransformers/server/requirements.txt (line 8)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r ktransformers/server/requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (3.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (3.0.0)\n",
            "Installing ktransformers\n",
            "Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
            "Processing /content/ktransformers\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "  /usr/local/lib/python3.11/dist-packages/setuptools/config/_apply_pyprojecttoml.py:78: SetuptoolsWarning: `install_requires` overwritten in `pyproject.toml` (dependencies)\n",
            "    corresp(dist, value, root_dir)\n",
            "  cpuinfer_ext /content/ktransformers/csrc/ktransformers_ext\n",
            "  Using native cpu instruct\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-_urk7ljy/ktransformers.egg-info\n",
            "  writing /tmp/pip-modern-metadata-_urk7ljy/ktransformers.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-_urk7ljy/ktransformers.egg-info/dependency_links.txt\n",
            "  writing entry points to /tmp/pip-modern-metadata-_urk7ljy/ktransformers.egg-info/entry_points.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-_urk7ljy/ktransformers.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-_urk7ljy/ktransformers.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-_urk7ljy/ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-_urk7ljy/ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  warning: no directories found matching 'local_chat.py'\n",
            "  no previously-included directories found matching 'ktransformers/logs'\n",
            "  no previously-included directories found matching 'ktransformers.egg-info'\n",
            "  no previously-included directories found matching 'third_party/llama.cpp/models'\n",
            "  warning: no directories found matching 'ktransformers/website/dist'\n",
            "  warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
            "  warning: no files found matching 'KTransformersOps.*.so'\n",
            "  warning: no files found matching 'cpuinfer_ext.*.so'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-_urk7ljy/ktransformers.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-_urk7ljy/ktransformers-0.3.1+cu125torch26fancy.dist-info'\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (4.51.3)\n",
            "Requirement already satisfied: fastapi>=0.111.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.115.12)\n",
            "Requirement already satisfied: uvicorn>=0.30.1 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.34.3)\n",
            "Requirement already satisfied: langchain>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.3.25)\n",
            "Requirement already satisfied: blessed>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (1.21.0)\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (1.7.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (75.2.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (1.11.1.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.45.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (6.9.0)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (1.2.2.post1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.7.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.31.0->ktransformers==0.3.1+cu125torch26fancy) (5.9.5)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.20.0->ktransformers==0.3.1+cu125torch26fancy) (0.2.13)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (4.14.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (0.3.45)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (2.0.41)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers==0.3.1+cu125torch26fancy) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers==0.3.1+cu125torch26fancy) (0.16.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build->ktransformers==0.3.1+cu125torch26fancy) (1.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->ktransformers==0.3.1+cu125torch26fancy) (3.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (1.1.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (3.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (1.0.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (3.0.0)\n",
            "Building wheels for collected packages: ktransformers\n",
            "  Running command Building wheel for ktransformers (pyproject.toml)\n",
            "  /usr/local/lib/python3.11/dist-packages/setuptools/config/_apply_pyprojecttoml.py:78: SetuptoolsWarning: `install_requires` overwritten in `pyproject.toml` (dependencies)\n",
            "    corresp(dist, value, root_dir)\n",
            "  cpuinfer_ext /content/ktransformers/csrc/ktransformers_ext\n",
            "  Using native cpu instruct\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  copying ktransformers/local_chat.py -> build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  copying ktransformers/local_chat_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  copying ktransformers/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/triton_fp8gemm_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/mmlu_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/mmlu_pro_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/dequant_gpu_t.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/score.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/test_client.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/test_pytorch_q8.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/mmlu_test_multi.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/dequant_gpu.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/test_speed.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/function_call_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/exceptions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/args.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/main.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/gate.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/RoPE.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/flashinfer_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/base_operator.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/mlp.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/experts.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/linear.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/dynamic_attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/triton_attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/triton_attention_prefill.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/cpuinfer.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/flashinfer_batch_prefill_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/layernorm.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/balance_serve_attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/models.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/custom_gguf.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/modeling_rope_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/cuda_graph_runner.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/vendors.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/weight_loader.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/custom_loader.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/textstream.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_deepseek_v2.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_deepseek_v3.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_deepseek_v3.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_qwen3_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_qwen3_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_qwen2_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_qwen3_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_deepseek.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_cache.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_llama.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_llama.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_qwen2_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_mixtral.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_deepseek_v3.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_deepseek.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_qwen2_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize\n",
            "  copying ktransformers/optimize/optimize.py -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  copying ktransformers/tests/AIME_2024/evaluation.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  copying ktransformers/tests/AIME_2024/eval_api.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  copying ktransformers/tests/AIME_2024/prompts.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  copying ktransformers/tests/humaneval/evaluation.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  copying ktransformers/tests/humaneval/eval_api.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  copying ktransformers/tests/humaneval/prompts.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/base.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/context_manager.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/args.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  copying ktransformers/server/schemas/base.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  copying ktransformers/server/schemas/conversation.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  copying ktransformers/server/schemas/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/crud\n",
            "  copying ktransformers/server/crud/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api\n",
            "  copying ktransformers/server/api/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/sql_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/create_interface.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/multi_timer.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/models\n",
            "  copying ktransformers/server/models/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  copying ktransformers/server/config/config.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  copying ktransformers/server/config/log.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  copying ktransformers/server/config/singleton.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve\n",
            "  copying ktransformers/server/balance_serve/settings.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve\n",
            "  copying ktransformers/server/balance_serve/sched_rpc.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/exllamav2.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/ktransformers.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/transformers.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/balance_serve.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy\n",
            "  copying ktransformers/server/schemas/legacy/completions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy\n",
            "  copying ktransformers/server/schemas/legacy/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/endpoints\n",
            "  copying ktransformers/server/schemas/endpoints/chat.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/endpoints\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/streaming.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/tool.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai\n",
            "  copying ktransformers/server/api/openai/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web\n",
            "  copying ktransformers/server/api/web/system.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web\n",
            "  copying ktransformers/server/api/web/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama\n",
            "  copying ktransformers/server/api/ollama/completions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama\n",
            "  copying ktransformers/server/api/ollama/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy\n",
            "  copying ktransformers/server/api/openai/legacy/completions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy\n",
            "  copying ktransformers/server/api/openai/legacy/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints\n",
            "  copying ktransformers/server/api/openai/endpoints/chat.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints\n",
            "  copying ktransformers/server/api/openai/endpoints/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/run_steps.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/model_runner.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/forward_batch.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/config.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/query_manager.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/sampler.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/custom_all_reduce_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/pynccl_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/pynccl.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/communication_op.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/cuda_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/custom_all_reduce.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/parallel_state.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/orchestrator.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/min_new_tokens.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/presence_penalty.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/repetition_penalty.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/frequency_penalty.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/triton\n",
            "  copying ktransformers/ktransformers_ext/triton/fp8gemm.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/triton\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_perms.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_24_perms.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/quant_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/format_24.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  running egg_info\n",
            "  creating ktransformers.egg-info\n",
            "  writing ktransformers.egg-info/PKG-INFO\n",
            "  writing dependency_links to ktransformers.egg-info/dependency_links.txt\n",
            "  writing entry points to ktransformers.egg-info/entry_points.txt\n",
            "  writing requirements to ktransformers.egg-info/requires.txt\n",
            "  writing top-level names to ktransformers.egg-info/top_level.txt\n",
            "  writing manifest file 'ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest file 'ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  warning: no directories found matching 'local_chat.py'\n",
            "  no previously-included directories found matching 'ktransformers/logs'\n",
            "  no previously-included directories found matching 'third_party/llama.cpp/models'\n",
            "  warning: no directories found matching 'ktransformers/website/dist'\n",
            "  warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
            "  warning: no files found matching 'KTransformersOps.*.so'\n",
            "  warning: no files found matching 'cpuinfer_ext.*.so'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'ktransformers.egg-info/SOURCES.txt'\n",
            "  copying ktransformers/tests/.gitignore -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/server/requirements.txt -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/configs\n",
            "  copying ktransformers/configs/config.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/configs\n",
            "  copying ktransformers/configs/log_config.ini -> build/lib.linux-x86_64-cpython-311/ktransformers/configs\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu-4.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-4.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-8.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-fp8-linear-ggml-experts.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-marlin.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Internlm2_5-7b-Chat-1m.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Mixtral.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Moonlight-16B-A3B-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Moonlight-16B-A3B.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-serve-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen3Moe-serve-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen3Moe-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  copying ktransformers/optimize/optimize_rules/xpu/DeepSeek-V2-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  copying ktransformers/optimize/optimize_rules/xpu/DeepSeek-V3-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  copying ktransformers/optimize/optimize_rules/xpu/Qwen3Moe-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/rocm\n",
            "  copying ktransformers/optimize/optimize_rules/rocm/DeepSeek-V3-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/rocm\n",
            "  running build_ext\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n",
            "    warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n",
            "    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  -- Compiler and CPU support AVX512F (tested by compiling a program)\n",
            "  -- Compiler does NOT support AMX\n",
            "  CMake Error at CMakeLists.txt:275 (add_subdirectory):\n",
            "    The source directory\n",
            "\n",
            "      /content/ktransformers/third_party/pybind11\n",
            "\n",
            "    does not contain a CMakeLists.txt file.\n",
            "\n",
            "\n",
            "  CMake Error at CMakeLists.txt:276 (add_subdirectory):\n",
            "    The source directory\n",
            "\n",
            "      /content/ktransformers/third_party/llama.cpp\n",
            "\n",
            "    does not contain a CMakeLists.txt file.\n",
            "\n",
            "\n",
            "  CMake Warning (dev) at CMakeLists.txt:310 (find_package):\n",
            "    Policy CMP0146 is not set: The FindCUDA module is removed.  Run \"cmake\n",
            "    --help-policy CMP0146\" for policy details.  Use the cmake_policy command to\n",
            "    set the policy and suppress this warning.\n",
            "\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  -- Found CUDA: /usr/local/cuda (found version \"12.5\")\n",
            "  -- Looking for a CUDA compiler\n",
            "  -- Looking for a CUDA compiler - /usr/local/cuda/bin/nvcc\n",
            "  -- CUDA detected\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.5.82\")\n",
            "  -- enabling CUDA\n",
            "  -- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- CMAKE_CXX_FLAGS:  -O3 -ffast-math -fopenmp\n",
            "  -- ARCH_FLAGS: -mfma;-mavx;-mavx2;-march=native\n",
            "  CMake Error at CMakeLists.txt:354 (pybind11_add_module):\n",
            "    Unknown CMake command \"pybind11_add_module\".\n",
            "\n",
            "\n",
            "  -- Configuring incomplete, errors occurred!\n",
            "  CMake args: ['-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/content/ktransformers/build/lib.linux-x86_64-cpython-311/', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DCMAKE_BUILD_TYPE=Release', '-DKTRANSFORMERS_USE_CUDA=ON', '-D_GLIBCXX_USE_CXX11_ABI=0']\n",
            "  CMake args: ['-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/content/ktransformers/build/lib.linux-x86_64-cpython-311/', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DCMAKE_BUILD_TYPE=Release', '-DKTRANSFORMERS_USE_CUDA=ON', '-D_GLIBCXX_USE_CXX11_ABI=0', '-DLLAMA_NATIVE=ON', '-DEXAMPLE_VERSION_INFO=0.3.1+cu125torch26fancy']\n",
            "  build_temp: /content/ktransformers/csrc/ktransformers_ext/build\n",
            "  Traceback (most recent call last):\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
            "      main()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
            "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 251, in build_wheel\n",
            "      return _build_backend().build_wheel(wheel_directory, config_settings,\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/build_meta.py\", line 434, in build_wheel\n",
            "      return _build(['bdist_wheel', '--dist-info-dir', metadata_directory])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/build_meta.py\", line 422, in _build\n",
            "      return self._build_with_temp_dir(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/build_meta.py\", line 403, in _build_with_temp_dir\n",
            "      self.run_setup()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/build_meta.py\", line 318, in run_setup\n",
            "      exec(code, locals())\n",
            "    File \"<string>\", line 668, in <module>\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/__init__.py\", line 117, in setup\n",
            "      return distutils.core.setup(**attrs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/core.py\", line 183, in setup\n",
            "      return run_commands(dist)\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/core.py\", line 199, in run_commands\n",
            "      dist.run_commands()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py\", line 954, in run_commands\n",
            "      self.run_command(cmd)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/dist.py\", line 991, in run_command\n",
            "      super().run_command(command)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
            "      cmd_obj.run()\n",
            "    File \"<string>\", line 263, in run\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/command/bdist_wheel.py\", line 407, in run\n",
            "      self.run_command(\"build\")\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "      self.distribution.run_command(command)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/dist.py\", line 991, in run_command\n",
            "      super().run_command(command)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
            "      cmd_obj.run()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/command/build.py\", line 135, in run\n",
            "      self.run_command(cmd_name)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "      self.distribution.run_command(command)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/dist.py\", line 991, in run_command\n",
            "      super().run_command(command)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
            "      cmd_obj.run()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/command/build_ext.py\", line 98, in run\n",
            "      _build_ext.run(self)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\n",
            "      self.build_extensions()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py\", line 900, in build_extensions\n",
            "      build_ext.build_extensions(self)\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/command/build_ext.py\", line 476, in build_extensions\n",
            "      self._build_extensions_serial()\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/command/build_ext.py\", line 502, in _build_extensions_serial\n",
            "      self.build_extension(ext)\n",
            "    File \"<string>\", line 591, in build_extension\n",
            "    File \"<string>\", line 370, in run_command_with_live_tail\n",
            "    File \"/usr/lib/python3.11/subprocess.py\", line 571, in run\n",
            "      raise CalledProcessError(retcode, process.args,\n",
            "  subprocess.CalledProcessError: Command '['cmake', '/content/ktransformers/csrc/ktransformers_ext', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/content/ktransformers/build/lib.linux-x86_64-cpython-311/', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DCMAKE_BUILD_TYPE=Release', '-DKTRANSFORMERS_USE_CUDA=ON', '-D_GLIBCXX_USE_CXX11_ABI=0', '-DLLAMA_NATIVE=ON', '-DEXAMPLE_VERSION_INFO=0.3.1+cu125torch26fancy']' returned non-zero exit status 1.\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for ktransformers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  \u001b[1;35mfull command\u001b[0m: \u001b[34m/usr/bin/python3 /usr/local/lib/python3.11/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_wheel /tmp/tmpc45qdkka\u001b[0m\n",
            "  \u001b[1;35mcwd\u001b[0m: /content/ktransformers\n",
            "  Building wheel for ktransformers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for ktransformers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build ktransformers\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (ktransformers)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16wLTBtOU2o7",
        "outputId": "18f3df2e-1590-4a5e-c6c6-c522b85fa636"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/ktransformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1) (4.51.3)\n",
            "Requirement already satisfied: fastapi>=0.111.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1) (0.115.12)\n",
            "Requirement already satisfied: uvicorn>=0.30.1 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1) (0.34.3)\n",
            "Requirement already satisfied: langchain>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1) (0.3.25)\n",
            "Requirement already satisfied: blessed>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1) (1.21.0)\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1) (1.7.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1) (75.2.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1) (1.11.1.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1) (0.45.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1) (6.9.0)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1) (1.2.2.post1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1) (0.7.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.31.0->ktransformers==0.3.1) (5.9.5)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.20.0->ktransformers==0.3.1) (0.2.13)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers==0.3.1) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers==0.3.1) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers==0.3.1) (4.14.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1) (0.3.45)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1) (2.0.41)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->ktransformers==0.3.1) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers==0.3.1) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers==0.3.1) (0.16.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build->ktransformers==0.3.1) (1.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->ktransformers==0.3.1) (3.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3->ktransformers==0.3.1) (1.1.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers==0.3.1) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers==0.3.1) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.3.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.3.1) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.3.1) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2.0->ktransformers==0.3.1) (3.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.111.0->ktransformers==0.3.1) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->ktransformers==0.3.1) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.111.0->ktransformers==0.3.1) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1) (1.0.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers==0.3.1) (3.0.0)\n",
            "Building wheels for collected packages: ktransformers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding editable for ktransformers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building editable for ktransformers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building editable for ktransformers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build ktransformers\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (ktransformers)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ktransformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ln4HG8HVB0D",
        "outputId": "7f954038-0317-4c9e-d373-29300ca444f6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ktransformers\n",
            "  Downloading ktransformers-0.2.1.post1.tar.gz (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (2.6.0+cu124)\n",
            "Collecting transformers==4.43.2 (from ktransformers)\n",
            "  Downloading transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastapi>=0.111.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.115.12)\n",
            "Requirement already satisfied: uvicorn>=0.30.1 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.34.3)\n",
            "Requirement already satisfied: langchain>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.3.25)\n",
            "Requirement already satisfied: blessed>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (1.21.0)\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (1.7.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ktransformers) (75.2.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from ktransformers) (1.11.1.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.45.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from ktransformers) (6.9.0)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.11/dist-packages (from ktransformers) (1.2.2.post1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.7.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from ktransformers) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.43.2->ktransformers)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.31.0->ktransformers) (5.9.5)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.20.0->ktransformers) (0.2.13)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers) (4.14.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers) (0.3.45)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers) (2.0.41)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->ktransformers) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers) (0.16.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build->ktransformers) (1.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->ktransformers) (3.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.2->ktransformers) (1.1.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.43.2->ktransformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.43.2->ktransformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.43.2->ktransformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.43.2->ktransformers) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2.0->ktransformers) (3.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.111.0->ktransformers) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->ktransformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.111.0->ktransformers) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers) (1.0.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers) (3.0.0)\n",
            "Downloading transformers-4.43.2-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: ktransformers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for ktransformers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for ktransformers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for ktransformers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build ktransformers\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (ktransformers)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zzBB9gykVttt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e915742a",
        "outputId": "12616649-7735-4e6b-c811-bad55890d0b3"
      },
      "source": [
        "!pip install ktransformers"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ktransformers\n",
            "  Using cached ktransformers-0.2.1.post1.tar.gz (6.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (2.6.0+cu124)\n",
            "Collecting transformers==4.43.2 (from ktransformers)\n",
            "  Using cached transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: fastapi>=0.111.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.115.12)\n",
            "Requirement already satisfied: uvicorn>=0.30.1 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.34.3)\n",
            "Requirement already satisfied: langchain>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.3.25)\n",
            "Requirement already satisfied: blessed>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (1.21.0)\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (1.7.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ktransformers) (75.2.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from ktransformers) (1.11.1.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.45.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from ktransformers) (6.9.0)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.11/dist-packages (from ktransformers) (1.2.2.post1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from ktransformers) (0.7.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from ktransformers) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.43.2->ktransformers)\n",
            "  Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.43.2->ktransformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.31.0->ktransformers) (5.9.5)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.20.0->ktransformers) (0.2.13)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers) (4.14.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers) (0.3.45)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers) (2.0.41)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->ktransformers) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers) (0.16.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build->ktransformers) (1.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->ktransformers) (3.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.2->ktransformers) (1.1.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.43.2->ktransformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.43.2->ktransformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.43.2->ktransformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.43.2->ktransformers) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2.0->ktransformers) (3.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.111.0->ktransformers) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->ktransformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.111.0->ktransformers) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers) (1.0.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers) (3.0.0)\n",
            "Using cached transformers-4.43.2-py3-none-any.whl (9.4 MB)\n",
            "Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Building wheels for collected packages: ktransformers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for ktransformers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for ktransformers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for ktransformers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build ktransformers\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (ktransformers)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding CUDA to PATH\n",
        "if [ -d \"/usr/local/cuda/bin\" ]; then\n",
        "    export PATH=$PATH:/usr/local/cuda/bin\n",
        "fi\n",
        "\n",
        "if [ -d \"/usr/local/cuda/lib64\" ]; then\n",
        "    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64\n",
        "    # Or you can add it to /etc/ld.so.conf and run ldconfig as root:\n",
        "    # echo \"/usr/local/cuda-12.x/lib64\" | sudo tee -a /etc/ld.so.conf\n",
        "    # sudo ldconfig\n",
        "fi\n",
        "\n",
        "if [ -d \"/usr/local/cuda\" ]; then\n",
        "    export CUDA_PATH=$CUDA_PATH:/usr/local/cuda\n",
        "fi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "A7VzEe86Xc38",
        "outputId": "fcfdcc15-f7d3-46c4-9ad4-002098deb7fb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (ipython-input-11-2846425257.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-11-2846425257.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    if [ -d \"/usr/local/cuda/bin\" ]; then\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export PATH=$PATH:/usr/local/cuda/\n",
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64\n",
        "!export CUDA_PATH=$CUDA_PATH:/usr/local/cuda"
      ],
      "metadata": {
        "id": "AZA-dSwDXmSb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install build-essential cmake ninja-build patchelf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeVWDzcGXuvM",
        "outputId": "6bf6ef35-b6c1-4444-e8f2-d6d1caab931e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [Connecting to security.\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "The following NEW packages will be installed:\n",
            "  ninja-build patchelf\n",
            "0 upgraded, 2 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 184 kB of archives.\n",
            "After this operation, 545 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ninja-build amd64 1.10.1-1 [111 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 patchelf amd64 0.14.3-1 [72.1 kB]\n",
            "Fetched 184 kB in 1s (291 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package ninja-build.\n",
            "(Reading database ... 126319 files and directories currently installed.)\n",
            "Preparing to unpack .../ninja-build_1.10.1-1_amd64.deb ...\n",
            "Unpacking ninja-build (1.10.1-1) ...\n",
            "Selecting previously unselected package patchelf.\n",
            "Preparing to unpack .../patchelf_0.14.3-1_amd64.deb ...\n",
            "Unpacking patchelf (0.14.3-1) ...\n",
            "Setting up ninja-build (1.10.1-1) ...\n",
            "Setting up patchelf (0.14.3-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
        "!pip3 install packaging ninja cpufeature numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bUPW-4CXweU",
        "outputId": "fe29b65b-7e02-4524-c0e2-a1e4ce11efd0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (24.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (1.11.1.4)\n",
            "Collecting cpufeature\n",
            "  Using cached cpufeature-0.2.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Using cached cpufeature-0.2.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Installing collected packages: cpufeature\n",
            "Successfully installed cpufeature-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install libtbb-dev libssl-dev libcurl4-openssl-dev libaio1 libaio-dev libgflags-dev zlib1g-dev libfmt-dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41kdYCq4X5dU",
        "outputId": "c4ac839d-f1c4-4ce7-8cf5-191eb3a0ecf4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libaio1 is already the newest version (0.3.112-13build1).\n",
            "libaio1 set to manually installed.\n",
            "libtbb-dev is already the newest version (2021.5.0-7ubuntu2).\n",
            "libtbb-dev set to manually installed.\n",
            "libcurl4-openssl-dev is already the newest version (7.81.0-1ubuntu1.20).\n",
            "libssl-dev is already the newest version (3.0.2-0ubuntu1.19).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu9.2).\n",
            "zlib1g-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  libfmt8 libgflags2.2\n",
            "Suggested packages:\n",
            "  libfmt-doc\n",
            "The following NEW packages will be installed:\n",
            "  libaio-dev libfmt-dev libfmt8 libgflags-dev libgflags2.2\n",
            "0 upgraded, 5 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 371 kB of archives.\n",
            "After this operation, 1,820 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libaio-dev amd64 0.3.112-13build1 [21.2 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfmt8 amd64 8.1.1+ds1-2 [60.2 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgflags2.2 amd64 2.2.2-2 [78.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgflags-dev amd64 2.2.2-2 [93.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfmt-dev amd64 8.1.1+ds1-2 [118 kB]\n",
            "Fetched 371 kB in 1s (324 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libaio-dev:amd64.\n",
            "(Reading database ... 126337 files and directories currently installed.)\n",
            "Preparing to unpack .../libaio-dev_0.3.112-13build1_amd64.deb ...\n",
            "Unpacking libaio-dev:amd64 (0.3.112-13build1) ...\n",
            "Selecting previously unselected package libfmt8:amd64.\n",
            "Preparing to unpack .../libfmt8_8.1.1+ds1-2_amd64.deb ...\n",
            "Unpacking libfmt8:amd64 (8.1.1+ds1-2) ...\n",
            "Selecting previously unselected package libgflags2.2.\n",
            "Preparing to unpack .../libgflags2.2_2.2.2-2_amd64.deb ...\n",
            "Unpacking libgflags2.2 (2.2.2-2) ...\n",
            "Selecting previously unselected package libgflags-dev.\n",
            "Preparing to unpack .../libgflags-dev_2.2.2-2_amd64.deb ...\n",
            "Unpacking libgflags-dev (2.2.2-2) ...\n",
            "Selecting previously unselected package libfmt-dev:amd64.\n",
            "Preparing to unpack .../libfmt-dev_8.1.1+ds1-2_amd64.deb ...\n",
            "Unpacking libfmt-dev:amd64 (8.1.1+ds1-2) ...\n",
            "Setting up libaio-dev:amd64 (0.3.112-13build1) ...\n",
            "Setting up libfmt8:amd64 (8.1.1+ds1-2) ...\n",
            "Setting up libgflags2.2 (2.2.2-2) ...\n",
            "Setting up libgflags-dev (2.2.2-2) ...\n",
            "Setting up libfmt-dev:amd64 (8.1.1+ds1-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git submodule update --init --recursive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nK0lo1wX9vk",
        "outputId": "5077cdf7-02f9-42cb-952b-78f1a408378d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submodule 'third_party/custom_flashinfer' (https://github.com/kvcache-ai/custom_flashinfer.git) registered for path 'third_party/custom_flashinfer'\n",
            "Submodule 'third_party/llama.cpp' (https://github.com/ggerganov/llama.cpp.git) registered for path 'third_party/llama.cpp'\n",
            "Submodule 'third_party/prometheus-cpp' (https://github.com/jupp0r/prometheus-cpp) registered for path 'third_party/prometheus-cpp'\n",
            "Submodule 'third_party/pybind11' (https://github.com/pybind/pybind11.git) registered for path 'third_party/pybind11'\n",
            "Submodule 'third_party/spdlog' (https://github.com/gabime/spdlog.git) registered for path 'third_party/spdlog'\n",
            "Submodule 'third_party/xxHash' (https://github.com/Cyan4973/xxHash.git) registered for path 'third_party/xxHash'\n",
            "Cloning into '/content/ktransformers/third_party/custom_flashinfer'...\n",
            "Cloning into '/content/ktransformers/third_party/llama.cpp'...\n",
            "Cloning into '/content/ktransformers/third_party/prometheus-cpp'...\n",
            "Cloning into '/content/ktransformers/third_party/pybind11'...\n",
            "Cloning into '/content/ktransformers/third_party/spdlog'...\n",
            "Cloning into '/content/ktransformers/third_party/xxHash'...\n",
            "Submodule path 'third_party/custom_flashinfer': checked out 'a3ebdf890fd15550283e692c1cd3d673be78a93c'\n",
            "Submodule '3rdparty/composable_kernels' (https://github.com/ROCm/composable_kernel.git) registered for path 'third_party/custom_flashinfer/3rdparty/composable_kernels'\n",
            "Submodule '3rdparty/cutlass' (https://github.com/NVIDIA/cutlass.git) registered for path 'third_party/custom_flashinfer/3rdparty/cutlass'\n",
            "Submodule '3rdparty/googletest' (https://github.com/google/googletest.git) registered for path 'third_party/custom_flashinfer/3rdparty/googletest'\n",
            "Submodule '3rdparty/mscclpp' (https://github.com/microsoft/mscclpp.git) registered for path 'third_party/custom_flashinfer/3rdparty/mscclpp'\n",
            "Submodule '3rdparty/nvbench' (https://github.com/NVIDIA/nvbench.git) registered for path 'third_party/custom_flashinfer/3rdparty/nvbench'\n",
            "Submodule '3rdparty/spdlog' (https://github.com/gabime/spdlog.git) registered for path 'third_party/custom_flashinfer/3rdparty/spdlog'\n",
            "Cloning into '/content/ktransformers/third_party/custom_flashinfer/3rdparty/composable_kernels'...\n",
            "Cloning into '/content/ktransformers/third_party/custom_flashinfer/3rdparty/cutlass'...\n",
            "Cloning into '/content/ktransformers/third_party/custom_flashinfer/3rdparty/googletest'...\n",
            "Cloning into '/content/ktransformers/third_party/custom_flashinfer/3rdparty/mscclpp'...\n",
            "Cloning into '/content/ktransformers/third_party/custom_flashinfer/3rdparty/nvbench'...\n",
            "Cloning into '/content/ktransformers/third_party/custom_flashinfer/3rdparty/spdlog'...\n",
            "Submodule path 'third_party/custom_flashinfer/3rdparty/composable_kernels': checked out '5055b3bdcb5a7f0b8f359b606d3c5b75efd6df54'\n",
            "Submodule path 'third_party/custom_flashinfer/3rdparty/cutlass': checked out 'cc3c29a81a140f7b97045718fb88eb0664c37bd7'\n",
            "Submodule path 'third_party/custom_flashinfer/3rdparty/googletest': checked out '5a37b517ad4ab6738556f0284c256cae1466c5b4'\n",
            "Submodule path 'third_party/custom_flashinfer/3rdparty/mscclpp': checked out 'cddffbc8b6dfa6facf7c64c1b7d73acf30e600b3'\n",
            "Submodule path 'third_party/custom_flashinfer/3rdparty/nvbench': checked out '555d628e9b250868c9da003e4407087ff1982e8e'\n",
            "Submodule path 'third_party/custom_flashinfer/3rdparty/spdlog': checked out 'c3aed4b68373955e1cc94307683d44dca1515d2b'\n",
            "Submodule path 'third_party/llama.cpp': checked out 'a94e6ff8774b7c9f950d9545baf0ce35e8d1ed2f'\n",
            "Submodule 'kompute' (https://github.com/nomic-ai/kompute.git) registered for path 'third_party/llama.cpp/kompute'\n",
            "Cloning into '/content/ktransformers/third_party/llama.cpp/kompute'...\n",
            "Submodule path 'third_party/llama.cpp/kompute': checked out '4565194ed7c32d1d2efa32ceab4d3c6cae006306'\n",
            "Submodule path 'third_party/prometheus-cpp': checked out 'f13cdd052eeae5e89decc11bf03697d0f78b15bc'\n",
            "Submodule 'civetweb' (https://github.com/civetweb/civetweb.git) registered for path 'third_party/prometheus-cpp/3rdparty/civetweb'\n",
            "Submodule 'googletest' (https://github.com/google/googletest.git) registered for path 'third_party/prometheus-cpp/3rdparty/googletest'\n",
            "Cloning into '/content/ktransformers/third_party/prometheus-cpp/3rdparty/civetweb'...\n",
            "Cloning into '/content/ktransformers/third_party/prometheus-cpp/3rdparty/googletest'...\n",
            "Submodule path 'third_party/prometheus-cpp/3rdparty/civetweb': checked out 'd7ba35bbb649209c66e582d5a0244ba988a15159'\n",
            "Submodule path 'third_party/prometheus-cpp/3rdparty/googletest': checked out 'e2239ee6043f73722e7aa812a459f54a28552929'\n",
            "Submodule path 'third_party/pybind11': checked out 'bb05e0810b87e74709d9f4c4545f1f57a1b386f5'\n",
            "Submodule path 'third_party/spdlog': checked out '48bcf39a661a13be22666ac64db8a7f886f2637e'\n",
            "Submodule path 'third_party/xxHash': checked out '953a09abc39096da9e216b6eb0002c681cdc1199'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash install.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN2aVehVYCG7",
        "outputId": "44fb6869-22ea-4f7c-8805-624a11daa5f3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected backend: cuda\n",
            "Installing python dependencies from requirements.txt\n",
            "Ignoring cpufeature: markers 'sys_platform == \"win32\" or sys_platform == \"Windows\"' don't match your environment\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 2)) (4.51.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 4)) (2.6.0+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 7)) (5.29.5)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: blobfile in /usr/local/lib/python3.11/dist-packages (from -r requirements-local_chat.txt (line 9)) (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (0.33.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->-r requirements-local_chat.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements-local_chat.txt (line 9)) (3.23.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements-local_chat.txt (line 9)) (2.4.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements-local_chat.txt (line 9)) (5.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r requirements-local_chat.txt (line 2)) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->-r requirements-local_chat.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 2)) (4.51.3)\n",
            "Requirement already satisfied: fastapi>=0.111.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 3)) (0.115.12)\n",
            "Requirement already satisfied: langchain>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 4)) (0.3.25)\n",
            "Requirement already satisfied: blessed>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 5)) (1.21.0)\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 6)) (1.7.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 8)) (1.86.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 9)) (75.2.0)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 10)) (1.2.2.post1)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 11)) (1.11.1.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 12)) (0.45.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 13)) (6.9.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 14)) (0.7.0)\n",
            "Requirement already satisfied: zmq in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 15)) (0.0.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r ktransformers/server/requirements.txt (line 16)) (5.9.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (2.11.7)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.3.45)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (2.0.41)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.20.0->-r ktransformers/server/requirements.txt (line 5)) (0.2.13)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->-r ktransformers/server/requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build->-r ktransformers/server/requirements.txt (line 10)) (1.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->-r ktransformers/server/requirements.txt (line 14)) (3.1.0)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from zmq->-r ktransformers/server/requirements.txt (line 15)) (24.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai->-r ktransformers/server/requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r ktransformers/server/requirements.txt (line 8)) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r ktransformers/server/requirements.txt (line 8)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r ktransformers/server/requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->-r ktransformers/server/requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->-r ktransformers/server/requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (3.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->-r ktransformers/server/requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->-r ktransformers/server/requirements.txt (line 4)) (3.0.0)\n",
            "Installing ktransformers\n",
            "Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
            "Processing /content/ktransformers\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "  /usr/local/lib/python3.11/dist-packages/setuptools/config/_apply_pyprojecttoml.py:78: SetuptoolsWarning: `install_requires` overwritten in `pyproject.toml` (dependencies)\n",
            "    corresp(dist, value, root_dir)\n",
            "  cpuinfer_ext /content/ktransformers/csrc/ktransformers_ext\n",
            "  Using native cpu instruct\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-vhxitqm3/ktransformers.egg-info\n",
            "  writing /tmp/pip-modern-metadata-vhxitqm3/ktransformers.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-vhxitqm3/ktransformers.egg-info/dependency_links.txt\n",
            "  writing entry points to /tmp/pip-modern-metadata-vhxitqm3/ktransformers.egg-info/entry_points.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-vhxitqm3/ktransformers.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-vhxitqm3/ktransformers.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-vhxitqm3/ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-vhxitqm3/ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  warning: no directories found matching 'local_chat.py'\n",
            "  no previously-included directories found matching 'ktransformers/logs'\n",
            "  no previously-included directories found matching 'ktransformers.egg-info'\n",
            "  warning: no directories found matching 'ktransformers/website/dist'\n",
            "  warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
            "  warning: no files found matching 'KTransformersOps.*.so'\n",
            "  warning: no files found matching 'cpuinfer_ext.*.so'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-vhxitqm3/ktransformers.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-vhxitqm3/ktransformers-0.3.1+cu125torch26fancy.dist-info'\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (4.51.3)\n",
            "Requirement already satisfied: fastapi>=0.111.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.115.12)\n",
            "Requirement already satisfied: uvicorn>=0.30.1 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.34.3)\n",
            "Requirement already satisfied: langchain>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.3.25)\n",
            "Requirement already satisfied: blessed>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (1.21.0)\n",
            "Requirement already satisfied: accelerate>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (1.7.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (75.2.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (1.11.1.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.45.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (6.9.0)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (1.2.2.post1)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (0.7.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from ktransformers==0.3.1+cu125torch26fancy) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.31.0->ktransformers==0.3.1+cu125torch26fancy) (5.9.5)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.20.0->ktransformers==0.3.1+cu125torch26fancy) (0.2.13)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (4.14.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (0.3.45)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (2.0.41)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers==0.3.1+cu125torch26fancy) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.1->ktransformers==0.3.1+cu125torch26fancy) (0.16.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build->ktransformers==0.3.1+cu125torch26fancy) (1.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->ktransformers==0.3.1+cu125torch26fancy) (3.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (1.1.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.3->ktransformers==0.3.1+cu125torch26fancy) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (3.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->ktransformers==0.3.1+cu125torch26fancy) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.111.0->ktransformers==0.3.1+cu125torch26fancy) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (1.0.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain>=0.2.0->ktransformers==0.3.1+cu125torch26fancy) (3.0.0)\n",
            "Building wheels for collected packages: ktransformers\n",
            "  Running command Building wheel for ktransformers (pyproject.toml)\n",
            "  /usr/local/lib/python3.11/dist-packages/setuptools/config/_apply_pyprojecttoml.py:78: SetuptoolsWarning: `install_requires` overwritten in `pyproject.toml` (dependencies)\n",
            "    corresp(dist, value, root_dir)\n",
            "  cpuinfer_ext /content/ktransformers/csrc/ktransformers_ext\n",
            "  Using native cpu instruct\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  copying ktransformers/local_chat.py -> build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  copying ktransformers/local_chat_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  copying ktransformers/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/triton_fp8gemm_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/mmlu_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/mmlu_pro_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/dequant_gpu_t.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/score.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/test_client.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/test_pytorch_q8.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/mmlu_test_multi.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/dequant_gpu.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/test_speed.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/tests/function_call_test.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/exceptions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/args.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  copying ktransformers/server/main.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/gate.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/RoPE.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/flashinfer_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/base_operator.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/mlp.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/experts.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/linear.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/dynamic_attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/triton_attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/triton_attention_prefill.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/cpuinfer.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/flashinfer_batch_prefill_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/layernorm.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/balance_serve_attention.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  copying ktransformers/operators/models.py -> build/lib.linux-x86_64-cpython-311/ktransformers/operators\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/custom_gguf.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/modeling_rope_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/cuda_graph_runner.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/vendors.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/weight_loader.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/custom_loader.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  copying ktransformers/util/textstream.py -> build/lib.linux-x86_64-cpython-311/ktransformers/util\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_deepseek_v2.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_deepseek_v3.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_deepseek_v3.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_qwen3_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_qwen3_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_modeling_qwen2_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_qwen3_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_deepseek.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/custom_cache.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_llama.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_llama.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_qwen2_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_mixtral.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_deepseek_v3.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/configuration_deepseek.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  copying ktransformers/models/modeling_qwen2_moe.py -> build/lib.linux-x86_64-cpython-311/ktransformers/models\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize\n",
            "  copying ktransformers/optimize/optimize.py -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  copying ktransformers/tests/AIME_2024/evaluation.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  copying ktransformers/tests/AIME_2024/eval_api.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  copying ktransformers/tests/AIME_2024/prompts.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  copying ktransformers/tests/humaneval/evaluation.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  copying ktransformers/tests/humaneval/eval_api.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  copying ktransformers/tests/humaneval/prompts.py -> build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/base.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/context_manager.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/args.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  copying ktransformers/server/backend/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  copying ktransformers/server/schemas/base.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  copying ktransformers/server/schemas/conversation.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  copying ktransformers/server/schemas/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/crud\n",
            "  copying ktransformers/server/crud/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api\n",
            "  copying ktransformers/server/api/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/sql_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/create_interface.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/multi_timer.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  copying ktransformers/server/utils/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/utils\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/models\n",
            "  copying ktransformers/server/models/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  copying ktransformers/server/config/config.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  copying ktransformers/server/config/log.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  copying ktransformers/server/config/singleton.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/config\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve\n",
            "  copying ktransformers/server/balance_serve/settings.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve\n",
            "  copying ktransformers/server/balance_serve/sched_rpc.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/exllamav2.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/ktransformers.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/transformers.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  copying ktransformers/server/backend/interfaces/balance_serve.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy\n",
            "  copying ktransformers/server/schemas/legacy/completions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy\n",
            "  copying ktransformers/server/schemas/legacy/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/endpoints\n",
            "  copying ktransformers/server/schemas/endpoints/chat.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/endpoints\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/streaming.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/tool.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  copying ktransformers/server/schemas/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  copying ktransformers/server/crud/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai\n",
            "  copying ktransformers/server/api/openai/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web\n",
            "  copying ktransformers/server/api/web/system.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web\n",
            "  copying ktransformers/server/api/web/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama\n",
            "  copying ktransformers/server/api/ollama/completions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama\n",
            "  copying ktransformers/server/api/ollama/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy\n",
            "  copying ktransformers/server/api/openai/legacy/completions.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy\n",
            "  copying ktransformers/server/api/openai/legacy/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints\n",
            "  copying ktransformers/server/api/openai/endpoints/chat.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints\n",
            "  copying ktransformers/server/api/openai/endpoints/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  copying ktransformers/server/api/openai/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/run_steps.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/threads.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/messages.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/runs.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  copying ktransformers/server/models/assistants/assistants.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/model_runner.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/forward_batch.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/config.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  copying ktransformers/server/balance_serve/inference/query_manager.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/sampler.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/custom_all_reduce_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/pynccl_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/pynccl.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/communication_op.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/cuda_wrapper.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/custom_all_reduce.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying ktransformers/server/balance_serve/inference/distributed/parallel_state.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/orchestrator.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/min_new_tokens.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/presence_penalty.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/repetition_penalty.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/frequency_penalty.py -> build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/triton\n",
            "  copying ktransformers/ktransformers_ext/triton/fp8gemm.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/triton\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_perms.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_24_perms.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/quant_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_utils.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/format_24.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/__init__.py -> build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  running egg_info\n",
            "  creating ktransformers.egg-info\n",
            "  writing ktransformers.egg-info/PKG-INFO\n",
            "  writing dependency_links to ktransformers.egg-info/dependency_links.txt\n",
            "  writing entry points to ktransformers.egg-info/entry_points.txt\n",
            "  writing requirements to ktransformers.egg-info/requires.txt\n",
            "  writing top-level names to ktransformers.egg-info/top_level.txt\n",
            "  writing manifest file 'ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest file 'ktransformers.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  warning: no directories found matching 'local_chat.py'\n",
            "  no previously-included directories found matching 'ktransformers/logs'\n",
            "  warning: no directories found matching 'ktransformers/website/dist'\n",
            "  warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
            "  warning: no files found matching 'KTransformersOps.*.so'\n",
            "  warning: no files found matching 'cpuinfer_ext.*.so'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'ktransformers.egg-info/SOURCES.txt'\n",
            "  copying ktransformers/tests/.gitignore -> build/lib.linux-x86_64-cpython-311/ktransformers/tests\n",
            "  copying ktransformers/server/requirements.txt -> build/lib.linux-x86_64-cpython-311/ktransformers/server\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/configs\n",
            "  copying ktransformers/configs/config.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/configs\n",
            "  copying ktransformers/configs/log_config.ini -> build/lib.linux-x86_64-cpython-311/ktransformers/configs\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu-4.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-4.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-8.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-fp8-linear-ggml-experts.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-marlin.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Internlm2_5-7b-Chat-1m.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Mixtral.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Moonlight-16B-A3B-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Moonlight-16B-A3B.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct-multi-gpu.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-serve-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen2-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen3Moe-serve-amx.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  copying ktransformers/optimize/optimize_rules/Qwen3Moe-serve.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  copying ktransformers/optimize/optimize_rules/xpu/DeepSeek-V2-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  copying ktransformers/optimize/optimize_rules/xpu/DeepSeek-V3-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  copying ktransformers/optimize/optimize_rules/xpu/Qwen3Moe-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu\n",
            "  creating build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/rocm\n",
            "  copying ktransformers/optimize/optimize_rules/rocm/DeepSeek-V3-Chat.yaml -> build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/rocm\n",
            "  running build_ext\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n",
            "    warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n",
            "    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  -- Compiler and CPU support AVX512F (tested by compiling a program)\n",
            "  -- Compiler does NOT support AMX\n",
            "  CMake Deprecation Warning at /content/ktransformers/third_party/pybind11/CMakeLists.txt:13 (cmake_minimum_required):\n",
            "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "    CMake.\n",
            "\n",
            "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "    to tell CMake that the project requires at least <min> but has been updated\n",
            "    to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\n",
            "  -- pybind11 v2.14.0 dev1\n",
            "  -- Found PythonInterp: /usr/bin/python3 (found suitable version \"3.11.13\", minimum required is \"3.7\")\n",
            "  -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.11.so\n",
            "  -- Performing Test HAS_FLTO\n",
            "  -- Performing Test HAS_FLTO - Success\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  -- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "  -- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "  -- Found OpenMP: TRUE (found version \"4.5\")\n",
            "  -- OpenMP found\n",
            "  -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  CMake Warning (dev) at CMakeLists.txt:310 (find_package):\n",
            "    Policy CMP0146 is not set: The FindCUDA module is removed.  Run \"cmake\n",
            "    --help-policy CMP0146\" for policy details.  Use the cmake_policy command to\n",
            "    set the policy and suppress this warning.\n",
            "\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  -- Found CUDA: /usr/local/cuda (found version \"12.5\")\n",
            "  -- Looking for a CUDA compiler\n",
            "  -- Looking for a CUDA compiler - /usr/local/cuda/bin/nvcc\n",
            "  -- CUDA detected\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.5.82\")\n",
            "  -- enabling CUDA\n",
            "  -- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- CMAKE_CXX_FLAGS:  -O3 -ffast-math -fopenmp\n",
            "  -- ARCH_FLAGS: -mfma;-mavx;-mavx2;-march=native\n",
            "  -- NUMA support is disabled\n",
            "  -- NUMA library not found or user not set USE_NUMA - disabling NUMA support\n",
            "  -- Configuring done (7.3s)\n",
            "  -- Generating done (0.0s)\n",
            "  CMake Warning:\n",
            "    Manually-specified variables were not used by the project:\n",
            "\n",
            "      EXAMPLE_VERSION_INFO\n",
            "\n",
            "\n",
            "  -- Build files have been written to: /content/ktransformers/csrc/ktransformers_ext/build\n",
            "  Change Dir: '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "\n",
            "  Run Build Command(s): /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E env VERBOSE=1 /usr/bin/gmake -f Makefile -j2\n",
            "  /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -S/content/ktransformers/csrc/ktransformers_ext -B/content/ktransformers/csrc/ktransformers_ext/build --check-build-system CMakeFiles/Makefile.cmake 0\n",
            "  /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_progress_start /content/ktransformers/csrc/ktransformers_ext/build/CMakeFiles /content/ktransformers/csrc/ktransformers_ext/build//CMakeFiles/progress.marks\n",
            "  /usr/bin/gmake  -f CMakeFiles/Makefile2 all\n",
            "  gmake[1]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/bin/gmake  -f CMakeFiles/llamafile.dir/build.make CMakeFiles/llamafile.dir/depend\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/CMakeFiles/ggml.dir/build.make third_party/llama.cpp/CMakeFiles/ggml.dir/depend\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build/CMakeFiles/llamafile.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/third_party/llama.cpp /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/CMakeFiles/ggml.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/bin/gmake  -f CMakeFiles/llamafile.dir/build.make CMakeFiles/llamafile.dir/build\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/CMakeFiles/ggml.dir/build.make third_party/llama.cpp/CMakeFiles/ggml.dir/build\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [  1%] Building C object third_party/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/cc -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -DNDEBUG -std=gnu11 -fPIC -mfma -mavx -mavx2 -march=native -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF CMakeFiles/ggml.dir/ggml.c.o.d -o CMakeFiles/ggml.dir/ggml.c.o -c /content/ktransformers/third_party/llama.cpp/ggml.c\n",
            "  [  2%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/flags.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/flags.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/flags.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/flags.cpp.o -c /content/ktransformers/third_party/llamafile/flags.cpp\n",
            "  [  3%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o -c /content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp\n",
            "  [  5%] Building C object third_party/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/cc -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -DNDEBUG -std=gnu11 -fPIC -mfma -mavx -mavx2 -march=native -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o CMakeFiles/ggml.dir/ggml-alloc.c.o -c /content/ktransformers/third_party/llama.cpp/ggml-alloc.c\n",
            "  [  6%] Building C object third_party/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/cc -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -DNDEBUG -std=gnu11 -fPIC -mfma -mavx -mavx2 -march=native -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF CMakeFiles/ggml.dir/ggml-backend.c.o.d -o CMakeFiles/ggml.dir/ggml-backend.c.o -c /content/ktransformers/third_party/llama.cpp/ggml-backend.c\n",
            "  [  7%] Building C object third_party/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/cc -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -DNDEBUG -std=gnu11 -fPIC -mfma -mavx -mavx2 -march=native -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF CMakeFiles/ggml.dir/ggml-quants.c.o.d -o CMakeFiles/ggml.dir/ggml-quants.c.o -c /content/ktransformers/third_party/llama.cpp/ggml-quants.c\n",
            "  [  9%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o -c /content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp\n",
            "  [ 10%] Building CXX object third_party/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/ggml.dir/sgemm.cpp.o -MF CMakeFiles/ggml.dir/sgemm.cpp.o.d -o CMakeFiles/ggml.dir/sgemm.cpp.o -c /content/ktransformers/third_party/llama.cpp/sgemm.cpp\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 10%] Built target ggml\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/common/CMakeFiles/build_info.dir/build.make third_party/llama.cpp/common/CMakeFiles/build_info.dir/depend\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 11%] Generating build details from Git\n",
            "  cd /content/ktransformers/third_party/llama.cpp && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=11.4.0 -DCMAKE_C_COMPILER_ID=GNU -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/usr/bin/cc -P /content/ktransformers/third_party/llama.cpp/common/../scripts/gen-build-info-cpp.cmake\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/third_party/llama.cpp/common /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common/CMakeFiles/build_info.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/common/CMakeFiles/build_info.dir/build.make third_party/llama.cpp/common/CMakeFiles/build_info.dir/build\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 12%] Building CXX object third_party/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__  -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF CMakeFiles/build_info.dir/build-info.cpp.o.d -o CMakeFiles/build_info.dir/build-info.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/build-info.cpp\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 12%] Built target build_info\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/CMakeFiles/llama.dir/build.make third_party/llama.cpp/CMakeFiles/llama.dir/depend\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/third_party/llama.cpp /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/CMakeFiles/llama.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/CMakeFiles/llama.dir/build.make third_party/llama.cpp/CMakeFiles/llama.dir/build\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 14%] Building CXX object third_party/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF CMakeFiles/llama.dir/llama.cpp.o.d -o CMakeFiles/llama.dir/llama.cpp.o -c /content/ktransformers/third_party/llama.cpp/llama.cpp\n",
            "  [ 15%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o -c /content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp\n",
            "  [ 16%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o -c /content/ktransformers/third_party/llamafile/sgemm.cpp\n",
            "  [ 18%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp\n",
            "  [ 19%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp\n",
            "  [ 20%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp\n",
            "  [ 22%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp\n",
            "  [ 23%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp\n",
            "  [ 24%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp\n",
            "  [ 25%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp\n",
            "  [ 27%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp\n",
            "  [ 28%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp\n",
            "  [ 29%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp\n",
            "  [ 31%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp\n",
            "  [ 32%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp\n",
            "  [ 33%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp\n",
            "  [ 35%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp\n",
            "  [ 36%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp\n",
            "  [ 37%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp\n",
            "  [ 38%] Building CXX object CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -mfma -mavx -mavx2 -march=native -MD -MT CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o -MF CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o.d -o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp\n",
            "  [ 40%] Linking CXX static library libllamafile.a\n",
            "  /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -P CMakeFiles/llamafile.dir/cmake_clean_target.cmake\n",
            "  /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llamafile.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libllamafile.a CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/flags.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o CMakeFiles/llamafile.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o\n",
            "  /usr/bin/ranlib libllamafile.a\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 40%] Built target llamafile\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/CMakeFiles/ggml_static.dir/build.make third_party/llama.cpp/CMakeFiles/ggml_static.dir/depend\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/third_party/llama.cpp /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/CMakeFiles/ggml_static.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/CMakeFiles/ggml_static.dir/build.make third_party/llama.cpp/CMakeFiles/ggml_static.dir/build\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 41%] Linking CXX static library libggml_static.a\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -P CMakeFiles/ggml_static.dir/cmake_clean_target.cmake\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/ggml_static.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libggml_static.a CMakeFiles/ggml.dir/ggml.c.o \"CMakeFiles/ggml.dir/ggml-alloc.c.o\" \"CMakeFiles/ggml.dir/ggml-backend.c.o\" \"CMakeFiles/ggml.dir/ggml-quants.c.o\" CMakeFiles/ggml.dir/sgemm.cpp.o\n",
            "  /usr/bin/ranlib libggml_static.a\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 41%] Built target ggml_static\n",
            "  [ 42%] Building CXX object third_party/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o -MF CMakeFiles/llama.dir/unicode.cpp.o.d -o CMakeFiles/llama.dir/unicode.cpp.o -c /content/ktransformers/third_party/llama.cpp/unicode.cpp\n",
            "  [ 44%] Building CXX object third_party/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -fopenmp -MD -MT third_party/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -MF CMakeFiles/llama.dir/unicode-data.cpp.o.d -o CMakeFiles/llama.dir/unicode-data.cpp.o -c /content/ktransformers/third_party/llama.cpp/unicode-data.cpp\n",
            "  [ 45%] Linking CXX static library libllama.a\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -P CMakeFiles/llama.dir/cmake_clean_target.cmake\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llama.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libllama.a CMakeFiles/llama.dir/llama.cpp.o CMakeFiles/llama.dir/unicode.cpp.o \"CMakeFiles/llama.dir/unicode-data.cpp.o\" CMakeFiles/ggml.dir/ggml.c.o \"CMakeFiles/ggml.dir/ggml-alloc.c.o\" \"CMakeFiles/ggml.dir/ggml-backend.c.o\" \"CMakeFiles/ggml.dir/ggml-quants.c.o\" CMakeFiles/ggml.dir/sgemm.cpp.o\n",
            "  /usr/bin/ranlib libllama.a\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 45%] Built target llama\n",
            "  /usr/bin/gmake  -f CMakeFiles/cpuinfer_ext.dir/build.make CMakeFiles/cpuinfer_ext.dir/depend\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/common/CMakeFiles/common.dir/build.make third_party/llama.cpp/common/CMakeFiles/common.dir/depend\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build/CMakeFiles/cpuinfer_ext.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /content/ktransformers/csrc/ktransformers_ext /content/ktransformers/third_party/llama.cpp/common /content/ktransformers/csrc/ktransformers_ext/build /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common/CMakeFiles/common.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/bin/gmake  -f CMakeFiles/cpuinfer_ext.dir/build.make CMakeFiles/cpuinfer_ext.dir/build\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/bin/gmake  -f third_party/llama.cpp/common/CMakeFiles/common.dir/build.make third_party/llama.cpp/common/CMakeFiles/common.dir/build\n",
            "  [ 46%] Building CXX object CMakeFiles/cpuinfer_ext.dir/ext_bindings.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/ext_bindings.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/ext_bindings.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/ext_bindings.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/ext_bindings.cpp\n",
            "  gmake[2]: Entering directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 48%] Building CXX object third_party/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/common/. -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF CMakeFiles/common.dir/common.cpp.o.d -o CMakeFiles/common.dir/common.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/common.cpp\n",
            "  [ 49%] Building CXX object CMakeFiles/cpuinfer_ext.dir/cpu_backend/backend.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/cpu_backend/backend.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/cpu_backend/backend.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/cpu_backend/backend.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/cpu_backend/backend.cpp\n",
            "  [ 50%] Building CXX object CMakeFiles/cpuinfer_ext.dir/cpu_backend/shared_mem_buffer.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/cpu_backend/shared_mem_buffer.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/cpu_backend/shared_mem_buffer.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/cpu_backend/shared_mem_buffer.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/cpu_backend/shared_mem_buffer.cpp\n",
            "  [ 51%] Building CXX object CMakeFiles/cpuinfer_ext.dir/cpu_backend/task_queue.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/cpu_backend/task_queue.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/cpu_backend/task_queue.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/cpu_backend/task_queue.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/cpu_backend/task_queue.cpp\n",
            "  [ 53%] Building CXX object CMakeFiles/cpuinfer_ext.dir/operators/llamafile/linear.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/operators/llamafile/linear.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/operators/llamafile/linear.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/operators/llamafile/linear.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/operators/llamafile/linear.cpp\n",
            "  [ 54%] Building CXX object CMakeFiles/cpuinfer_ext.dir/operators/llamafile/mlp.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/operators/llamafile/mlp.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/operators/llamafile/mlp.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/operators/llamafile/mlp.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/operators/llamafile/mlp.cpp\n",
            "  [ 55%] Building CXX object CMakeFiles/cpuinfer_ext.dir/operators/llamafile/moe.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/operators/llamafile/moe.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/operators/llamafile/moe.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/operators/llamafile/moe.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/operators/llamafile/moe.cpp\n",
            "  [ 57%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/flags.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/flags.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/flags.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/flags.cpp.o -c /content/ktransformers/third_party/llamafile/flags.cpp\n",
            "  [ 58%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o -c /content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp\n",
            "  [ 59%] Building CXX object third_party/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/common/. -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF CMakeFiles/common.dir/sampling.cpp.o.d -o CMakeFiles/common.dir/sampling.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/sampling.cpp\n",
            "  [ 61%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o -c /content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp\n",
            "  [ 62%] Building CXX object third_party/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/common/. -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF CMakeFiles/common.dir/console.cpp.o.d -o CMakeFiles/common.dir/console.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/console.cpp\n",
            "  [ 63%] Building CXX object third_party/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/common/. -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF CMakeFiles/common.dir/grammar-parser.cpp.o.d -o CMakeFiles/common.dir/grammar-parser.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/grammar-parser.cpp\n",
            "  [ 64%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o -c /content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp\n",
            "  [ 66%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o -c /content/ktransformers/third_party/llamafile/sgemm.cpp\n",
            "  [ 67%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp\n",
            "  [ 68%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp\n",
            "  [ 70%] Building CXX object third_party/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/common/. -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -MF CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d -o CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/json-schema-to-grammar.cpp\n",
            "  [ 71%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp\n",
            "  [ 72%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp\n",
            "  [ 74%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp\n",
            "  [ 75%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp\n",
            "  [ 76%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp\n",
            "  [ 77%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp\n",
            "  [ 79%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp\n",
            "  [ 80%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp\n",
            "  [ 81%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp\n",
            "  [ 83%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp\n",
            "  [ 84%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp\n",
            "  [ 85%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp\n",
            "  [ 87%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp\n",
            "  [ 88%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp\n",
            "  [ 89%] Building CXX object CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o -c /content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp\n",
            "  [ 90%] Building CXX object CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_attn.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_attn.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_attn.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_attn.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/operators/kvcache/kvcache_attn.cpp\n",
            "  [ 92%] Building CXX object CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_load_dump.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_load_dump.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_load_dump.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_load_dump.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/operators/kvcache/kvcache_load_dump.cpp\n",
            "  [ 93%] Building CXX object third_party/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/common/. -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF CMakeFiles/common.dir/train.cpp.o.d -o CMakeFiles/common.dir/train.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/train.cpp\n",
            "  [ 94%] Building CXX object CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_read_write.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_read_write.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_read_write.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_read_write.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/operators/kvcache/kvcache_read_write.cpp\n",
            "  [ 96%] Building CXX object third_party/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/bin/c++ -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GLIBCXX_USE_CXX11_ABI=0 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -D__HAS_AVX512F__ -D__x86_64__ -I/content/ktransformers/third_party/llama.cpp/common/. -I/content/ktransformers/third_party/llama.cpp/. -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++11 -fPIC -mfma -mavx -mavx2 -march=native -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -MD -MT third_party/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -MF CMakeFiles/common.dir/ngram-cache.cpp.o.d -o CMakeFiles/common.dir/ngram-cache.cpp.o -c /content/ktransformers/third_party/llama.cpp/common/ngram-cache.cpp\n",
            "  [ 97%] Building CXX object CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_utils.cpp.o\n",
            "  /usr/bin/c++ -DKTRANSFORMERS_USE_CUDA=1 -D_GLIBCXX_USE_CXX11_ABI=0 -D__HAS_AVX512F__ -D__x86_64__ -Dcpuinfer_ext_EXPORTS -I/content/ktransformers/csrc/ktransformers_ext/../../third_party -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include -I/content/ktransformers/third_party/llama.cpp/. -isystem /content/ktransformers/third_party/pybind11/include -isystem /usr/include/python3.11 -O3 -ffast-math -fopenmp -O3 -DNDEBUG -std=gnu++17 -fPIC -fvisibility=hidden -mfma -mavx -mavx2 -march=native -flto -fno-fat-lto-objects -MD -MT CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_utils.cpp.o -MF CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_utils.cpp.o.d -o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_utils.cpp.o -c /content/ktransformers/csrc/ktransformers_ext/operators/kvcache/kvcache_utils.cpp\n",
            "  [ 98%] Linking CXX static library libcommon.a\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -P CMakeFiles/common.dir/cmake_clean_target.cmake\n",
            "  cd /content/ktransformers/csrc/ktransformers_ext/build/third_party/llama.cpp/common && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/common.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libcommon.a CMakeFiles/common.dir/common.cpp.o CMakeFiles/common.dir/sampling.cpp.o CMakeFiles/common.dir/console.cpp.o \"CMakeFiles/common.dir/grammar-parser.cpp.o\" \"CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\" CMakeFiles/common.dir/train.cpp.o \"CMakeFiles/common.dir/ngram-cache.cpp.o\" \"CMakeFiles/build_info.dir/build-info.cpp.o\"\n",
            "  /usr/bin/ranlib libcommon.a\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [ 98%] Built target common\n",
            "  [100%] Linking CXX shared module /content/ktransformers/build/lib.linux-x86_64-cpython-311/cpuinfer_ext.cpython-311-x86_64-linux-gnu.so\n",
            "  /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/cpuinfer_ext.dir/link.txt --verbose=1\n",
            "  lto-wrapper: warning: using serial compilation of 14 LTRANS jobs\n",
            "  /usr/bin/c++ -fPIC  -O3 -ffast-math -fopenmp -O3 -DNDEBUG -flto -shared  -o /content/ktransformers/build/lib.linux-x86_64-cpython-311/cpuinfer_ext.cpython-311-x86_64-linux-gnu.so CMakeFiles/cpuinfer_ext.dir/ext_bindings.cpp.o CMakeFiles/cpuinfer_ext.dir/cpu_backend/backend.cpp.o CMakeFiles/cpuinfer_ext.dir/cpu_backend/shared_mem_buffer.cpp.o CMakeFiles/cpuinfer_ext.dir/cpu_backend/task_queue.cpp.o CMakeFiles/cpuinfer_ext.dir/operators/llamafile/linear.cpp.o CMakeFiles/cpuinfer_ext.dir/operators/llamafile/mlp.cpp.o CMakeFiles/cpuinfer_ext.dir/operators/llamafile/moe.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/flags.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_avx2.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_amd_zen4.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/iqk_mul_mat_arm82.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/sgemm.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx2.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avx512f.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_avxvnni.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_fma.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_amd_zen4.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm80.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_mixmul_arm82.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx2.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avx512f.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_avxvnni.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_fma.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_amd_zen4.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm80.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_sgemm_arm82.cpp.o CMakeFiles/cpuinfer_ext.dir/content/ktransformers/third_party/llamafile/tinyblas_cpu_unsupported.cpp.o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_attn.cpp.o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_load_dump.cpp.o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_read_write.cpp.o CMakeFiles/cpuinfer_ext.dir/operators/kvcache/kvcache_utils.cpp.o  -Wl,-rpath,/usr/local/cuda/lib64 third_party/llama.cpp/libllama.a /usr/local/cuda/lib64/libcudart.so /usr/lib/gcc/x86_64-linux-gnu/11/libgomp.so /usr/lib/x86_64-linux-gnu/libpthread.a\n",
            "  /usr/bin/strip /content/ktransformers/build/lib.linux-x86_64-cpython-311/cpuinfer_ext.cpython-311-x86_64-linux-gnu.so\n",
            "  gmake[2]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  [100%] Built target cpuinfer_ext\n",
            "  gmake[1]: Leaving directory '/content/ktransformers/csrc/ktransformers_ext/build'\n",
            "  /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_progress_start /content/ktransformers/csrc/ktransformers_ext/build/CMakeFiles 0\n",
            "\n",
            "  CMake args: ['-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/content/ktransformers/build/lib.linux-x86_64-cpython-311/', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DCMAKE_BUILD_TYPE=Release', '-DKTRANSFORMERS_USE_CUDA=ON', '-D_GLIBCXX_USE_CXX11_ABI=0']\n",
            "  CMake args: ['-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=/content/ktransformers/build/lib.linux-x86_64-cpython-311/', '-DPYTHON_EXECUTABLE=/usr/bin/python3', '-DCMAKE_BUILD_TYPE=Release', '-DKTRANSFORMERS_USE_CUDA=ON', '-D_GLIBCXX_USE_CXX11_ABI=0', '-DLLAMA_NATIVE=ON', '-DEXAMPLE_VERSION_INFO=0.3.1+cu125torch26fancy']\n",
            "  build_temp: /content/ktransformers/csrc/ktransformers_ext/build\n",
            "  building 'KTransformersOps' extension\n",
            "  creating /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda\n",
            "  creating /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/custom_gguf\n",
            "  creating /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/gptq_marlin\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\n",
            "  If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "    warnings.warn(\n",
            "  Emitting ninja build file /content/ktransformers/build/temp.linux-x86_64-cpython-311/build.ninja...\n",
            "  Compiling objects...\n",
            "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "  [1/3] c++ -MMD -MF /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/binding.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /content/ktransformers/csrc/ktransformers_ext/cuda/binding.cpp -o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/binding.o -O3 -DKTRANSFORMERS_USE_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=KTransformersOps -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  [2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/gptq_marlin/gptq_marlin.o.d -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /content/ktransformers/csrc/ktransformers_ext/cuda/gptq_marlin/gptq_marlin.cu -o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/gptq_marlin/gptq_marlin.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -Xcompiler -fPIC -DKTRANSFORMERS_USE_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=KTransformersOps -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  [3/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/custom_gguf/dequant.o.d -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /content/ktransformers/csrc/ktransformers_ext/cuda/custom_gguf/dequant.cu -o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/custom_gguf/dequant.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -Xcompiler -fPIC -DKTRANSFORMERS_USE_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=KTransformersOps -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/binding.o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/custom_gguf/dequant.o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/ktransformers_ext/cuda/gptq_marlin/gptq_marlin.o -L/usr/local/lib/python3.11/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/KTransformersOps.cpython-311-x86_64-linux-gnu.so\n",
            "  building 'vLLMMarlin' extension\n",
            "  creating /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin\n",
            "  creating /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/gptq_marlin\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\n",
            "  If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "    warnings.warn(\n",
            "  Emitting ninja build file /content/ktransformers/build/temp.linux-x86_64-cpython-311/build.ninja...\n",
            "  Compiling objects...\n",
            "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "  [1/3] c++ -MMD -MF /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/binding.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /content/ktransformers/csrc/custom_marlin/binding.cpp -o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/binding.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=vLLMMarlin -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  [2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/gptq_marlin/gptq_marlin_repack.o.d -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /content/ktransformers/csrc/custom_marlin/gptq_marlin/gptq_marlin_repack.cu -o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/gptq_marlin/gptq_marlin_repack.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -Xcompiler -fPIC -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=vLLMMarlin -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  /content/ktransformers/csrc/custom_marlin/gptq_marlin/gptq_marlin_repack.cu(5): warning #177-D: variable \"gptq_marlin::repack_stages\" was declared but never referenced\n",
            "    static constexpr int repack_stages = 8;\n",
            "                         ^\n",
            "\n",
            "  Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "  /content/ktransformers/csrc/custom_marlin/gptq_marlin/gptq_marlin_repack.cu(7): warning #177-D: variable \"gptq_marlin::repack_threads\" was declared but never referenced\n",
            "    static constexpr int repack_threads = 256;\n",
            "                         ^\n",
            "\n",
            "  /content/ktransformers/csrc/custom_marlin/gptq_marlin/gptq_marlin_repack.cu(10): warning #177-D: variable \"gptq_marlin::tile_n_size\" was declared but never referenced\n",
            "    static constexpr int tile_n_size = tile_k_size * 4;\n",
            "                         ^\n",
            "\n",
            "  [3/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/gptq_marlin/gptq_marlin.o.d -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /content/ktransformers/csrc/custom_marlin/gptq_marlin/gptq_marlin.cu -o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/gptq_marlin/gptq_marlin.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -Xcompiler -fPIC -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=vLLMMarlin -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/binding.o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/gptq_marlin/gptq_marlin.o /content/ktransformers/build/temp.linux-x86_64-cpython-311/csrc/custom_marlin/gptq_marlin/gptq_marlin_repack.o -L/usr/local/lib/python3.11/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/vLLMMarlin.cpython-311-x86_64-linux-gnu.so\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  copying build/lib.linux-x86_64-cpython-311/cpuinfer_ext.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/.\n",
            "  copying build/lib.linux-x86_64-cpython-311/KTransformersOps.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/.\n",
            "  copying build/lib.linux-x86_64-cpython-311/vLLMMarlin.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/.\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/triton_fp8gemm_test.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/mmlu_test.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/mmlu_pro_test.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/dequant_gpu_t.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/.gitignore -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/score.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/test_client.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/test_pytorch_q8.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/mmlu_test_multi.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/dequant_gpu.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/tests/AIME_2024\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024/evaluation.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests/AIME_2024\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024/eval_api.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests/AIME_2024\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/AIME_2024/prompts.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests/AIME_2024\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/test_speed.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/function_call_test.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/tests/humaneval\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval/evaluation.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests/humaneval\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval/eval_api.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests/humaneval\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/tests/humaneval/prompts.py -> build/bdist.linux-x86_64/wheel/./ktransformers/tests/humaneval\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/backend\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/base.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/backend/interfaces\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces/exllamav2.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend/interfaces\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend/interfaces\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces/ktransformers.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend/interfaces\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces/transformers.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend/interfaces\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/interfaces/balance_serve.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend/interfaces\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/context_manager.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/args.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/backend/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/backend\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/exceptions.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/requirements.txt -> build/bdist.linux-x86_64/wheel/./ktransformers/server\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/schemas\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/schemas/legacy\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy/completions.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/legacy\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/legacy/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/legacy\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/base.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/schemas/endpoints\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/endpoints/chat.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/endpoints\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/conversation.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/schemas/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants/streaming.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants/threads.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants/messages.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants/tool.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants/runs.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/schemas/assistants/assistants.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/schemas/assistants\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/crud\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/crud\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/crud/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants/threads.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/crud/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants/messages.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/crud/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants/runs.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/crud/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/crud/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/crud/assistants/assistants.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/crud/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/args.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/api\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/api/openai\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/api/openai/legacy\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy/completions.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/legacy\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/legacy/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/legacy\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/api/openai/endpoints\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints/chat.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/endpoints\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/endpoints/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/endpoints\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/api/openai/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants/threads.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants/messages.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants/runs.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/openai/assistants/assistants.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/openai/assistants\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/api/web\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web/system.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/web\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/web/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/web\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/api/ollama\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama/completions.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/ollama\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/ollama/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api/ollama\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/api/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/api\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/utils/sql_utils.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/utils/create_interface.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/utils/multi_timer.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/utils/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/utils\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/models/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/models\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/models/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants/run_steps.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/models/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants/threads.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/models/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants/messages.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/models/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants/runs.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/models/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/models/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/models/assistants/assistants.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/models/assistants\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/main.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/config\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/config/config.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/config\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/config/log.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/config\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/config/singleton.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/config\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/balance_serve\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/balance_serve/inference\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/model_runner.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/balance_serve/inference/sampling\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/min_new_tokens.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/presence_penalty.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/repetition_penalty.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/frequency_penalty.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/orchestrator.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/penaltylib/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/sampling/penaltylib\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/sampling/sampler.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/sampling\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/forward_batch.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/config.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/custom_all_reduce_utils.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/pynccl_wrapper.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/pynccl.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/communication_op.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/cuda_wrapper.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/utils.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/custom_all_reduce.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/distributed/parallel_state.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference/distributed\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/inference/query_manager.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve/inference\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/settings.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/server/balance_serve/sched_rpc.py -> build/bdist.linux-x86_64/wheel/./ktransformers/server/balance_serve\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/configs\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/configs/config.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/configs\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/configs/log_config.ini -> build/bdist.linux-x86_64/wheel/./ktransformers/configs\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/attention.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/gate.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/RoPE.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/flashinfer_wrapper.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/base_operator.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/mlp.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/experts.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/linear.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/dynamic_attention.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/triton_attention.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/triton_attention_prefill.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/cpuinfer.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/flashinfer_batch_prefill_wrapper.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/layernorm.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/balance_serve_attention.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/operators/models.py -> build/bdist.linux-x86_64/wheel/./ktransformers/operators\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/local_chat.py -> build/bdist.linux-x86_64/wheel/./ktransformers\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/utils.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/custom_gguf.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/modeling_rope_utils.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/cuda_graph_runner.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/vendors.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/weight_loader.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/custom_loader.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/util/textstream.py -> build/bdist.linux-x86_64/wheel/./ktransformers/util\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/local_chat_test.py -> build/bdist.linux-x86_64/wheel/./ktransformers\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/custom_modeling_deepseek_v2.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/custom_modeling_deepseek_v3.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/configuration_deepseek_v3.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/configuration_qwen3_moe.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/custom_modeling_qwen3_moe.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/custom_modeling_qwen2_moe.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/modeling_qwen3_moe.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/modeling_deepseek.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/custom_cache.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/configuration_llama.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/modeling_llama.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/configuration_qwen2_moe.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/modeling_mixtral.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/modeling_deepseek_v3.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/configuration_deepseek.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/models/modeling_qwen2_moe.py -> build/bdist.linux-x86_64/wheel/./ktransformers/models\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/ktransformers_ext\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/ktransformers_ext/operators\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/ktransformers_ext/operators/custom_marlin\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/ktransformers_ext/operators/custom_marlin/quantize\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_perms.py -> build/bdist.linux-x86_64/wheel/./ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_24_perms.py -> build/bdist.linux-x86_64/wheel/./ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/quant_utils.py -> build/bdist.linux-x86_64/wheel/./ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_utils.py -> build/bdist.linux-x86_64/wheel/./ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/format_24.py -> build/bdist.linux-x86_64/wheel/./ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/ktransformers_ext/triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/ktransformers_ext/triton/fp8gemm.py -> build/bdist.linux-x86_64/wheel/./ktransformers/ktransformers_ext/triton\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/__init__.py -> build/bdist.linux-x86_64/wheel/./ktransformers\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/optimize\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize.py -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/optimize/optimize_rules\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/optimize/optimize_rules/xpu\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu/DeepSeek-V2-Chat.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules/xpu\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu/Qwen3Moe-Chat.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules/xpu\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/xpu/DeepSeek-V3-Chat.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules/xpu\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-serve.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers/optimize/optimize_rules/rocm\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/rocm/DeepSeek-V3-Chat.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules/rocm\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Moonlight-16B-A3B-serve.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve-amx.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-8.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-marlin.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-4.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu-4.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-fp8-linear-ggml-experts.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Mixtral.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct-multi-gpu.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Internlm2_5-7b-Chat-1m.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Qwen2-serve-amx.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-multi-gpu.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Qwen2-serve.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Qwen3Moe-serve-amx.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Moonlight-16B-A3B.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/Qwen3Moe-serve.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  copying build/lib.linux-x86_64-cpython-311/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-amx.yaml -> build/bdist.linux-x86_64/wheel/./ktransformers/optimize/optimize_rules\n",
            "  running install_egg_info\n",
            "  Copying ktransformers.egg-info to build/bdist.linux-x86_64/wheel/./ktransformers-0.3.1+cu125torch26fancy-py3.11.egg-info\n",
            "  running install_scripts\n",
            "  creating build/bdist.linux-x86_64/wheel/ktransformers-0.3.1+cu125torch26fancy.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-q9eorlpt/.tmp-qv59rqu8/ktransformers-0.3.1+cu125torch26fancy-cp311-cp311-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'KTransformersOps.cpython-311-x86_64-linux-gnu.so'\n",
            "  adding 'cpuinfer_ext.cpython-311-x86_64-linux-gnu.so'\n",
            "  adding 'vLLMMarlin.cpython-311-x86_64-linux-gnu.so'\n",
            "  adding 'ktransformers/__init__.py'\n",
            "  adding 'ktransformers/local_chat.py'\n",
            "  adding 'ktransformers/local_chat_test.py'\n",
            "  adding 'ktransformers/configs/config.yaml'\n",
            "  adding 'ktransformers/configs/log_config.ini'\n",
            "  adding 'ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/__init__.py'\n",
            "  adding 'ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/format_24.py'\n",
            "  adding 'ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_24_perms.py'\n",
            "  adding 'ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_perms.py'\n",
            "  adding 'ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/marlin_utils.py'\n",
            "  adding 'ktransformers/ktransformers_ext/operators/custom_marlin/quantize/utils/quant_utils.py'\n",
            "  adding 'ktransformers/ktransformers_ext/triton/fp8gemm.py'\n",
            "  adding 'ktransformers/models/__init__.py'\n",
            "  adding 'ktransformers/models/configuration_deepseek.py'\n",
            "  adding 'ktransformers/models/configuration_deepseek_v3.py'\n",
            "  adding 'ktransformers/models/configuration_llama.py'\n",
            "  adding 'ktransformers/models/configuration_qwen2_moe.py'\n",
            "  adding 'ktransformers/models/configuration_qwen3_moe.py'\n",
            "  adding 'ktransformers/models/custom_cache.py'\n",
            "  adding 'ktransformers/models/custom_modeling_deepseek_v2.py'\n",
            "  adding 'ktransformers/models/custom_modeling_deepseek_v3.py'\n",
            "  adding 'ktransformers/models/custom_modeling_qwen2_moe.py'\n",
            "  adding 'ktransformers/models/custom_modeling_qwen3_moe.py'\n",
            "  adding 'ktransformers/models/modeling_deepseek.py'\n",
            "  adding 'ktransformers/models/modeling_deepseek_v3.py'\n",
            "  adding 'ktransformers/models/modeling_llama.py'\n",
            "  adding 'ktransformers/models/modeling_mixtral.py'\n",
            "  adding 'ktransformers/models/modeling_qwen2_moe.py'\n",
            "  adding 'ktransformers/models/modeling_qwen3_moe.py'\n",
            "  adding 'ktransformers/operators/RoPE.py'\n",
            "  adding 'ktransformers/operators/__init__.py'\n",
            "  adding 'ktransformers/operators/attention.py'\n",
            "  adding 'ktransformers/operators/balance_serve_attention.py'\n",
            "  adding 'ktransformers/operators/base_operator.py'\n",
            "  adding 'ktransformers/operators/cpuinfer.py'\n",
            "  adding 'ktransformers/operators/dynamic_attention.py'\n",
            "  adding 'ktransformers/operators/experts.py'\n",
            "  adding 'ktransformers/operators/flashinfer_batch_prefill_wrapper.py'\n",
            "  adding 'ktransformers/operators/flashinfer_wrapper.py'\n",
            "  adding 'ktransformers/operators/gate.py'\n",
            "  adding 'ktransformers/operators/layernorm.py'\n",
            "  adding 'ktransformers/operators/linear.py'\n",
            "  adding 'ktransformers/operators/mlp.py'\n",
            "  adding 'ktransformers/operators/models.py'\n",
            "  adding 'ktransformers/operators/triton_attention.py'\n",
            "  adding 'ktransformers/operators/triton_attention_prefill.py'\n",
            "  adding 'ktransformers/optimize/optimize.py'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu-4.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-gpu-cpu.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat-multi-gpu.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V2-Lite-Chat.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-amx.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve-amx.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts-serve.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-fp8-linear-ggml-experts.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-4.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-8.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-fp8-linear-ggml-experts.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu-marlin.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-multi-gpu.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-serve.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Internlm2_5-7b-Chat-1m.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Mixtral.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Moonlight-16B-A3B-serve.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Moonlight-16B-A3B.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct-multi-gpu.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Qwen2-57B-A14B-Instruct.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Qwen2-serve-amx.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Qwen2-serve.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Qwen3Moe-serve-amx.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/Qwen3Moe-serve.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/rocm/DeepSeek-V3-Chat.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/xpu/DeepSeek-V2-Chat.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/xpu/DeepSeek-V3-Chat.yaml'\n",
            "  adding 'ktransformers/optimize/optimize_rules/xpu/Qwen3Moe-Chat.yaml'\n",
            "  adding 'ktransformers/server/__init__.py'\n",
            "  adding 'ktransformers/server/args.py'\n",
            "  adding 'ktransformers/server/exceptions.py'\n",
            "  adding 'ktransformers/server/main.py'\n",
            "  adding 'ktransformers/server/requirements.txt'\n",
            "  adding 'ktransformers/server/api/__init__.py'\n",
            "  adding 'ktransformers/server/api/ollama/__init__.py'\n",
            "  adding 'ktransformers/server/api/ollama/completions.py'\n",
            "  adding 'ktransformers/server/api/openai/__init__.py'\n",
            "  adding 'ktransformers/server/api/openai/assistants/__init__.py'\n",
            "  adding 'ktransformers/server/api/openai/assistants/assistants.py'\n",
            "  adding 'ktransformers/server/api/openai/assistants/messages.py'\n",
            "  adding 'ktransformers/server/api/openai/assistants/runs.py'\n",
            "  adding 'ktransformers/server/api/openai/assistants/threads.py'\n",
            "  adding 'ktransformers/server/api/openai/endpoints/__init__.py'\n",
            "  adding 'ktransformers/server/api/openai/endpoints/chat.py'\n",
            "  adding 'ktransformers/server/api/openai/legacy/__init__.py'\n",
            "  adding 'ktransformers/server/api/openai/legacy/completions.py'\n",
            "  adding 'ktransformers/server/api/web/__init__.py'\n",
            "  adding 'ktransformers/server/api/web/system.py'\n",
            "  adding 'ktransformers/server/backend/__init__.py'\n",
            "  adding 'ktransformers/server/backend/args.py'\n",
            "  adding 'ktransformers/server/backend/base.py'\n",
            "  adding 'ktransformers/server/backend/context_manager.py'\n",
            "  adding 'ktransformers/server/backend/interfaces/__init__.py'\n",
            "  adding 'ktransformers/server/backend/interfaces/balance_serve.py'\n",
            "  adding 'ktransformers/server/backend/interfaces/exllamav2.py'\n",
            "  adding 'ktransformers/server/backend/interfaces/ktransformers.py'\n",
            "  adding 'ktransformers/server/backend/interfaces/transformers.py'\n",
            "  adding 'ktransformers/server/balance_serve/sched_rpc.py'\n",
            "  adding 'ktransformers/server/balance_serve/settings.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/__init__.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/config.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/forward_batch.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/model_runner.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/query_manager.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/__init__.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/communication_op.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/cuda_wrapper.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/custom_all_reduce.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/custom_all_reduce_utils.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/parallel_state.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/pynccl.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/pynccl_wrapper.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/distributed/utils.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/sampling/sampler.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/sampling/penaltylib/__init__.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/sampling/penaltylib/orchestrator.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/frequency_penalty.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/min_new_tokens.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/presence_penalty.py'\n",
            "  adding 'ktransformers/server/balance_serve/inference/sampling/penaltylib/penalizers/repetition_penalty.py'\n",
            "  adding 'ktransformers/server/config/config.py'\n",
            "  adding 'ktransformers/server/config/log.py'\n",
            "  adding 'ktransformers/server/config/singleton.py'\n",
            "  adding 'ktransformers/server/crud/__init__.py'\n",
            "  adding 'ktransformers/server/crud/assistants/__init__.py'\n",
            "  adding 'ktransformers/server/crud/assistants/assistants.py'\n",
            "  adding 'ktransformers/server/crud/assistants/messages.py'\n",
            "  adding 'ktransformers/server/crud/assistants/runs.py'\n",
            "  adding 'ktransformers/server/crud/assistants/threads.py'\n",
            "  adding 'ktransformers/server/models/__init__.py'\n",
            "  adding 'ktransformers/server/models/assistants/__init__.py'\n",
            "  adding 'ktransformers/server/models/assistants/assistants.py'\n",
            "  adding 'ktransformers/server/models/assistants/messages.py'\n",
            "  adding 'ktransformers/server/models/assistants/run_steps.py'\n",
            "  adding 'ktransformers/server/models/assistants/runs.py'\n",
            "  adding 'ktransformers/server/models/assistants/threads.py'\n",
            "  adding 'ktransformers/server/schemas/__init__.py'\n",
            "  adding 'ktransformers/server/schemas/base.py'\n",
            "  adding 'ktransformers/server/schemas/conversation.py'\n",
            "  adding 'ktransformers/server/schemas/assistants/__init__.py'\n",
            "  adding 'ktransformers/server/schemas/assistants/assistants.py'\n",
            "  adding 'ktransformers/server/schemas/assistants/messages.py'\n",
            "  adding 'ktransformers/server/schemas/assistants/runs.py'\n",
            "  adding 'ktransformers/server/schemas/assistants/streaming.py'\n",
            "  adding 'ktransformers/server/schemas/assistants/threads.py'\n",
            "  adding 'ktransformers/server/schemas/assistants/tool.py'\n",
            "  adding 'ktransformers/server/schemas/endpoints/chat.py'\n",
            "  adding 'ktransformers/server/schemas/legacy/__init__.py'\n",
            "  adding 'ktransformers/server/schemas/legacy/completions.py'\n",
            "  adding 'ktransformers/server/utils/__init__.py'\n",
            "  adding 'ktransformers/server/utils/create_interface.py'\n",
            "  adding 'ktransformers/server/utils/multi_timer.py'\n",
            "  adding 'ktransformers/server/utils/sql_utils.py'\n",
            "  adding 'ktransformers/tests/.gitignore'\n",
            "  adding 'ktransformers/tests/dequant_gpu.py'\n",
            "  adding 'ktransformers/tests/dequant_gpu_t.py'\n",
            "  adding 'ktransformers/tests/function_call_test.py'\n",
            "  adding 'ktransformers/tests/mmlu_pro_test.py'\n",
            "  adding 'ktransformers/tests/mmlu_test.py'\n",
            "  adding 'ktransformers/tests/mmlu_test_multi.py'\n",
            "  adding 'ktransformers/tests/score.py'\n",
            "  adding 'ktransformers/tests/test_client.py'\n",
            "  adding 'ktransformers/tests/test_pytorch_q8.py'\n",
            "  adding 'ktransformers/tests/test_speed.py'\n",
            "  adding 'ktransformers/tests/triton_fp8gemm_test.py'\n",
            "  adding 'ktransformers/tests/AIME_2024/eval_api.py'\n",
            "  adding 'ktransformers/tests/AIME_2024/evaluation.py'\n",
            "  adding 'ktransformers/tests/AIME_2024/prompts.py'\n",
            "  adding 'ktransformers/tests/humaneval/eval_api.py'\n",
            "  adding 'ktransformers/tests/humaneval/evaluation.py'\n",
            "  adding 'ktransformers/tests/humaneval/prompts.py'\n",
            "  adding 'ktransformers/util/cuda_graph_runner.py'\n",
            "  adding 'ktransformers/util/custom_gguf.py'\n",
            "  adding 'ktransformers/util/custom_loader.py'\n",
            "  adding 'ktransformers/util/modeling_rope_utils.py'\n",
            "  adding 'ktransformers/util/textstream.py'\n",
            "  adding 'ktransformers/util/utils.py'\n",
            "  adding 'ktransformers/util/vendors.py'\n",
            "  adding 'ktransformers/util/weight_loader.py'\n",
            "  adding 'ktransformers-0.3.1+cu125torch26fancy.dist-info/LICENSE'\n",
            "  adding 'ktransformers-0.3.1+cu125torch26fancy.dist-info/METADATA'\n",
            "  adding 'ktransformers-0.3.1+cu125torch26fancy.dist-info/WHEEL'\n",
            "  adding 'ktransformers-0.3.1+cu125torch26fancy.dist-info/entry_points.txt'\n",
            "  adding 'ktransformers-0.3.1+cu125torch26fancy.dist-info/top_level.txt'\n",
            "  adding 'ktransformers-0.3.1+cu125torch26fancy.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "  Building wheel for ktransformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktransformers: filename=ktransformers-0.3.1+cu125torch26fancy-cp311-cp311-linux_x86_64.whl size=6839077 sha256=ec3de57e5f83d61780f0f9aebc888ed0e4b690b9fe0d6f824f97c6227b0f5b84\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1zj7sqc0/wheels/5f/b7/4e/f9145db9bbbc2f9bd6d7c671f5078258219e4b9eaf9d320765\n",
            "Successfully built ktransformers\n",
            "Installing collected packages: ktransformers\n",
            "  changing mode of /usr/local/bin/ktransformers to 755\n",
            "Successfully installed ktransformers-0.3.1+cu125torch26fancy\n",
            "Installing custom_flashinfer for CUDA backend\n",
            "Processing ./third_party/custom_flashinfer\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from flashinfer-python==0.2.3) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flashinfer-python==0.2.3) (2.6.0+cu124)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from flashinfer-python==0.2.3) (1.11.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flashinfer-python==0.2.3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flashinfer-python==0.2.3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flashinfer-python==0.2.3) (3.0.2)\n",
            "Building wheels for collected packages: flashinfer-python\n",
            "  Building wheel for flashinfer-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashinfer-python: filename=flashinfer_python-0.2.3-py3-none-any.whl size=3165547 sha256=97f61cf55a7f33c9c9d1b00d455b2353c12e599c6150d30e8af38a358417f651\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-olgtjvtl/wheels/ac/ca/e2/57d75be767d1d4ad30e954122867935ba1eb7009e978098bdb\n",
            "Successfully built flashinfer-python\n",
            "Installing collected packages: flashinfer-python\n",
            "Successfully installed flashinfer-python-0.2.3\n",
            "Installation completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x /content/ktransformers/install.sh\n",
        "!/content/ktransformers/install.sh"
      ],
      "metadata": {
        "id": "eFRTdfhtYIps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!get https://huggingface.co/mradermacher/DeepSeek-V2-Lite-GGUF/resolve/main/DeepSeek-V2-Lite.Q4_K_M.gguf -O DeepSeek-V2-Lite-Chat.Q4_K_M.gguf"
      ],
      "metadata": {
        "id": "6tccdYevbZ3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers/1\n",
        "!wget https://huggingface.co/bartowski/Mixtral-8x22B-v0.1-GGUF/resolve/main/Mixtral-8x22B-v0.1-Q2_K-00001-of-00005.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2BtlCS3boZ0",
        "outputId": "ed95929d-06af-479e-f739-3ff4fd51d235"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ktransformers/1\n",
            "--2025-06-19 23:04:44--  https://huggingface.co/bartowski/Mixtral-8x22B-v0.1-GGUF/resolve/main/Mixtral-8x22B-v0.1-Q2_K-00001-of-00005.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.118, 18.164.174.23, 18.164.174.55, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/661769999f952b376d7725d3/80a33bb87ed05503690e6b88085e2123cdd06fa5985d3b0b4702922d57611849?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250619T230445Z&X-Amz-Expires=3600&X-Amz-Signature=dbe2f12e63aaeb725921199c0ef43c8aa2b2d444cd4a171d59420d0d5d516f51&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mixtral-8x22B-v0.1-Q2_K-00001-of-00005.gguf%3B+filename%3D%22Mixtral-8x22B-v0.1-Q2_K-00001-of-00005.gguf%22%3B&x-id=GetObject&Expires=1750377885&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDM3Nzg4NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjE3Njk5OTlmOTUyYjM3NmQ3NzI1ZDMvODBhMzNiYjg3ZWQwNTUwMzY5MGU2Yjg4MDg1ZTIxMjNjZGQwNmZhNTk4NWQzYjBiNDcwMjkyMmQ1NzYxMTg0OSoifV19&Signature=m05UHUmoSqaWqvAJ9jVP-ukdYrivQsb6nWbe1JTn933WTewfYoV2mUY5F5Rir%7ElELMLaXGl3dIPMRacmiDytuvBLXhjKDd%7EWZE8Smy1KjkaWmobAQ1ign2hjnQ2DdF5vWO-RL3ciQ-A31QOTGv4PBGax2CLI4eAh4EPGMNc5nB4ULqiFjCJvEpDx5lyLwM5zecqJ2hAOkC6FTtnZaRqNgbIv4cFC5WRtFj5YITu8MRigTLlaetheCkDliazJIgfEf8VADkNreRaBRaCF3TYMgAlbQO%7ELNUbryUnRLKtxwsTrFIJURJJ%7E0PQqgAQDSlrsQIALP001ne7ZrJvLa1Urxw__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-06-19 23:04:45--  https://cas-bridge.xethub.hf.co/xet-bridge-us/661769999f952b376d7725d3/80a33bb87ed05503690e6b88085e2123cdd06fa5985d3b0b4702922d57611849?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250619T230445Z&X-Amz-Expires=3600&X-Amz-Signature=dbe2f12e63aaeb725921199c0ef43c8aa2b2d444cd4a171d59420d0d5d516f51&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mixtral-8x22B-v0.1-Q2_K-00001-of-00005.gguf%3B+filename%3D%22Mixtral-8x22B-v0.1-Q2_K-00001-of-00005.gguf%22%3B&x-id=GetObject&Expires=1750377885&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDM3Nzg4NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjE3Njk5OTlmOTUyYjM3NmQ3NzI1ZDMvODBhMzNiYjg3ZWQwNTUwMzY5MGU2Yjg4MDg1ZTIxMjNjZGQwNmZhNTk4NWQzYjBiNDcwMjkyMmQ1NzYxMTg0OSoifV19&Signature=m05UHUmoSqaWqvAJ9jVP-ukdYrivQsb6nWbe1JTn933WTewfYoV2mUY5F5Rir%7ElELMLaXGl3dIPMRacmiDytuvBLXhjKDd%7EWZE8Smy1KjkaWmobAQ1ign2hjnQ2DdF5vWO-RL3ciQ-A31QOTGv4PBGax2CLI4eAh4EPGMNc5nB4ULqiFjCJvEpDx5lyLwM5zecqJ2hAOkC6FTtnZaRqNgbIv4cFC5WRtFj5YITu8MRigTLlaetheCkDliazJIgfEf8VADkNreRaBRaCF3TYMgAlbQO%7ELNUbryUnRLKtxwsTrFIJURJJ%7E0PQqgAQDSlrsQIALP001ne7ZrJvLa1Urxw__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.164.174.4, 18.164.174.110, 18.164.174.68, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.164.174.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11843728352 (11G)\n",
            "Saving to: ‘Mixtral-8x22B-v0.1-Q2_K-00001-of-00005.gguf’\n",
            "\n",
            "Mixtral-8x22B-v0.1- 100%[===================>]  11.03G  41.7MB/s    in 3m 4s   \n",
            "\n",
            "2025-06-19 23:07:49 (61.5 MB/s) - ‘Mixtral-8x22B-v0.1-Q2_K-00001-of-00005.gguf’ saved [11843728352/11843728352]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/Mixtral-8x22B-v0.1-GGUF/resolve/main/Mixtral-8x22B-v0.1-Q2_K-00002-of-00005.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAM2b0lcb1lu",
        "outputId": "94f4623d-01f8-4a76-c193-3998b071b3be"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-19 23:07:49--  https://huggingface.co/bartowski/Mixtral-8x22B-v0.1-GGUF/resolve/main/Mixtral-8x22B-v0.1-Q2_K-00002-of-00005.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.23, 18.164.174.118, 18.164.174.55, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/661769999f952b376d7725d3/39f53fb4de7219a45caf17d9d34d5f54b1e2aeea9e63bdf32434badc84a15eef?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250619T230749Z&X-Amz-Expires=3600&X-Amz-Signature=da69073e9e1803f6e2c46ad12fb9dfb5b3943a9f74590553ca23547059fa4b2e&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mixtral-8x22B-v0.1-Q2_K-00002-of-00005.gguf%3B+filename%3D%22Mixtral-8x22B-v0.1-Q2_K-00002-of-00005.gguf%22%3B&x-id=GetObject&Expires=1750378069&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDM3ODA2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjE3Njk5OTlmOTUyYjM3NmQ3NzI1ZDMvMzlmNTNmYjRkZTcyMTlhNDVjYWYxN2Q5ZDM0ZDVmNTRiMWUyYWVlYTllNjNiZGYzMjQzNGJhZGM4NGExNWVlZioifV19&Signature=SFa5UAsW2YiWTJ4Ab%7E-eGA5p-R8tgc05nCa-aDVHcUJbH56p1tUlrA7c50SfwdQ8Q%7EAVOaNUqz5TZqJ-YO0Imvh2rpsH4KxI5OfzrpaGMezevjWVSZPVcmWGxOCV5%7E9AkutXc6ClYuIT6dzqL5%7E2Mce9zYcXQ-xEHGxUZdm6n2Idk8NQ-NROkxNwSpc8nskxrDF%7EfTLFPsji7HgQtTVWcSqSOw3nCEV%7EdhkHziHA9UWTW92bAyc5dnylWjHmRIv35cLs-8164reJDvAr83ZSMop6J1mrdjRRbjwz97nhnY04WMk74-CecCQ9zRMKKTmwNbjUnwHo2jaRqgei4WwNOQ__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-06-19 23:07:49--  https://cas-bridge.xethub.hf.co/xet-bridge-us/661769999f952b376d7725d3/39f53fb4de7219a45caf17d9d34d5f54b1e2aeea9e63bdf32434badc84a15eef?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250619T230749Z&X-Amz-Expires=3600&X-Amz-Signature=da69073e9e1803f6e2c46ad12fb9dfb5b3943a9f74590553ca23547059fa4b2e&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mixtral-8x22B-v0.1-Q2_K-00002-of-00005.gguf%3B+filename%3D%22Mixtral-8x22B-v0.1-Q2_K-00002-of-00005.gguf%22%3B&x-id=GetObject&Expires=1750378069&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDM3ODA2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjE3Njk5OTlmOTUyYjM3NmQ3NzI1ZDMvMzlmNTNmYjRkZTcyMTlhNDVjYWYxN2Q5ZDM0ZDVmNTRiMWUyYWVlYTllNjNiZGYzMjQzNGJhZGM4NGExNWVlZioifV19&Signature=SFa5UAsW2YiWTJ4Ab%7E-eGA5p-R8tgc05nCa-aDVHcUJbH56p1tUlrA7c50SfwdQ8Q%7EAVOaNUqz5TZqJ-YO0Imvh2rpsH4KxI5OfzrpaGMezevjWVSZPVcmWGxOCV5%7E9AkutXc6ClYuIT6dzqL5%7E2Mce9zYcXQ-xEHGxUZdm6n2Idk8NQ-NROkxNwSpc8nskxrDF%7EfTLFPsji7HgQtTVWcSqSOw3nCEV%7EdhkHziHA9UWTW92bAyc5dnylWjHmRIv35cLs-8164reJDvAr83ZSMop6J1mrdjRRbjwz97nhnY04WMk74-CecCQ9zRMKKTmwNbjUnwHo2jaRqgei4WwNOQ__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.164.174.68, 18.164.174.110, 18.164.174.21, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.164.174.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12267708608 (11G)\n",
            "Saving to: ‘Mixtral-8x22B-v0.1-Q2_K-00002-of-00005.gguf’\n",
            "\n",
            "Mixtral-8x22B-v0.1- 100%[===================>]  11.42G  77.2MB/s    in 3m 6s   \n",
            "\n",
            "2025-06-19 23:10:55 (63.0 MB/s) - ‘Mixtral-8x22B-v0.1-Q2_K-00002-of-00005.gguf’ saved [12267708608/12267708608]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/Mixtral-8x22B-v0.1-GGUF/resolve/main/Mixtral-8x22B-v0.1-Q2_K-00003-of-00005.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5-RguxqcGpE",
        "outputId": "ca3b80ca-8894-4aad-e47a-2fb846af2ca1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-19 23:10:55--  https://huggingface.co/bartowski/Mixtral-8x22B-v0.1-GGUF/resolve/main/Mixtral-8x22B-v0.1-Q2_K-00003-of-00005.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.118, 18.164.174.55, 18.164.174.17, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/661769999f952b376d7725d3/951e7ba87aff336bbda772bf66e7bc5f7a3eb8ad28f6d3715342671cbd620cef?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250619T231055Z&X-Amz-Expires=3600&X-Amz-Signature=36d97c6f09f0671bde8153632f816cba1c0f611c6cea5e007f056d06ad213b10&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mixtral-8x22B-v0.1-Q2_K-00003-of-00005.gguf%3B+filename%3D%22Mixtral-8x22B-v0.1-Q2_K-00003-of-00005.gguf%22%3B&x-id=GetObject&Expires=1750378255&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDM3ODI1NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjE3Njk5OTlmOTUyYjM3NmQ3NzI1ZDMvOTUxZTdiYTg3YWZmMzM2YmJkYTc3MmJmNjZlN2JjNWY3YTNlYjhhZDI4ZjZkMzcxNTM0MjY3MWNiZDYyMGNlZioifV19&Signature=amLvZuFTgRUYN1MyVLZIRhxg8Gd%7Efm4hCUdCEJXucPuFRDqBNKdGioLj%7ECTbZRrt8GJ7jkq2Wq6H2XbUX9oo92pqsHRSfhPHg4qzVpstSZFdEMx4eoedpYLU88Lq9rf8V6kah5LGP-eID72OQIQr1C0mTJVEVzDhpYWRgH5C4RVOxzqRkh-6S%7EBUTwf3g6PYA0BbzVmFoxTstQ0ZnqBzOs9gJCSIxjzGfFeb6TESpdk179D5QAH5h5gO6C-5YPMLfCUONfD7v%7E5YJ8rENwSBPfqr4-WzB8ipEjl2rF4PuXBnQBCnagM1ZAH-pHHvLFolGjRc50iGVD4-3p7dBmwKzA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-06-19 23:10:55--  https://cas-bridge.xethub.hf.co/xet-bridge-us/661769999f952b376d7725d3/951e7ba87aff336bbda772bf66e7bc5f7a3eb8ad28f6d3715342671cbd620cef?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250619T231055Z&X-Amz-Expires=3600&X-Amz-Signature=36d97c6f09f0671bde8153632f816cba1c0f611c6cea5e007f056d06ad213b10&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mixtral-8x22B-v0.1-Q2_K-00003-of-00005.gguf%3B+filename%3D%22Mixtral-8x22B-v0.1-Q2_K-00003-of-00005.gguf%22%3B&x-id=GetObject&Expires=1750378255&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDM3ODI1NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjE3Njk5OTlmOTUyYjM3NmQ3NzI1ZDMvOTUxZTdiYTg3YWZmMzM2YmJkYTc3MmJmNjZlN2JjNWY3YTNlYjhhZDI4ZjZkMzcxNTM0MjY3MWNiZDYyMGNlZioifV19&Signature=amLvZuFTgRUYN1MyVLZIRhxg8Gd%7Efm4hCUdCEJXucPuFRDqBNKdGioLj%7ECTbZRrt8GJ7jkq2Wq6H2XbUX9oo92pqsHRSfhPHg4qzVpstSZFdEMx4eoedpYLU88Lq9rf8V6kah5LGP-eID72OQIQr1C0mTJVEVzDhpYWRgH5C4RVOxzqRkh-6S%7EBUTwf3g6PYA0BbzVmFoxTstQ0ZnqBzOs9gJCSIxjzGfFeb6TESpdk179D5QAH5h5gO6C-5YPMLfCUONfD7v%7E5YJ8rENwSBPfqr4-WzB8ipEjl2rF4PuXBnQBCnagM1ZAH-pHHvLFolGjRc50iGVD4-3p7dBmwKzA__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.164.174.68, 18.164.174.110, 18.164.174.21, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.164.174.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11188527232 (10G)\n",
            "Saving to: ‘Mixtral-8x22B-v0.1-Q2_K-00003-of-00005.gguf’\n",
            "\n",
            "Mixtral-8x22B-v0.1- 100%[===================>]  10.42G  53.0MB/s    in 2m 51s  \n",
            "\n",
            "2025-06-19 23:13:47 (62.3 MB/s) - ‘Mixtral-8x22B-v0.1-Q2_K-00003-of-00005.gguf’ saved [11188527232/11188527232]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/Mixtral-8x22B-v0.1-GGUF/resolve/main/Mixtral-8x22B-v0.1-Q2_K-00004-of-00005.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QdwCSsPcIgT",
        "outputId": "361223f1-3c30-4336-a9b2-c7af4aef5e69"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-19 23:13:47--  https://huggingface.co/bartowski/Mixtral-8x22B-v0.1-GGUF/resolve/main/Mixtral-8x22B-v0.1-Q2_K-00004-of-00005.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.17, 18.164.174.55, 18.164.174.23, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/661769999f952b376d7725d3/471e9d184dab6ca4dc9e53955052c851d8e620563159ac2a3ed53321b3603ded?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250619T231347Z&X-Amz-Expires=3600&X-Amz-Signature=c41c8067c0a7a055e8009806a39cd09361237c84dcdc1e7e05145eb420f3fd1f&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mixtral-8x22B-v0.1-Q2_K-00004-of-00005.gguf%3B+filename%3D%22Mixtral-8x22B-v0.1-Q2_K-00004-of-00005.gguf%22%3B&x-id=GetObject&Expires=1750378427&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDM3ODQyN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjE3Njk5OTlmOTUyYjM3NmQ3NzI1ZDMvNDcxZTlkMTg0ZGFiNmNhNGRjOWU1Mzk1NTA1MmM4NTFkOGU2MjA1NjMxNTlhYzJhM2VkNTMzMjFiMzYwM2RlZCoifV19&Signature=OOAql2TUPwwWh0Qw2Ol3sur-shkKceBmaHmUhqUdsxLtqBpS0i4ynJ83V6N7MiRrjxgOLmT4KvfCP0u9dsfLj9kcsIFLswoIhaKud0nr6UserseizSOhMAqbTPw68EEg9Ex8-slE%7EFQDpWlxGSz1DAmY8VTW3U1bR2aELJb8OlosBAS0zvCDLaMW03zKvxl7qhBqbi5OAbI6xon51Dgu%7ETO96nPncOzwg0dc50ltc6S2zHBx9m9cYk2lIaH8VVy05k3ISi9wkebDx%7EDvskD4T61GN8ZTRHvdAq3nAH3tjOeh35Yn-4XByaIVBQJCAu-ngTxQlnGhBUnpOEL1DjQuRQ__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-06-19 23:13:47--  https://cas-bridge.xethub.hf.co/xet-bridge-us/661769999f952b376d7725d3/471e9d184dab6ca4dc9e53955052c851d8e620563159ac2a3ed53321b3603ded?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250619T231347Z&X-Amz-Expires=3600&X-Amz-Signature=c41c8067c0a7a055e8009806a39cd09361237c84dcdc1e7e05145eb420f3fd1f&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mixtral-8x22B-v0.1-Q2_K-00004-of-00005.gguf%3B+filename%3D%22Mixtral-8x22B-v0.1-Q2_K-00004-of-00005.gguf%22%3B&x-id=GetObject&Expires=1750378427&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDM3ODQyN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjE3Njk5OTlmOTUyYjM3NmQ3NzI1ZDMvNDcxZTlkMTg0ZGFiNmNhNGRjOWU1Mzk1NTA1MmM4NTFkOGU2MjA1NjMxNTlhYzJhM2VkNTMzMjFiMzYwM2RlZCoifV19&Signature=OOAql2TUPwwWh0Qw2Ol3sur-shkKceBmaHmUhqUdsxLtqBpS0i4ynJ83V6N7MiRrjxgOLmT4KvfCP0u9dsfLj9kcsIFLswoIhaKud0nr6UserseizSOhMAqbTPw68EEg9Ex8-slE%7EFQDpWlxGSz1DAmY8VTW3U1bR2aELJb8OlosBAS0zvCDLaMW03zKvxl7qhBqbi5OAbI6xon51Dgu%7ETO96nPncOzwg0dc50ltc6S2zHBx9m9cYk2lIaH8VVy05k3ISi9wkebDx%7EDvskD4T61GN8ZTRHvdAq3nAH3tjOeh35Yn-4XByaIVBQJCAu-ngTxQlnGhBUnpOEL1DjQuRQ__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.164.174.21, 18.164.174.68, 18.164.174.110, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.164.174.21|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12016738464 (11G)\n",
            "Saving to: ‘Mixtral-8x22B-v0.1-Q2_K-00004-of-00005.gguf’\n",
            "\n",
            "Mixtral-8x22B-v0.1- 100%[===================>]  11.19G  73.6MB/s    in 3m 5s   \n",
            "\n",
            "2025-06-19 23:16:53 (61.9 MB/s) - ‘Mixtral-8x22B-v0.1-Q2_K-00004-of-00005.gguf’ saved [12016738464/12016738464]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/Mixtral-8x22B-v0.1-GGUF/resolve/main/Mixtral-8x22B-v0.1-Q2_K-00005-of-00005.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KICT7ebpcNOM",
        "outputId": "fc2b0d2a-c42c-4893-cab4-12359d536d76"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-19 23:16:53--  https://huggingface.co/bartowski/Mixtral-8x22B-v0.1-GGUF/resolve/main/Mixtral-8x22B-v0.1-Q2_K-00005-of-00005.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.55, 18.164.174.17, 18.164.174.118, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/661769999f952b376d7725d3/401cfd75762bea16d0e765b2bcc1dc1afb6acf36eee65d002d5f7572c30e4bcf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250619T231653Z&X-Amz-Expires=3600&X-Amz-Signature=20257344a2a778be6644156e36859f441df9f215459705ed50503346511a8cf5&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mixtral-8x22B-v0.1-Q2_K-00005-of-00005.gguf%3B+filename%3D%22Mixtral-8x22B-v0.1-Q2_K-00005-of-00005.gguf%22%3B&x-id=GetObject&Expires=1750378613&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDM3ODYxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjE3Njk5OTlmOTUyYjM3NmQ3NzI1ZDMvNDAxY2ZkNzU3NjJiZWExNmQwZTc2NWIyYmNjMWRjMWFmYjZhY2YzNmVlZTY1ZDAwMmQ1Zjc1NzJjMzBlNGJjZioifV19&Signature=cS5unedkmBPh6r1-fncbm9YTen1Es54-%7EglvuoiTIgEXYt3RPC6TIXKobVOp99e%7Ej8UBZ%7Eohm2he15ZVdMBFDmeAyxrpXmS3xCjADPQy0SpRe9ocrnF5Kw5k3IvLnVKZTa0dec4o0hoGpMH7EQF4QBTOHlWr1QdMZhDYb3jjYluXkBJkp8iSUo8p0NLdxrwWyrTDx-qsgvk41SWoCyn6ezTShlpPXqGcYv-Bz-UPDtpdWcy%7Eh3WnLkjaRydxEyhtGd9HeJ4EgTWxnjVO0eG1hBpG0eILlFdcrYOaA5wQw9O-At1OTU2cFuuMXDpASCK-a78bLdN9YHAqm2w%7E1R5atw__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-06-19 23:16:53--  https://cas-bridge.xethub.hf.co/xet-bridge-us/661769999f952b376d7725d3/401cfd75762bea16d0e765b2bcc1dc1afb6acf36eee65d002d5f7572c30e4bcf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250619%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250619T231653Z&X-Amz-Expires=3600&X-Amz-Signature=20257344a2a778be6644156e36859f441df9f215459705ed50503346511a8cf5&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Mixtral-8x22B-v0.1-Q2_K-00005-of-00005.gguf%3B+filename%3D%22Mixtral-8x22B-v0.1-Q2_K-00005-of-00005.gguf%22%3B&x-id=GetObject&Expires=1750378613&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDM3ODYxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjE3Njk5OTlmOTUyYjM3NmQ3NzI1ZDMvNDAxY2ZkNzU3NjJiZWExNmQwZTc2NWIyYmNjMWRjMWFmYjZhY2YzNmVlZTY1ZDAwMmQ1Zjc1NzJjMzBlNGJjZioifV19&Signature=cS5unedkmBPh6r1-fncbm9YTen1Es54-%7EglvuoiTIgEXYt3RPC6TIXKobVOp99e%7Ej8UBZ%7Eohm2he15ZVdMBFDmeAyxrpXmS3xCjADPQy0SpRe9ocrnF5Kw5k3IvLnVKZTa0dec4o0hoGpMH7EQF4QBTOHlWr1QdMZhDYb3jjYluXkBJkp8iSUo8p0NLdxrwWyrTDx-qsgvk41SWoCyn6ezTShlpPXqGcYv-Bz-UPDtpdWcy%7Eh3WnLkjaRydxEyhtGd9HeJ4EgTWxnjVO0eG1hBpG0eILlFdcrYOaA5wQw9O-At1OTU2cFuuMXDpASCK-a78bLdN9YHAqm2w%7E1R5atw__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.164.174.4, 18.164.174.21, 18.164.174.68, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.164.174.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4786462016 (4.5G)\n",
            "Saving to: ‘Mixtral-8x22B-v0.1-Q2_K-00005-of-00005.gguf’\n",
            "\n",
            "Mixtral-8x22B-v0.1- 100%[===================>]   4.46G  68.3MB/s    in 1m 41s  \n",
            "\n",
            "2025-06-19 23:18:35 (45.0 MB/s) - ‘Mixtral-8x22B-v0.1-Q2_K-00005-of-00005.gguf’ saved [4786462016/4786462016]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m ktransformers.local_chat --model_path mistralai/Mixtral-8x22B-v0.1 --gguf_path ./1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E2GI56vfQk1",
        "outputId": "df004110-95ab-497a-b580-3989d6411834"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-19 23:20:40,507 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-19 23:20:42.263390: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750375242.284110   25120 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750375242.290448   25120 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-19 23:20:42.311324: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "tokenizer_config.json: 100% 996/996 [00:00<00:00, 6.42MB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 13.2MB/s]\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 11.8MB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 2.41MB/s]\n",
            "config.json: 100% 714/714 [00:00<00:00, 4.46MB/s]\n",
            "using custom modeling_xxx.py.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 196, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 100, in local_chat\n",
            "    model = custom_models[config.architectures[0]](config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\", line 1341, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1868, in __init__\n",
            "    config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
            "    cls._check_and_enable_flash_attn_2(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2252, in _check_and_enable_flash_attn_2\n",
            "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
            "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeebZ33gflB9",
        "outputId": "33e477d0-34e2-4b60-c3c4-13cc8ecea1d1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m ktransformers.local_chat -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DloofFVFflz1",
        "outputId": "7679a535-e650-4729-83e9-27ff11cacd2d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-19 23:21:14,070 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-19 23:21:15.745543: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750375275.765571   25331 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750375275.771522   25331 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-19 23:21:15.791989: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO: Showing help with the command 'local_chat.py -- --help'.\n",
            "\n",
            "\u001b[1mNAME\u001b[0m\n",
            "    local_chat.py\n",
            "\n",
            "\u001b[1mSYNOPSIS\u001b[0m\n",
            "    local_chat.py <flags>\n",
            "\n",
            "\u001b[1mFLAGS\u001b[0m\n",
            "    --model_path=\u001b[4mMODEL_PATH\u001b[0m\n",
            "        Type: Optional[str | None]\n",
            "        Default: None\n",
            "    -o, --optimize_config_path=\u001b[4mOPTIMIZE_CONFIG_PATH\u001b[0m\n",
            "        Type: Optional[str]\n",
            "        Default: None\n",
            "    -g, --gguf_path=\u001b[4mGGUF_PATH\u001b[0m\n",
            "        Type: Optional[str | None]\n",
            "        Default: None\n",
            "    --max_new_tokens=\u001b[4mMAX_NEW_TOKENS\u001b[0m\n",
            "        Type: int\n",
            "        Default: 1000\n",
            "    --cpu_infer=\u001b[4mCPU_INFER\u001b[0m\n",
            "        Type: int\n",
            "        Default: -2\n",
            "    -u, --use_cuda_graph=\u001b[4mUSE_CUDA_GRAPH\u001b[0m\n",
            "        Type: bool\n",
            "        Default: True\n",
            "    -p, --prompt_file=\u001b[4mPROMPT_FILE\u001b[0m\n",
            "        Type: Optional[str | None]\n",
            "        Default: None\n",
            "    --mode=\u001b[4mMODE\u001b[0m\n",
            "        Type: str\n",
            "        Default: 'normal'\n",
            "    -f, --force_think=\u001b[4mFORCE_THINK\u001b[0m\n",
            "        Type: bool\n",
            "        Default: False\n",
            "    --chunk_size=\u001b[4mCHUNK_SIZE\u001b[0m\n",
            "        Type: int\n",
            "        Default: 8192\n",
            "    -d, --device=\u001b[4mDEVICE\u001b[0m\n",
            "        Type: str\n",
            "        Default: 'cuda'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "-mla 2 -fa 1 \\"
      ],
      "metadata": {
        "id": "SgCTs9O6iuwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TOG0Me7akp7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m ktransformers.local_chat --model_path mistralai/Mixtral-8x22B-v0.1 --gguf_path ./1 --max_new_tokens 1 --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml --device cpu use_cuda_graph false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73RYQX5afwet",
        "outputId": "9f8f7109-4422-443e-f78d-008ada65ae7f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-19 23:38:37,951 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-19 23:38:39.625326: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750376319.646708   31725 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750376319.652937   31725 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-19 23:38:39.673273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 196, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 100, in local_chat\n",
            "    model = custom_models[config.architectures[0]](config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\", line 1341, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1868, in __init__\n",
            "    config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
            "    cls._check_and_enable_flash_attn_2(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2252, in _check_and_enable_flash_attn_2\n",
            "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
            "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6745e75d",
        "outputId": "08f345a9-b377-40ab-b669-1f0e0a542993"
      },
      "source": [
        "# Modify the model loading to explicitly disable FlashAttention\n",
        "from transformers import AutoModelForCausalLM, AutoConfig\n",
        "\n",
        "model_path = \"mistralai/Mixtral-8x22B-v0.1\"\n",
        "config = AutoConfig.from_pretrained(model_path)\n",
        "\n",
        "# Disable FlashAttention in the config\n",
        "config._attn_implementation = \"eager\"\n",
        "\n",
        "# Now load the model with the modified config\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_path, config=config)\n",
        "\n",
        "# Note: You will need to integrate this into the ktransformers loading process\n",
        "# based on how ktransformers handles model loading and configuration.\n",
        "# The ktransformers.local_chat script might need modification to pass this config."
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zxybgPGil8N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yQbwagqol8Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qvN5bI4Nl8JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZW4BlT7Ql8Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m ktransformers.local_chat --model_path mistralai/Mixtral-8x22B-v0.1 --gguf_path ./1 --max_new_tokens 1 --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml --device cpu use_cuda_graph false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWOoiuYGl8Bv",
        "outputId": "9864d2fd-dcb9-4d56-908d-4e095544b8f6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-19 23:55:27,280 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-19 23:55:28.969030: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750377328.990597   37912 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750377328.997081   37912 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-19 23:55:29.018844: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 196, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 100, in local_chat\n",
            "    model = custom_models[config.architectures[0]](config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\", line 1341, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1868, in __init__\n",
            "    config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
            "    cls._check_and_enable_flash_attn_2(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2252, in _check_and_enable_flash_attn_2\n",
            "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
            "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers\n",
        "!python -m ktransformers.local_chat --model_path mistralai/Mixtral-8x22B-v0.1 --gguf_path ./1 --max_new_tokens 1 --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml --device cpu use_cuda_graph false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5V_ch64nszw",
        "outputId": "294bc97b-741a-41e4-b729-67bcfb7cb606"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ktransformers\n",
            "no balance_serve\n",
            "2025-06-19 23:56:09,928 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-19 23:56:11.953903: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750377371.979232   38237 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750377371.989244   38237 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-19 23:56:12.014724: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Injecting model as default\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as default\n",
            "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 188, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 115, in local_chat\n",
            "    optimize_and_load_gguf(model, optimize_config_path, gguf_path, config)\n",
            "  File \"/content/ktransformers/ktransformers/optimize/optimize.py\", line 128, in optimize_and_load_gguf\n",
            "    inject(module, optimize_config, model_config, weights_loader)\n",
            "  File \"/content/ktransformers/ktransformers/optimize/optimize.py\", line 42, in inject\n",
            "    inject(child, child_optimization_dict, model_config, gguf_loader, child_prefix)\n",
            "  File \"/content/ktransformers/ktransformers/optimize/optimize.py\", line 42, in inject\n",
            "    inject(child, child_optimization_dict, model_config, gguf_loader, child_prefix)\n",
            "  File \"/content/ktransformers/ktransformers/optimize/optimize.py\", line 42, in inject\n",
            "    inject(child, child_optimization_dict, model_config, gguf_loader, child_prefix)\n",
            "  File \"/content/ktransformers/ktransformers/optimize/optimize.py\", line 31, in inject\n",
            "    module_cls=getattr(__import__(import_module_name, fromlist=[\"\"]), import_class_name)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/ktransformers/operators/experts.py\", line 134, in <module>\n",
            "    class KExpertsCPU(KExpertsBase):\n",
            "  File \"/content/ktransformers/ktransformers/operators/experts.py\", line 142, in KExpertsCPU\n",
            "    CPU_INFER = CPUInfer(Config().cpu_infer)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/ktransformers/operators/cpuinfer.py\", line 733, in __init__\n",
            "    if thread_num > CPUInfer.cur_backend_thread_num:\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: '>' not supported between instances of 'str' and 'int'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m ktransformers.local_chat --model_path mistralai/Mixtral-8x22B-v0.1 --gguf_path ./1 --max_new_tokens 1 --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml --device cpu --use_cuda_graph=false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRGs-eSZnwOw",
        "outputId": "4763997b-aff3-4f9f-da40-886233c398bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-19 23:57:28,291 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-19 23:57:30.224671: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750377450.258710   38729 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750377450.268811   38729 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-19 23:57:30.300456: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Injecting model as default\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as default\n",
            "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.0.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.0.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as default\n",
            "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.1.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.1.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as default\n",
            "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.2.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.2.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as default\n",
            "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.3.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.3.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as default\n",
            "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.4.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.4.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as default\n",
            "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.5.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.5.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as default\n",
            "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.6.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.6.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as default\n",
            "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.7.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.7.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as default\n",
            "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.8.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.8.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as default\n",
            "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.9.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.9.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as default\n",
            "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.10.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.10.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as default\n",
            "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.11.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.11.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as default\n",
            "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.12.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.12.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as default\n",
            "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.13.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.13.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as default\n",
            "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.14.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.14.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as default\n",
            "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.15.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.15.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as default\n",
            "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.16.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.16.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as default\n",
            "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.17.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.17.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as default\n",
            "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.18.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.18.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as default\n",
            "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.19.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.19.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as default\n",
            "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.20.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.20.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as default\n",
            "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.21.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.21.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as default\n",
            "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.22.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.22.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as default\n",
            "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.23.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.23.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as default\n",
            "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.24.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.24.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as default\n",
            "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.25.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.25.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as default\n",
            "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.26.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.26.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.layers.27 as default\n",
            "Injecting model.layers.27.self_attn as default\n",
            "Injecting model.layers.27.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.27.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.27.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.27.input_layernorm as default\n",
            "Injecting model.layers.27.post_attention_layernorm as default\n",
            "Injecting model.layers.28 as default\n",
            "Injecting model.layers.28.self_attn as default\n",
            "Injecting model.layers.28.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.28.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.28.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.28.input_layernorm as default\n",
            "Injecting model.layers.28.post_attention_layernorm as default\n",
            "Injecting model.layers.29 as default\n",
            "Injecting model.layers.29.self_attn as default\n",
            "Injecting model.layers.29.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.29.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.29.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.29.input_layernorm as default\n",
            "Injecting model.layers.29.post_attention_layernorm as default\n",
            "Injecting model.layers.30 as default\n",
            "Injecting model.layers.30.self_attn as default\n",
            "Injecting model.layers.30.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.30.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.30.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.30.input_layernorm as default\n",
            "Injecting model.layers.30.post_attention_layernorm as default\n",
            "Injecting model.layers.31 as default\n",
            "Injecting model.layers.31.self_attn as default\n",
            "Injecting model.layers.31.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.31.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.31.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.31.input_layernorm as default\n",
            "Injecting model.layers.31.post_attention_layernorm as default\n",
            "Injecting model.layers.32 as default\n",
            "Injecting model.layers.32.self_attn as default\n",
            "Injecting model.layers.32.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.32.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.32.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.32.input_layernorm as default\n",
            "Injecting model.layers.32.post_attention_layernorm as default\n",
            "Injecting model.layers.33 as default\n",
            "Injecting model.layers.33.self_attn as default\n",
            "Injecting model.layers.33.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.33.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.33.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.33.input_layernorm as default\n",
            "Injecting model.layers.33.post_attention_layernorm as default\n",
            "Injecting model.layers.34 as default\n",
            "Injecting model.layers.34.self_attn as default\n",
            "Injecting model.layers.34.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.34.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.34.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.34.input_layernorm as default\n",
            "Injecting model.layers.34.post_attention_layernorm as default\n",
            "Injecting model.layers.35 as default\n",
            "Injecting model.layers.35.self_attn as default\n",
            "Injecting model.layers.35.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.35.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.35.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.35.input_layernorm as default\n",
            "Injecting model.layers.35.post_attention_layernorm as default\n",
            "Injecting model.layers.36 as default\n",
            "Injecting model.layers.36.self_attn as default\n",
            "Injecting model.layers.36.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.36.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.36.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.36.input_layernorm as default\n",
            "Injecting model.layers.36.post_attention_layernorm as default\n",
            "Injecting model.layers.37 as default\n",
            "Injecting model.layers.37.self_attn as default\n",
            "Injecting model.layers.37.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.37.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.37.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.37.input_layernorm as default\n",
            "Injecting model.layers.37.post_attention_layernorm as default\n",
            "Injecting model.layers.38 as default\n",
            "Injecting model.layers.38.self_attn as default\n",
            "Injecting model.layers.38.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.38.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.38.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.38.input_layernorm as default\n",
            "Injecting model.layers.38.post_attention_layernorm as default\n",
            "Injecting model.layers.39 as default\n",
            "Injecting model.layers.39.self_attn as default\n",
            "Injecting model.layers.39.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.39.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.39.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.39.input_layernorm as default\n",
            "Injecting model.layers.39.post_attention_layernorm as default\n",
            "Injecting model.layers.40 as default\n",
            "Injecting model.layers.40.self_attn as default\n",
            "Injecting model.layers.40.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.40.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.40.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.40.input_layernorm as default\n",
            "Injecting model.layers.40.post_attention_layernorm as default\n",
            "Injecting model.layers.41 as default\n",
            "Injecting model.layers.41.self_attn as default\n",
            "Injecting model.layers.41.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.41.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.41.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.41.input_layernorm as default\n",
            "Injecting model.layers.41.post_attention_layernorm as default\n",
            "Injecting model.layers.42 as default\n",
            "Injecting model.layers.42.self_attn as default\n",
            "Injecting model.layers.42.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.42.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.42.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.42.input_layernorm as default\n",
            "Injecting model.layers.42.post_attention_layernorm as default\n",
            "Injecting model.layers.43 as default\n",
            "Injecting model.layers.43.self_attn as default\n",
            "Injecting model.layers.43.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.43.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.43.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.43.input_layernorm as default\n",
            "Injecting model.layers.43.post_attention_layernorm as default\n",
            "Injecting model.layers.44 as default\n",
            "Injecting model.layers.44.self_attn as default\n",
            "Injecting model.layers.44.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.44.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.44.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.44.input_layernorm as default\n",
            "Injecting model.layers.44.post_attention_layernorm as default\n",
            "Injecting model.layers.45 as default\n",
            "Injecting model.layers.45.self_attn as default\n",
            "Injecting model.layers.45.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.45.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.45.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.45.input_layernorm as default\n",
            "Injecting model.layers.45.post_attention_layernorm as default\n",
            "Injecting model.layers.46 as default\n",
            "Injecting model.layers.46.self_attn as default\n",
            "Injecting model.layers.46.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.46.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.46.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.46.input_layernorm as default\n",
            "Injecting model.layers.46.post_attention_layernorm as default\n",
            "Injecting model.layers.47 as default\n",
            "Injecting model.layers.47.self_attn as default\n",
            "Injecting model.layers.47.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.47.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.47.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.47.input_layernorm as default\n",
            "Injecting model.layers.47.post_attention_layernorm as default\n",
            "Injecting model.layers.48 as default\n",
            "Injecting model.layers.48.self_attn as default\n",
            "Injecting model.layers.48.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.48.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.48.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.48.input_layernorm as default\n",
            "Injecting model.layers.48.post_attention_layernorm as default\n",
            "Injecting model.layers.49 as default\n",
            "Injecting model.layers.49.self_attn as default\n",
            "Injecting model.layers.49.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.49.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.49.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.49.input_layernorm as default\n",
            "Injecting model.layers.49.post_attention_layernorm as default\n",
            "Injecting model.layers.50 as default\n",
            "Injecting model.layers.50.self_attn as default\n",
            "Injecting model.layers.50.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.50.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.50.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.50.input_layernorm as default\n",
            "Injecting model.layers.50.post_attention_layernorm as default\n",
            "Injecting model.layers.51 as default\n",
            "Injecting model.layers.51.self_attn as default\n",
            "Injecting model.layers.51.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.51.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.51.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.51.input_layernorm as default\n",
            "Injecting model.layers.51.post_attention_layernorm as default\n",
            "Injecting model.layers.52 as default\n",
            "Injecting model.layers.52.self_attn as default\n",
            "Injecting model.layers.52.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.52.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.52.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.52.input_layernorm as default\n",
            "Injecting model.layers.52.post_attention_layernorm as default\n",
            "Injecting model.layers.53 as default\n",
            "Injecting model.layers.53.self_attn as default\n",
            "Injecting model.layers.53.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.53.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.53.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.53.input_layernorm as default\n",
            "Injecting model.layers.53.post_attention_layernorm as default\n",
            "Injecting model.layers.54 as default\n",
            "Injecting model.layers.54.self_attn as default\n",
            "Injecting model.layers.54.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.54.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.54.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.54.input_layernorm as default\n",
            "Injecting model.layers.54.post_attention_layernorm as default\n",
            "Injecting model.layers.55 as default\n",
            "Injecting model.layers.55.self_attn as default\n",
            "Injecting model.layers.55.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.55.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.55.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.55.input_layernorm as default\n",
            "Injecting model.layers.55.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "loading model.embed_tokens.weight to cpu\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 188, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 115, in local_chat\n",
            "    optimize_and_load_gguf(model, optimize_config_path, gguf_path, config)\n",
            "  File \"/content/ktransformers/ktransformers/optimize/optimize.py\", line 131, in optimize_and_load_gguf\n",
            "    load_weights(module, weights_loader, device=default_device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/base_operator.py\", line 63, in load\n",
            "    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/experts.py\", line 684, in load\n",
            "    self.generate_experts.load(w, warmup=warmup)\n",
            "  File \"/content/ktransformers/ktransformers/operators/experts.py\", line 245, in load\n",
            "    self.cpu_infer.submit(self.moe.warm_up())\n",
            "  File \"/content/ktransformers/ktransformers/operators/cpuinfer.py\", line 739, in submit\n",
            "    CPUInfer.cpuinfer.submit(task)\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'submit'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x22B-v0.1 \\\n",
        "    --gguf_path ./1 \\\n",
        "    --max_new_tokens 1 \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "    --use_cuda_graph=False \\\n",
        "    --cpu_infer=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyBRzu8OoDfg",
        "outputId": "3413bede-cc0b-431c-e078-3addd9d78158"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-19 23:58:12,311 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-19 23:58:13.991620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750377494.013140   39003 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750377494.019158   39003 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-19 23:58:14.039848: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Injecting model as default\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as default\n",
            "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.0.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.0.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as default\n",
            "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.1.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.1.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as default\n",
            "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.2.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.2.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as default\n",
            "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.3.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.3.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as default\n",
            "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.4.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.4.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as default\n",
            "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.5.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.5.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as default\n",
            "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.6.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.6.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as default\n",
            "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.7.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.7.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as default\n",
            "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.8.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.8.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as default\n",
            "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.9.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.9.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as default\n",
            "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.10.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.10.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as default\n",
            "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.11.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.11.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as default\n",
            "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.12.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.12.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as default\n",
            "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.13.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.13.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as default\n",
            "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.14.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.14.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as default\n",
            "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.15.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.15.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as default\n",
            "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.16.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.16.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as default\n",
            "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.17.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.17.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as default\n",
            "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.18.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.18.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as default\n",
            "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.19.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.19.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as default\n",
            "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.20.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.20.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as default\n",
            "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.21.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.21.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as default\n",
            "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.22.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.22.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as default\n",
            "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.23.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.23.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as default\n",
            "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.24.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.24.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as default\n",
            "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.25.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.25.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as default\n",
            "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.26.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.26.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.layers.27 as default\n",
            "Injecting model.layers.27.self_attn as default\n",
            "Injecting model.layers.27.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.27.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.27.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.27.input_layernorm as default\n",
            "Injecting model.layers.27.post_attention_layernorm as default\n",
            "Injecting model.layers.28 as default\n",
            "Injecting model.layers.28.self_attn as default\n",
            "Injecting model.layers.28.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.28.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.28.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.28.input_layernorm as default\n",
            "Injecting model.layers.28.post_attention_layernorm as default\n",
            "Injecting model.layers.29 as default\n",
            "Injecting model.layers.29.self_attn as default\n",
            "Injecting model.layers.29.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.29.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.29.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.29.input_layernorm as default\n",
            "Injecting model.layers.29.post_attention_layernorm as default\n",
            "Injecting model.layers.30 as default\n",
            "Injecting model.layers.30.self_attn as default\n",
            "Injecting model.layers.30.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.30.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.30.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.30.input_layernorm as default\n",
            "Injecting model.layers.30.post_attention_layernorm as default\n",
            "Injecting model.layers.31 as default\n",
            "Injecting model.layers.31.self_attn as default\n",
            "Injecting model.layers.31.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.31.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.31.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.31.input_layernorm as default\n",
            "Injecting model.layers.31.post_attention_layernorm as default\n",
            "Injecting model.layers.32 as default\n",
            "Injecting model.layers.32.self_attn as default\n",
            "Injecting model.layers.32.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.32.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.32.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.32.input_layernorm as default\n",
            "Injecting model.layers.32.post_attention_layernorm as default\n",
            "Injecting model.layers.33 as default\n",
            "Injecting model.layers.33.self_attn as default\n",
            "Injecting model.layers.33.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.33.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.33.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.33.input_layernorm as default\n",
            "Injecting model.layers.33.post_attention_layernorm as default\n",
            "Injecting model.layers.34 as default\n",
            "Injecting model.layers.34.self_attn as default\n",
            "Injecting model.layers.34.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.34.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.34.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.34.input_layernorm as default\n",
            "Injecting model.layers.34.post_attention_layernorm as default\n",
            "Injecting model.layers.35 as default\n",
            "Injecting model.layers.35.self_attn as default\n",
            "Injecting model.layers.35.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.35.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.35.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.35.input_layernorm as default\n",
            "Injecting model.layers.35.post_attention_layernorm as default\n",
            "Injecting model.layers.36 as default\n",
            "Injecting model.layers.36.self_attn as default\n",
            "Injecting model.layers.36.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.36.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.36.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.36.input_layernorm as default\n",
            "Injecting model.layers.36.post_attention_layernorm as default\n",
            "Injecting model.layers.37 as default\n",
            "Injecting model.layers.37.self_attn as default\n",
            "Injecting model.layers.37.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.37.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.37.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.37.input_layernorm as default\n",
            "Injecting model.layers.37.post_attention_layernorm as default\n",
            "Injecting model.layers.38 as default\n",
            "Injecting model.layers.38.self_attn as default\n",
            "Injecting model.layers.38.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.38.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.38.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.38.input_layernorm as default\n",
            "Injecting model.layers.38.post_attention_layernorm as default\n",
            "Injecting model.layers.39 as default\n",
            "Injecting model.layers.39.self_attn as default\n",
            "Injecting model.layers.39.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.39.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.39.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.39.input_layernorm as default\n",
            "Injecting model.layers.39.post_attention_layernorm as default\n",
            "Injecting model.layers.40 as default\n",
            "Injecting model.layers.40.self_attn as default\n",
            "Injecting model.layers.40.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.40.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.40.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.40.input_layernorm as default\n",
            "Injecting model.layers.40.post_attention_layernorm as default\n",
            "Injecting model.layers.41 as default\n",
            "Injecting model.layers.41.self_attn as default\n",
            "Injecting model.layers.41.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.41.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.41.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.41.input_layernorm as default\n",
            "Injecting model.layers.41.post_attention_layernorm as default\n",
            "Injecting model.layers.42 as default\n",
            "Injecting model.layers.42.self_attn as default\n",
            "Injecting model.layers.42.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.42.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.42.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.42.input_layernorm as default\n",
            "Injecting model.layers.42.post_attention_layernorm as default\n",
            "Injecting model.layers.43 as default\n",
            "Injecting model.layers.43.self_attn as default\n",
            "Injecting model.layers.43.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.43.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.43.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.43.input_layernorm as default\n",
            "Injecting model.layers.43.post_attention_layernorm as default\n",
            "Injecting model.layers.44 as default\n",
            "Injecting model.layers.44.self_attn as default\n",
            "Injecting model.layers.44.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.44.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.44.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.44.input_layernorm as default\n",
            "Injecting model.layers.44.post_attention_layernorm as default\n",
            "Injecting model.layers.45 as default\n",
            "Injecting model.layers.45.self_attn as default\n",
            "Injecting model.layers.45.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.45.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.45.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.45.input_layernorm as default\n",
            "Injecting model.layers.45.post_attention_layernorm as default\n",
            "Injecting model.layers.46 as default\n",
            "Injecting model.layers.46.self_attn as default\n",
            "Injecting model.layers.46.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.46.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.46.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.46.input_layernorm as default\n",
            "Injecting model.layers.46.post_attention_layernorm as default\n",
            "Injecting model.layers.47 as default\n",
            "Injecting model.layers.47.self_attn as default\n",
            "Injecting model.layers.47.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.47.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.47.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.47.input_layernorm as default\n",
            "Injecting model.layers.47.post_attention_layernorm as default\n",
            "Injecting model.layers.48 as default\n",
            "Injecting model.layers.48.self_attn as default\n",
            "Injecting model.layers.48.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.48.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.48.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.48.input_layernorm as default\n",
            "Injecting model.layers.48.post_attention_layernorm as default\n",
            "Injecting model.layers.49 as default\n",
            "Injecting model.layers.49.self_attn as default\n",
            "Injecting model.layers.49.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.49.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.49.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.49.input_layernorm as default\n",
            "Injecting model.layers.49.post_attention_layernorm as default\n",
            "Injecting model.layers.50 as default\n",
            "Injecting model.layers.50.self_attn as default\n",
            "Injecting model.layers.50.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.50.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.50.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.50.input_layernorm as default\n",
            "Injecting model.layers.50.post_attention_layernorm as default\n",
            "Injecting model.layers.51 as default\n",
            "Injecting model.layers.51.self_attn as default\n",
            "Injecting model.layers.51.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.51.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.51.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.51.input_layernorm as default\n",
            "Injecting model.layers.51.post_attention_layernorm as default\n",
            "Injecting model.layers.52 as default\n",
            "Injecting model.layers.52.self_attn as default\n",
            "Injecting model.layers.52.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.52.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.52.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.52.input_layernorm as default\n",
            "Injecting model.layers.52.post_attention_layernorm as default\n",
            "Injecting model.layers.53 as default\n",
            "Injecting model.layers.53.self_attn as default\n",
            "Injecting model.layers.53.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.53.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.53.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.53.input_layernorm as default\n",
            "Injecting model.layers.53.post_attention_layernorm as default\n",
            "Injecting model.layers.54 as default\n",
            "Injecting model.layers.54.self_attn as default\n",
            "Injecting model.layers.54.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.54.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.54.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.54.input_layernorm as default\n",
            "Injecting model.layers.54.post_attention_layernorm as default\n",
            "Injecting model.layers.55 as default\n",
            "Injecting model.layers.55.self_attn as default\n",
            "Injecting model.layers.55.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.55.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.55.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.55.input_layernorm as default\n",
            "Injecting model.layers.55.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "loading model.embed_tokens.weight to cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x22B-v0.1 \\\n",
        "    --gguf_path ./1 \\\n",
        "    --max_new_tokens 1 \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "    --cpu_infer=1 \\\n",
        "    --prompt_file /content/1.txt \\\n",
        "    --use_cuda_graph=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afoksAvooOOw",
        "outputId": "4721a756-8c20-4d06-da1a-52b871c6c179"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:04:10,886 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:04:12.716938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750377852.749415   41214 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750377852.760133   41214 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:04:12.794138: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Injecting model as default\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as default\n",
            "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.0.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.0.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as default\n",
            "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.1.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.1.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as default\n",
            "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.2.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.2.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as default\n",
            "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.3.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.3.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as default\n",
            "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.4.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.4.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as default\n",
            "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.5.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.5.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as default\n",
            "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.6.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.6.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as default\n",
            "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.7.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.7.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as default\n",
            "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.8.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.8.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as default\n",
            "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.9.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.9.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as default\n",
            "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.10.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.10.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as default\n",
            "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.11.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.11.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as default\n",
            "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.12.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.12.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as default\n",
            "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.13.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.13.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as default\n",
            "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.14.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.14.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as default\n",
            "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.15.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.15.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as default\n",
            "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.16.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.16.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as default\n",
            "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.17.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.17.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as default\n",
            "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.18.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.18.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as default\n",
            "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.19.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.19.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as default\n",
            "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.20.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.20.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as default\n",
            "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.21.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.21.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as default\n",
            "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.22.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.22.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as default\n",
            "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.23.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.23.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as default\n",
            "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.24.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.24.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as default\n",
            "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.25.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.25.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as default\n",
            "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.26.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.26.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.layers.27 as default\n",
            "Injecting model.layers.27.self_attn as default\n",
            "Injecting model.layers.27.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.27.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.27.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.27.input_layernorm as default\n",
            "Injecting model.layers.27.post_attention_layernorm as default\n",
            "Injecting model.layers.28 as default\n",
            "Injecting model.layers.28.self_attn as default\n",
            "Injecting model.layers.28.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.28.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.28.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.28.input_layernorm as default\n",
            "Injecting model.layers.28.post_attention_layernorm as default\n",
            "Injecting model.layers.29 as default\n",
            "Injecting model.layers.29.self_attn as default\n",
            "Injecting model.layers.29.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.29.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.29.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.29.input_layernorm as default\n",
            "Injecting model.layers.29.post_attention_layernorm as default\n",
            "Injecting model.layers.30 as default\n",
            "Injecting model.layers.30.self_attn as default\n",
            "Injecting model.layers.30.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.30.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.30.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.30.input_layernorm as default\n",
            "Injecting model.layers.30.post_attention_layernorm as default\n",
            "Injecting model.layers.31 as default\n",
            "Injecting model.layers.31.self_attn as default\n",
            "Injecting model.layers.31.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.31.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.31.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.31.input_layernorm as default\n",
            "Injecting model.layers.31.post_attention_layernorm as default\n",
            "Injecting model.layers.32 as default\n",
            "Injecting model.layers.32.self_attn as default\n",
            "Injecting model.layers.32.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.32.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.32.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.32.input_layernorm as default\n",
            "Injecting model.layers.32.post_attention_layernorm as default\n",
            "Injecting model.layers.33 as default\n",
            "Injecting model.layers.33.self_attn as default\n",
            "Injecting model.layers.33.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.33.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.33.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.33.input_layernorm as default\n",
            "Injecting model.layers.33.post_attention_layernorm as default\n",
            "Injecting model.layers.34 as default\n",
            "Injecting model.layers.34.self_attn as default\n",
            "Injecting model.layers.34.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.34.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.34.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.34.input_layernorm as default\n",
            "Injecting model.layers.34.post_attention_layernorm as default\n",
            "Injecting model.layers.35 as default\n",
            "Injecting model.layers.35.self_attn as default\n",
            "Injecting model.layers.35.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.35.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.35.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.35.input_layernorm as default\n",
            "Injecting model.layers.35.post_attention_layernorm as default\n",
            "Injecting model.layers.36 as default\n",
            "Injecting model.layers.36.self_attn as default\n",
            "Injecting model.layers.36.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.36.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.36.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.36.input_layernorm as default\n",
            "Injecting model.layers.36.post_attention_layernorm as default\n",
            "Injecting model.layers.37 as default\n",
            "Injecting model.layers.37.self_attn as default\n",
            "Injecting model.layers.37.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.37.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.37.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.37.input_layernorm as default\n",
            "Injecting model.layers.37.post_attention_layernorm as default\n",
            "Injecting model.layers.38 as default\n",
            "Injecting model.layers.38.self_attn as default\n",
            "Injecting model.layers.38.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.38.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.38.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.38.input_layernorm as default\n",
            "Injecting model.layers.38.post_attention_layernorm as default\n",
            "Injecting model.layers.39 as default\n",
            "Injecting model.layers.39.self_attn as default\n",
            "Injecting model.layers.39.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.39.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.39.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.39.input_layernorm as default\n",
            "Injecting model.layers.39.post_attention_layernorm as default\n",
            "Injecting model.layers.40 as default\n",
            "Injecting model.layers.40.self_attn as default\n",
            "Injecting model.layers.40.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.40.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.40.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.40.input_layernorm as default\n",
            "Injecting model.layers.40.post_attention_layernorm as default\n",
            "Injecting model.layers.41 as default\n",
            "Injecting model.layers.41.self_attn as default\n",
            "Injecting model.layers.41.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.41.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.41.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.41.input_layernorm as default\n",
            "Injecting model.layers.41.post_attention_layernorm as default\n",
            "Injecting model.layers.42 as default\n",
            "Injecting model.layers.42.self_attn as default\n",
            "Injecting model.layers.42.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.42.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.42.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.42.input_layernorm as default\n",
            "Injecting model.layers.42.post_attention_layernorm as default\n",
            "Injecting model.layers.43 as default\n",
            "Injecting model.layers.43.self_attn as default\n",
            "Injecting model.layers.43.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.43.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.43.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.43.input_layernorm as default\n",
            "Injecting model.layers.43.post_attention_layernorm as default\n",
            "Injecting model.layers.44 as default\n",
            "Injecting model.layers.44.self_attn as default\n",
            "Injecting model.layers.44.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.44.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.44.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.44.input_layernorm as default\n",
            "Injecting model.layers.44.post_attention_layernorm as default\n",
            "Injecting model.layers.45 as default\n",
            "Injecting model.layers.45.self_attn as default\n",
            "Injecting model.layers.45.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.45.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.45.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.45.input_layernorm as default\n",
            "Injecting model.layers.45.post_attention_layernorm as default\n",
            "Injecting model.layers.46 as default\n",
            "Injecting model.layers.46.self_attn as default\n",
            "Injecting model.layers.46.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.46.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.46.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.46.input_layernorm as default\n",
            "Injecting model.layers.46.post_attention_layernorm as default\n",
            "Injecting model.layers.47 as default\n",
            "Injecting model.layers.47.self_attn as default\n",
            "Injecting model.layers.47.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.47.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.47.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.47.input_layernorm as default\n",
            "Injecting model.layers.47.post_attention_layernorm as default\n",
            "Injecting model.layers.48 as default\n",
            "Injecting model.layers.48.self_attn as default\n",
            "Injecting model.layers.48.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.48.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.48.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.48.input_layernorm as default\n",
            "Injecting model.layers.48.post_attention_layernorm as default\n",
            "Injecting model.layers.49 as default\n",
            "Injecting model.layers.49.self_attn as default\n",
            "Injecting model.layers.49.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.49.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.49.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.49.input_layernorm as default\n",
            "Injecting model.layers.49.post_attention_layernorm as default\n",
            "Injecting model.layers.50 as default\n",
            "Injecting model.layers.50.self_attn as default\n",
            "Injecting model.layers.50.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.50.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.50.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.50.input_layernorm as default\n",
            "Injecting model.layers.50.post_attention_layernorm as default\n",
            "Injecting model.layers.51 as default\n",
            "Injecting model.layers.51.self_attn as default\n",
            "Injecting model.layers.51.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.51.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.51.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.51.input_layernorm as default\n",
            "Injecting model.layers.51.post_attention_layernorm as default\n",
            "Injecting model.layers.52 as default\n",
            "Injecting model.layers.52.self_attn as default\n",
            "Injecting model.layers.52.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.52.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.52.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.52.input_layernorm as default\n",
            "Injecting model.layers.52.post_attention_layernorm as default\n",
            "Injecting model.layers.53 as default\n",
            "Injecting model.layers.53.self_attn as default\n",
            "Injecting model.layers.53.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.53.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.53.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.53.input_layernorm as default\n",
            "Injecting model.layers.53.post_attention_layernorm as default\n",
            "Injecting model.layers.54 as default\n",
            "Injecting model.layers.54.self_attn as default\n",
            "Injecting model.layers.54.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.54.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.54.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.54.input_layernorm as default\n",
            "Injecting model.layers.54.post_attention_layernorm as default\n",
            "Injecting model.layers.55 as default\n",
            "Injecting model.layers.55.self_attn as default\n",
            "Injecting model.layers.55.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.55.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.55.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.55.input_layernorm as default\n",
            "Injecting model.layers.55.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "loading model.embed_tokens.weight to cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "    --gguf_path /path/to/your/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "    --use_cuda_graph=False \\\n",
        "    --cpu_infer=1"
      ],
      "metadata": {
        "id": "JY_RNlggooq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "--prompt_file <your prompt txt file>"
      ],
      "metadata": {
        "id": "LvaZr_DCqo6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x22B-v0.1 \\\n",
        "    --gguf_path ./1 \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "    --prompt_file ./1.txt \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X615PGJCqsJB",
        "outputId": "8138a3b4-4b29-4c4d-c45b-0bb27dec494f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:09:12,742 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:09:14.485590: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750378154.506443   43078 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750378154.512898   43078 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:09:14.535455: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Injecting model as default\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as default\n",
            "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.0.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.0.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as default\n",
            "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.1.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.1.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as default\n",
            "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.2.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.2.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as default\n",
            "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.3.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.3.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as default\n",
            "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.4.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.4.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as default\n",
            "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.5.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.5.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as default\n",
            "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.6.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.6.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as default\n",
            "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.7.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.7.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as default\n",
            "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.8.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.8.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as default\n",
            "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.9.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.9.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as default\n",
            "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.10.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.10.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as default\n",
            "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.11.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.11.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as default\n",
            "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.12.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.12.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as default\n",
            "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.13.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.13.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as default\n",
            "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.14.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.14.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as default\n",
            "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.15.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.15.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as default\n",
            "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.16.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.16.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as default\n",
            "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.17.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.17.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as default\n",
            "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.18.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.18.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as default\n",
            "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.19.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.19.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as default\n",
            "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.20.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.20.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as default\n",
            "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.21.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.21.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as default\n",
            "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.22.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.22.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as default\n",
            "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.23.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.23.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as default\n",
            "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.24.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.24.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as default\n",
            "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.25.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.25.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as default\n",
            "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.26.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.26.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.layers.27 as default\n",
            "Injecting model.layers.27.self_attn as default\n",
            "Injecting model.layers.27.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.27.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.27.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.27.input_layernorm as default\n",
            "Injecting model.layers.27.post_attention_layernorm as default\n",
            "Injecting model.layers.28 as default\n",
            "Injecting model.layers.28.self_attn as default\n",
            "Injecting model.layers.28.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.28.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.28.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.28.input_layernorm as default\n",
            "Injecting model.layers.28.post_attention_layernorm as default\n",
            "Injecting model.layers.29 as default\n",
            "Injecting model.layers.29.self_attn as default\n",
            "Injecting model.layers.29.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.29.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.29.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.29.input_layernorm as default\n",
            "Injecting model.layers.29.post_attention_layernorm as default\n",
            "Injecting model.layers.30 as default\n",
            "Injecting model.layers.30.self_attn as default\n",
            "Injecting model.layers.30.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.30.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.30.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.30.input_layernorm as default\n",
            "Injecting model.layers.30.post_attention_layernorm as default\n",
            "Injecting model.layers.31 as default\n",
            "Injecting model.layers.31.self_attn as default\n",
            "Injecting model.layers.31.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.31.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.31.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.31.input_layernorm as default\n",
            "Injecting model.layers.31.post_attention_layernorm as default\n",
            "Injecting model.layers.32 as default\n",
            "Injecting model.layers.32.self_attn as default\n",
            "Injecting model.layers.32.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.32.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.32.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.32.input_layernorm as default\n",
            "Injecting model.layers.32.post_attention_layernorm as default\n",
            "Injecting model.layers.33 as default\n",
            "Injecting model.layers.33.self_attn as default\n",
            "Injecting model.layers.33.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.33.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.33.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.33.input_layernorm as default\n",
            "Injecting model.layers.33.post_attention_layernorm as default\n",
            "Injecting model.layers.34 as default\n",
            "Injecting model.layers.34.self_attn as default\n",
            "Injecting model.layers.34.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.34.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.34.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.34.input_layernorm as default\n",
            "Injecting model.layers.34.post_attention_layernorm as default\n",
            "Injecting model.layers.35 as default\n",
            "Injecting model.layers.35.self_attn as default\n",
            "Injecting model.layers.35.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.35.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.35.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.35.input_layernorm as default\n",
            "Injecting model.layers.35.post_attention_layernorm as default\n",
            "Injecting model.layers.36 as default\n",
            "Injecting model.layers.36.self_attn as default\n",
            "Injecting model.layers.36.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.36.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.36.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.36.input_layernorm as default\n",
            "Injecting model.layers.36.post_attention_layernorm as default\n",
            "Injecting model.layers.37 as default\n",
            "Injecting model.layers.37.self_attn as default\n",
            "Injecting model.layers.37.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.37.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.37.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.37.input_layernorm as default\n",
            "Injecting model.layers.37.post_attention_layernorm as default\n",
            "Injecting model.layers.38 as default\n",
            "Injecting model.layers.38.self_attn as default\n",
            "Injecting model.layers.38.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.38.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.38.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.38.input_layernorm as default\n",
            "Injecting model.layers.38.post_attention_layernorm as default\n",
            "Injecting model.layers.39 as default\n",
            "Injecting model.layers.39.self_attn as default\n",
            "Injecting model.layers.39.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.39.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.39.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.39.input_layernorm as default\n",
            "Injecting model.layers.39.post_attention_layernorm as default\n",
            "Injecting model.layers.40 as default\n",
            "Injecting model.layers.40.self_attn as default\n",
            "Injecting model.layers.40.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.40.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.40.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.40.input_layernorm as default\n",
            "Injecting model.layers.40.post_attention_layernorm as default\n",
            "Injecting model.layers.41 as default\n",
            "Injecting model.layers.41.self_attn as default\n",
            "Injecting model.layers.41.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.41.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.41.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.41.input_layernorm as default\n",
            "Injecting model.layers.41.post_attention_layernorm as default\n",
            "Injecting model.layers.42 as default\n",
            "Injecting model.layers.42.self_attn as default\n",
            "Injecting model.layers.42.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.42.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.42.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.42.input_layernorm as default\n",
            "Injecting model.layers.42.post_attention_layernorm as default\n",
            "Injecting model.layers.43 as default\n",
            "Injecting model.layers.43.self_attn as default\n",
            "Injecting model.layers.43.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.43.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.43.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.43.input_layernorm as default\n",
            "Injecting model.layers.43.post_attention_layernorm as default\n",
            "Injecting model.layers.44 as default\n",
            "Injecting model.layers.44.self_attn as default\n",
            "Injecting model.layers.44.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.44.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.44.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.44.input_layernorm as default\n",
            "Injecting model.layers.44.post_attention_layernorm as default\n",
            "Injecting model.layers.45 as default\n",
            "Injecting model.layers.45.self_attn as default\n",
            "Injecting model.layers.45.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.45.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.45.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.45.input_layernorm as default\n",
            "Injecting model.layers.45.post_attention_layernorm as default\n",
            "Injecting model.layers.46 as default\n",
            "Injecting model.layers.46.self_attn as default\n",
            "Injecting model.layers.46.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.46.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.46.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.46.input_layernorm as default\n",
            "Injecting model.layers.46.post_attention_layernorm as default\n",
            "Injecting model.layers.47 as default\n",
            "Injecting model.layers.47.self_attn as default\n",
            "Injecting model.layers.47.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.47.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.47.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.47.input_layernorm as default\n",
            "Injecting model.layers.47.post_attention_layernorm as default\n",
            "Injecting model.layers.48 as default\n",
            "Injecting model.layers.48.self_attn as default\n",
            "Injecting model.layers.48.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.48.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.48.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.48.input_layernorm as default\n",
            "Injecting model.layers.48.post_attention_layernorm as default\n",
            "Injecting model.layers.49 as default\n",
            "Injecting model.layers.49.self_attn as default\n",
            "Injecting model.layers.49.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.49.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.49.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.49.input_layernorm as default\n",
            "Injecting model.layers.49.post_attention_layernorm as default\n",
            "Injecting model.layers.50 as default\n",
            "Injecting model.layers.50.self_attn as default\n",
            "Injecting model.layers.50.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.50.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.50.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.50.input_layernorm as default\n",
            "Injecting model.layers.50.post_attention_layernorm as default\n",
            "Injecting model.layers.51 as default\n",
            "Injecting model.layers.51.self_attn as default\n",
            "Injecting model.layers.51.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.51.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.51.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.51.input_layernorm as default\n",
            "Injecting model.layers.51.post_attention_layernorm as default\n",
            "Injecting model.layers.52 as default\n",
            "Injecting model.layers.52.self_attn as default\n",
            "Injecting model.layers.52.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.52.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.52.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.52.input_layernorm as default\n",
            "Injecting model.layers.52.post_attention_layernorm as default\n",
            "Injecting model.layers.53 as default\n",
            "Injecting model.layers.53.self_attn as default\n",
            "Injecting model.layers.53.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.53.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.53.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.53.input_layernorm as default\n",
            "Injecting model.layers.53.post_attention_layernorm as default\n",
            "Injecting model.layers.54 as default\n",
            "Injecting model.layers.54.self_attn as default\n",
            "Injecting model.layers.54.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.54.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.54.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.54.input_layernorm as default\n",
            "Injecting model.layers.54.post_attention_layernorm as default\n",
            "Injecting model.layers.55 as default\n",
            "Injecting model.layers.55.self_attn as default\n",
            "Injecting model.layers.55.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.55.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.55.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.55.input_layernorm as default\n",
            "Injecting model.layers.55.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "loading model.embed_tokens.weight to cpu\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 188, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 115, in local_chat\n",
            "    optimize_and_load_gguf(model, optimize_config_path, gguf_path, config)\n",
            "  File \"/content/ktransformers/ktransformers/optimize/optimize.py\", line 131, in optimize_and_load_gguf\n",
            "    load_weights(module, weights_loader, device=default_device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/base_operator.py\", line 63, in load\n",
            "    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/experts.py\", line 684, in load\n",
            "    self.generate_experts.load(w, warmup=warmup)\n",
            "  File \"/content/ktransformers/ktransformers/operators/experts.py\", line 245, in load\n",
            "    self.cpu_infer.submit(self.moe.warm_up())\n",
            "  File \"/content/ktransformers/ktransformers/operators/cpuinfer.py\", line 739, in submit\n",
            "    CPUInfer.cpuinfer.submit(task)\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'submit'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x22B-v0.1 \\\n",
        "    --gguf_path ./1 \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "    --prompt_file /content/1.txt \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIqVekzhqEJ5",
        "outputId": "297226cb-1fd3-4ca1-de65-8770f8f0c4dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:06:30,373 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:06:32.581695: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750377992.604019   42080 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750377992.610238   42080 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:06:32.632069: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Injecting model as default\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as default\n",
            "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.0.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.0.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as default\n",
            "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.1.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.1.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as default\n",
            "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.2.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.2.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as default\n",
            "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.3.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.3.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as default\n",
            "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.4.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.4.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as default\n",
            "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.5.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.5.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as default\n",
            "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.6.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.6.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as default\n",
            "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.7.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.7.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as default\n",
            "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.8.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.8.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as default\n",
            "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.9.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.9.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as default\n",
            "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.10.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.10.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as default\n",
            "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.11.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.11.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as default\n",
            "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.12.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.12.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as default\n",
            "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.13.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.13.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as default\n",
            "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.14.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.14.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as default\n",
            "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.15.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.15.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as default\n",
            "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.16.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.16.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as default\n",
            "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.17.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.17.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as default\n",
            "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.18.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.18.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as default\n",
            "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.19.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.19.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as default\n",
            "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.20.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.20.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as default\n",
            "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.21.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.21.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as default\n",
            "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.22.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.22.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as default\n",
            "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.23.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.23.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as default\n",
            "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.24.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.24.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as default\n",
            "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.25.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.25.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as default\n",
            "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.26.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.26.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.layers.27 as default\n",
            "Injecting model.layers.27.self_attn as default\n",
            "Injecting model.layers.27.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.27.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.27.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.27.input_layernorm as default\n",
            "Injecting model.layers.27.post_attention_layernorm as default\n",
            "Injecting model.layers.28 as default\n",
            "Injecting model.layers.28.self_attn as default\n",
            "Injecting model.layers.28.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.28.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.28.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.28.input_layernorm as default\n",
            "Injecting model.layers.28.post_attention_layernorm as default\n",
            "Injecting model.layers.29 as default\n",
            "Injecting model.layers.29.self_attn as default\n",
            "Injecting model.layers.29.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.29.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.29.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.29.input_layernorm as default\n",
            "Injecting model.layers.29.post_attention_layernorm as default\n",
            "Injecting model.layers.30 as default\n",
            "Injecting model.layers.30.self_attn as default\n",
            "Injecting model.layers.30.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.30.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.30.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.30.input_layernorm as default\n",
            "Injecting model.layers.30.post_attention_layernorm as default\n",
            "Injecting model.layers.31 as default\n",
            "Injecting model.layers.31.self_attn as default\n",
            "Injecting model.layers.31.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.31.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.31.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.31.input_layernorm as default\n",
            "Injecting model.layers.31.post_attention_layernorm as default\n",
            "Injecting model.layers.32 as default\n",
            "Injecting model.layers.32.self_attn as default\n",
            "Injecting model.layers.32.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.32.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.32.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.32.input_layernorm as default\n",
            "Injecting model.layers.32.post_attention_layernorm as default\n",
            "Injecting model.layers.33 as default\n",
            "Injecting model.layers.33.self_attn as default\n",
            "Injecting model.layers.33.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.33.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.33.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.33.input_layernorm as default\n",
            "Injecting model.layers.33.post_attention_layernorm as default\n",
            "Injecting model.layers.34 as default\n",
            "Injecting model.layers.34.self_attn as default\n",
            "Injecting model.layers.34.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.34.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.34.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.34.input_layernorm as default\n",
            "Injecting model.layers.34.post_attention_layernorm as default\n",
            "Injecting model.layers.35 as default\n",
            "Injecting model.layers.35.self_attn as default\n",
            "Injecting model.layers.35.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.35.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.35.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.35.input_layernorm as default\n",
            "Injecting model.layers.35.post_attention_layernorm as default\n",
            "Injecting model.layers.36 as default\n",
            "Injecting model.layers.36.self_attn as default\n",
            "Injecting model.layers.36.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.36.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.36.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.36.input_layernorm as default\n",
            "Injecting model.layers.36.post_attention_layernorm as default\n",
            "Injecting model.layers.37 as default\n",
            "Injecting model.layers.37.self_attn as default\n",
            "Injecting model.layers.37.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.37.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.37.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.37.input_layernorm as default\n",
            "Injecting model.layers.37.post_attention_layernorm as default\n",
            "Injecting model.layers.38 as default\n",
            "Injecting model.layers.38.self_attn as default\n",
            "Injecting model.layers.38.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.38.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.38.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.38.input_layernorm as default\n",
            "Injecting model.layers.38.post_attention_layernorm as default\n",
            "Injecting model.layers.39 as default\n",
            "Injecting model.layers.39.self_attn as default\n",
            "Injecting model.layers.39.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.39.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.39.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.39.input_layernorm as default\n",
            "Injecting model.layers.39.post_attention_layernorm as default\n",
            "Injecting model.layers.40 as default\n",
            "Injecting model.layers.40.self_attn as default\n",
            "Injecting model.layers.40.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.40.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.40.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.40.input_layernorm as default\n",
            "Injecting model.layers.40.post_attention_layernorm as default\n",
            "Injecting model.layers.41 as default\n",
            "Injecting model.layers.41.self_attn as default\n",
            "Injecting model.layers.41.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.41.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.41.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.41.input_layernorm as default\n",
            "Injecting model.layers.41.post_attention_layernorm as default\n",
            "Injecting model.layers.42 as default\n",
            "Injecting model.layers.42.self_attn as default\n",
            "Injecting model.layers.42.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.42.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.42.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.42.input_layernorm as default\n",
            "Injecting model.layers.42.post_attention_layernorm as default\n",
            "Injecting model.layers.43 as default\n",
            "Injecting model.layers.43.self_attn as default\n",
            "Injecting model.layers.43.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.43.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.43.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.43.input_layernorm as default\n",
            "Injecting model.layers.43.post_attention_layernorm as default\n",
            "Injecting model.layers.44 as default\n",
            "Injecting model.layers.44.self_attn as default\n",
            "Injecting model.layers.44.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.44.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.44.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.44.input_layernorm as default\n",
            "Injecting model.layers.44.post_attention_layernorm as default\n",
            "Injecting model.layers.45 as default\n",
            "Injecting model.layers.45.self_attn as default\n",
            "Injecting model.layers.45.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.45.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.45.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.45.input_layernorm as default\n",
            "Injecting model.layers.45.post_attention_layernorm as default\n",
            "Injecting model.layers.46 as default\n",
            "Injecting model.layers.46.self_attn as default\n",
            "Injecting model.layers.46.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.46.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.46.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.46.input_layernorm as default\n",
            "Injecting model.layers.46.post_attention_layernorm as default\n",
            "Injecting model.layers.47 as default\n",
            "Injecting model.layers.47.self_attn as default\n",
            "Injecting model.layers.47.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.47.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.47.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.47.input_layernorm as default\n",
            "Injecting model.layers.47.post_attention_layernorm as default\n",
            "Injecting model.layers.48 as default\n",
            "Injecting model.layers.48.self_attn as default\n",
            "Injecting model.layers.48.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.48.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.48.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.48.input_layernorm as default\n",
            "Injecting model.layers.48.post_attention_layernorm as default\n",
            "Injecting model.layers.49 as default\n",
            "Injecting model.layers.49.self_attn as default\n",
            "Injecting model.layers.49.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.49.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.49.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.49.input_layernorm as default\n",
            "Injecting model.layers.49.post_attention_layernorm as default\n",
            "Injecting model.layers.50 as default\n",
            "Injecting model.layers.50.self_attn as default\n",
            "Injecting model.layers.50.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.50.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.50.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.50.input_layernorm as default\n",
            "Injecting model.layers.50.post_attention_layernorm as default\n",
            "Injecting model.layers.51 as default\n",
            "Injecting model.layers.51.self_attn as default\n",
            "Injecting model.layers.51.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.51.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.51.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.51.input_layernorm as default\n",
            "Injecting model.layers.51.post_attention_layernorm as default\n",
            "Injecting model.layers.52 as default\n",
            "Injecting model.layers.52.self_attn as default\n",
            "Injecting model.layers.52.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.52.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.52.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.52.input_layernorm as default\n",
            "Injecting model.layers.52.post_attention_layernorm as default\n",
            "Injecting model.layers.53 as default\n",
            "Injecting model.layers.53.self_attn as default\n",
            "Injecting model.layers.53.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.53.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.53.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.53.input_layernorm as default\n",
            "Injecting model.layers.53.post_attention_layernorm as default\n",
            "Injecting model.layers.54 as default\n",
            "Injecting model.layers.54.self_attn as default\n",
            "Injecting model.layers.54.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.54.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.54.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.54.input_layernorm as default\n",
            "Injecting model.layers.54.post_attention_layernorm as default\n",
            "Injecting model.layers.55 as default\n",
            "Injecting model.layers.55.self_attn as default\n",
            "Injecting model.layers.55.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.55.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.55.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.55.input_layernorm as default\n",
            "Injecting model.layers.55.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "loading model.embed_tokens.weight to cpu\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 188, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 115, in local_chat\n",
            "    optimize_and_load_gguf(model, optimize_config_path, gguf_path, config)\n",
            "  File \"/content/ktransformers/ktransformers/optimize/optimize.py\", line 131, in optimize_and_load_gguf\n",
            "    load_weights(module, weights_loader, device=default_device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  [Previous line repeated 1 more time]\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/base_operator.py\", line 63, in load\n",
            "    utils.load_weights(child, self.gguf_loader, self.key+\".\")\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 174, in load_weights\n",
            "    load_weights(child, gguf_loader, prefix+name+\".\", device=device)\n",
            "  File \"/content/ktransformers/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/content/ktransformers/ktransformers/operators/experts.py\", line 684, in load\n",
            "    self.generate_experts.load(w, warmup=warmup)\n",
            "  File \"/content/ktransformers/ktransformers/operators/experts.py\", line 245, in load\n",
            "    self.cpu_infer.submit(self.moe.warm_up())\n",
            "  File \"/content/ktransformers/ktransformers/operators/cpuinfer.py\", line 739, in submit\n",
            "    CPUInfer.cpuinfer.submit(task)\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'submit'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x22B-v0.1 \\\n",
        "    --gguf_path ./1 \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "    --prompt_file /content/1.txt \\\n",
        "    --cpu_infer=1 \\\n",
        "    --use_cuda_graph=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqmnTV3-q9sB",
        "outputId": "400c2cc6-1226-4b46-93b0-d118efa60bd1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:10:13,402 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:10:15.081726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750378215.101784   43455 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750378215.108390   43455 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:10:15.131178: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Injecting model as default\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as default\n",
            "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.0.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.0.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as default\n",
            "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.1.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.1.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as default\n",
            "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.2.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.2.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as default\n",
            "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.3.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.3.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as default\n",
            "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.4.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.4.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as default\n",
            "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.5.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.5.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as default\n",
            "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.6.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.6.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as default\n",
            "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.7.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.7.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as default\n",
            "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.8.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.8.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as default\n",
            "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.9.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.9.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as default\n",
            "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.10.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.10.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as default\n",
            "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.11.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.11.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as default\n",
            "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.12.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.12.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as default\n",
            "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.13.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.13.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as default\n",
            "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.14.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.14.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as default\n",
            "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.15.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.15.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as default\n",
            "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.16.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.16.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as default\n",
            "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.17.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.17.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as default\n",
            "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.18.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.18.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as default\n",
            "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.19.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.19.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as default\n",
            "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.20.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.20.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as default\n",
            "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.21.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.21.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as default\n",
            "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.22.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.22.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as default\n",
            "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.23.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.23.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as default\n",
            "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.24.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.24.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as default\n",
            "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.25.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.25.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as default\n",
            "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.26.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.26.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.layers.27 as default\n",
            "Injecting model.layers.27.self_attn as default\n",
            "Injecting model.layers.27.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.27.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.27.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.27.input_layernorm as default\n",
            "Injecting model.layers.27.post_attention_layernorm as default\n",
            "Injecting model.layers.28 as default\n",
            "Injecting model.layers.28.self_attn as default\n",
            "Injecting model.layers.28.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.28.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.28.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.28.input_layernorm as default\n",
            "Injecting model.layers.28.post_attention_layernorm as default\n",
            "Injecting model.layers.29 as default\n",
            "Injecting model.layers.29.self_attn as default\n",
            "Injecting model.layers.29.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.29.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.29.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.29.input_layernorm as default\n",
            "Injecting model.layers.29.post_attention_layernorm as default\n",
            "Injecting model.layers.30 as default\n",
            "Injecting model.layers.30.self_attn as default\n",
            "Injecting model.layers.30.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.30.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.30.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.30.input_layernorm as default\n",
            "Injecting model.layers.30.post_attention_layernorm as default\n",
            "Injecting model.layers.31 as default\n",
            "Injecting model.layers.31.self_attn as default\n",
            "Injecting model.layers.31.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.31.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.31.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.31.input_layernorm as default\n",
            "Injecting model.layers.31.post_attention_layernorm as default\n",
            "Injecting model.layers.32 as default\n",
            "Injecting model.layers.32.self_attn as default\n",
            "Injecting model.layers.32.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.32.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.32.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.32.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.32.input_layernorm as default\n",
            "Injecting model.layers.32.post_attention_layernorm as default\n",
            "Injecting model.layers.33 as default\n",
            "Injecting model.layers.33.self_attn as default\n",
            "Injecting model.layers.33.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.33.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.33.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.33.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.33.input_layernorm as default\n",
            "Injecting model.layers.33.post_attention_layernorm as default\n",
            "Injecting model.layers.34 as default\n",
            "Injecting model.layers.34.self_attn as default\n",
            "Injecting model.layers.34.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.34.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.34.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.34.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.34.input_layernorm as default\n",
            "Injecting model.layers.34.post_attention_layernorm as default\n",
            "Injecting model.layers.35 as default\n",
            "Injecting model.layers.35.self_attn as default\n",
            "Injecting model.layers.35.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.35.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.35.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.35.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.35.input_layernorm as default\n",
            "Injecting model.layers.35.post_attention_layernorm as default\n",
            "Injecting model.layers.36 as default\n",
            "Injecting model.layers.36.self_attn as default\n",
            "Injecting model.layers.36.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.36.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.36.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.36.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.36.input_layernorm as default\n",
            "Injecting model.layers.36.post_attention_layernorm as default\n",
            "Injecting model.layers.37 as default\n",
            "Injecting model.layers.37.self_attn as default\n",
            "Injecting model.layers.37.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.37.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.37.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.37.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.37.input_layernorm as default\n",
            "Injecting model.layers.37.post_attention_layernorm as default\n",
            "Injecting model.layers.38 as default\n",
            "Injecting model.layers.38.self_attn as default\n",
            "Injecting model.layers.38.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.38.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.38.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.38.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.38.input_layernorm as default\n",
            "Injecting model.layers.38.post_attention_layernorm as default\n",
            "Injecting model.layers.39 as default\n",
            "Injecting model.layers.39.self_attn as default\n",
            "Injecting model.layers.39.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.39.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.39.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.39.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.39.input_layernorm as default\n",
            "Injecting model.layers.39.post_attention_layernorm as default\n",
            "Injecting model.layers.40 as default\n",
            "Injecting model.layers.40.self_attn as default\n",
            "Injecting model.layers.40.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.40.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.40.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.40.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.40.input_layernorm as default\n",
            "Injecting model.layers.40.post_attention_layernorm as default\n",
            "Injecting model.layers.41 as default\n",
            "Injecting model.layers.41.self_attn as default\n",
            "Injecting model.layers.41.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.41.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.41.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.41.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.41.input_layernorm as default\n",
            "Injecting model.layers.41.post_attention_layernorm as default\n",
            "Injecting model.layers.42 as default\n",
            "Injecting model.layers.42.self_attn as default\n",
            "Injecting model.layers.42.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.42.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.42.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.42.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.42.input_layernorm as default\n",
            "Injecting model.layers.42.post_attention_layernorm as default\n",
            "Injecting model.layers.43 as default\n",
            "Injecting model.layers.43.self_attn as default\n",
            "Injecting model.layers.43.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.43.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.43.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.43.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.43.input_layernorm as default\n",
            "Injecting model.layers.43.post_attention_layernorm as default\n",
            "Injecting model.layers.44 as default\n",
            "Injecting model.layers.44.self_attn as default\n",
            "Injecting model.layers.44.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.44.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.44.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.44.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.44.input_layernorm as default\n",
            "Injecting model.layers.44.post_attention_layernorm as default\n",
            "Injecting model.layers.45 as default\n",
            "Injecting model.layers.45.self_attn as default\n",
            "Injecting model.layers.45.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.45.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.45.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.45.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.45.input_layernorm as default\n",
            "Injecting model.layers.45.post_attention_layernorm as default\n",
            "Injecting model.layers.46 as default\n",
            "Injecting model.layers.46.self_attn as default\n",
            "Injecting model.layers.46.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.46.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.46.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.46.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.46.input_layernorm as default\n",
            "Injecting model.layers.46.post_attention_layernorm as default\n",
            "Injecting model.layers.47 as default\n",
            "Injecting model.layers.47.self_attn as default\n",
            "Injecting model.layers.47.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.47.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.47.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.47.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.47.input_layernorm as default\n",
            "Injecting model.layers.47.post_attention_layernorm as default\n",
            "Injecting model.layers.48 as default\n",
            "Injecting model.layers.48.self_attn as default\n",
            "Injecting model.layers.48.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.48.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.48.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.48.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.48.input_layernorm as default\n",
            "Injecting model.layers.48.post_attention_layernorm as default\n",
            "Injecting model.layers.49 as default\n",
            "Injecting model.layers.49.self_attn as default\n",
            "Injecting model.layers.49.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.49.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.49.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.49.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.49.input_layernorm as default\n",
            "Injecting model.layers.49.post_attention_layernorm as default\n",
            "Injecting model.layers.50 as default\n",
            "Injecting model.layers.50.self_attn as default\n",
            "Injecting model.layers.50.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.50.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.50.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.50.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.50.input_layernorm as default\n",
            "Injecting model.layers.50.post_attention_layernorm as default\n",
            "Injecting model.layers.51 as default\n",
            "Injecting model.layers.51.self_attn as default\n",
            "Injecting model.layers.51.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.51.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.51.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.51.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.51.input_layernorm as default\n",
            "Injecting model.layers.51.post_attention_layernorm as default\n",
            "Injecting model.layers.52 as default\n",
            "Injecting model.layers.52.self_attn as default\n",
            "Injecting model.layers.52.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.52.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.52.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.52.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.52.input_layernorm as default\n",
            "Injecting model.layers.52.post_attention_layernorm as default\n",
            "Injecting model.layers.53 as default\n",
            "Injecting model.layers.53.self_attn as default\n",
            "Injecting model.layers.53.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.53.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.53.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.53.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.53.input_layernorm as default\n",
            "Injecting model.layers.53.post_attention_layernorm as default\n",
            "Injecting model.layers.54 as default\n",
            "Injecting model.layers.54.self_attn as default\n",
            "Injecting model.layers.54.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.54.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.54.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.54.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.54.input_layernorm as default\n",
            "Injecting model.layers.54.post_attention_layernorm as default\n",
            "Injecting model.layers.55 as default\n",
            "Injecting model.layers.55.self_attn as default\n",
            "Injecting model.layers.55.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.55.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.55.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.55.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.55.input_layernorm as default\n",
            "Injecting model.layers.55.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "loading model.embed_tokens.weight to cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# مثال على الأمر الصحيح مع الموديل الأصغر\n",
        "!python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "    --gguf_path /path/to/your/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "    --prompt_file /content/1.txt \\\n",
        "    --cpu_infer=1 \\\n",
        "    --use_cuda_graph=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyyy1c3Xq-HY",
        "outputId": "70dd0931-8dba-4133-be83-d4b785ec8e1a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:12:48,183 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:12:49.860211: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750378369.881295   44416 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750378369.887444   44416 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:12:49.908773: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "tokenizer_config.json: 100% 2.10k/2.10k [00:00<00:00, 11.0MB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 13.3MB/s]\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 21.2MB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 2.18MB/s]\n",
            "config.json: 100% 720/720 [00:00<00:00, 4.36MB/s]\n",
            "using custom modeling_xxx.py.\n",
            "MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 188, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/ktransformers/local_chat.py\", line 115, in local_chat\n",
            "    optimize_and_load_gguf(model, optimize_config_path, gguf_path, config)\n",
            "  File \"/content/ktransformers/ktransformers/optimize/optimize.py\", line 126, in optimize_and_load_gguf\n",
            "    weights_loader = ModelLoaderFactory.create_loader(gguf_path)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ktransformers/ktransformers/util/custom_loader.py\", line 519, in create_loader\n",
            "    raise FileNotFoundError(f\"Path not found: {path}\")\n",
            "FileNotFoundError: Path not found: /path/to/your/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers/1\n",
        "!wget https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMWECfKSrkFi",
        "outputId": "26f69c51-4f91-48eb-aa71-b182bf2b13f3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ktransformers/1\n",
            "--2025-06-20 00:14:05--  https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.23, 18.164.174.55, 18.164.174.118, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/ac/ba/acba0635d39a127379c2c6ae1cefacc586bf413e8b044c5ca82daade27d7d503/d54b4f4ec06dbae558d25b2d1542417cdf9547907342db85eecd05b6e96e88f8?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mixtral-8x7b-instruct-v0.1.Q2_K.gguf%3B+filename%3D%22mixtral-8x7b-instruct-v0.1.Q2_K.gguf%22%3B&Expires=1750382045&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDM4MjA0NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2FjL2JhL2FjYmEwNjM1ZDM5YTEyNzM3OWMyYzZhZTFjZWZhY2M1ODZiZjQxM2U4YjA0NGM1Y2E4MmRhYWRlMjdkN2Q1MDMvZDU0YjRmNGVjMDZkYmFlNTU4ZDI1YjJkMTU0MjQxN2NkZjk1NDc5MDczNDJkYjg1ZWVjZDA1YjZlOTZlODhmOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=OAhanOMtUl8qlbuihgjGB0vH3%7EZuIJ3ZSv%7E8kx5Ts-1ZahH4Di7uG9VdxJ2UtTDCIqaF0xsClKf1FRHpOhGrVszDKHtf0NGbhxGbwz6C1NleqLfhtE8cj93HfI6tMR6nfITVsxSqKhLXRhFJ1pXM6FBCGiN0Fem35kW105etNpclFYUTrM37acr3LTwca8ZbDJHfjRPc5Bd456JQ4xVjQJv%7EX9qfshSRs3ZpOGD8ZOszdPtOD6VWCpaaeUD0vq4UsmVF0VSFjLV06OzDNd-uID5gx%7EkCp2RDjf6zd88Jn7SvQ5Gr286Yu2YNG8spPJkI2Q-hGa4VRqoPDXnF%7ErXRRQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-06-20 00:14:05--  https://cdn-lfs-us-1.hf.co/repos/ac/ba/acba0635d39a127379c2c6ae1cefacc586bf413e8b044c5ca82daade27d7d503/d54b4f4ec06dbae558d25b2d1542417cdf9547907342db85eecd05b6e96e88f8?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mixtral-8x7b-instruct-v0.1.Q2_K.gguf%3B+filename%3D%22mixtral-8x7b-instruct-v0.1.Q2_K.gguf%22%3B&Expires=1750382045&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDM4MjA0NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2FjL2JhL2FjYmEwNjM1ZDM5YTEyNzM3OWMyYzZhZTFjZWZhY2M1ODZiZjQxM2U4YjA0NGM1Y2E4MmRhYWRlMjdkN2Q1MDMvZDU0YjRmNGVjMDZkYmFlNTU4ZDI1YjJkMTU0MjQxN2NkZjk1NDc5MDczNDJkYjg1ZWVjZDA1YjZlOTZlODhmOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=OAhanOMtUl8qlbuihgjGB0vH3%7EZuIJ3ZSv%7E8kx5Ts-1ZahH4Di7uG9VdxJ2UtTDCIqaF0xsClKf1FRHpOhGrVszDKHtf0NGbhxGbwz6C1NleqLfhtE8cj93HfI6tMR6nfITVsxSqKhLXRhFJ1pXM6FBCGiN0Fem35kW105etNpclFYUTrM37acr3LTwca8ZbDJHfjRPc5Bd456JQ4xVjQJv%7EX9qfshSRs3ZpOGD8ZOszdPtOD6VWCpaaeUD0vq4UsmVF0VSFjLV06OzDNd-uID5gx%7EkCp2RDjf6zd88Jn7SvQ5Gr286Yu2YNG8spPJkI2Q-hGa4VRqoPDXnF%7ErXRRQ__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.164.174.98, 18.164.174.97, 18.164.174.52, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.164.174.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15644035008 (15G) [binary/octet-stream]\n",
            "Saving to: ‘mixtral-8x7b-instruct-v0.1.Q2_K.gguf’\n",
            "\n",
            "mixtral-8x7b-instru 100%[===================>]  14.57G  77.1MB/s    in 3m 18s  \n",
            "\n",
            "2025-06-20 00:17:24 (75.3 MB/s) - ‘mixtral-8x7b-instruct-v0.1.Q2_K.gguf’ saved [15644035008/15644035008]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mistralai/Mixtral-8x7B-Instruct-v0.1"
      ],
      "metadata": {
        "id": "aALgqcrTr4C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# مثال على الأمر الصحيح مع الموديل الأصغر\n",
        "!python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "    --gguf_path /content/ktransformers/1/mixtral-8x7b-instruct-v0.1.Q2_K.gguf \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "    --prompt_file /content/1.txt \\\n",
        "    --cpu_infer=1 \\\n",
        "    --use_cuda_graph=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wZPmfsksL3B",
        "outputId": "475777dd-f4f6-4305-c45b-e31a75b33ca9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:17:32,262 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:17:34.423562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750378654.458188   46155 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750378654.470495   46155 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:17:34.503251: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 196, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 100, in local_chat\n",
            "    model = custom_models[config.architectures[0]](config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\", line 1341, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1868, in __init__\n",
            "    config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
            "    cls._check_and_enable_flash_attn_2(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2252, in _check_and_enable_flash_attn_2\n",
            "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
            "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# مثال على الأمر الصحيح مع الموديل الأصغر\n",
        "!python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "    --gguf_path /content/ktransformers/1/mixtral-8x7b-instruct-v0.1.Q2_K.gguf \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "    --prompt_file /content/1.txt \\\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rcxnl7Fbs8Dq",
        "outputId": "04983e33-39b0-4b87-8f64-63ac4c209b31"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:18:55,837 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:18:57.813131: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750378737.833286   46681 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750378737.841120   46681 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:18:57.863223: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 196, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 100, in local_chat\n",
            "    model = custom_models[config.architectures[0]](config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\", line 1341, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1868, in __init__\n",
            "    config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
            "    cls._check_and_enable_flash_attn_2(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2252, in _check_and_enable_flash_attn_2\n",
            "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
            "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# مثال على الأمر الصحيح مع الموديل الأصغر\n",
        "!python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "    --gguf_path /content/ktransformers/1/mixtral-8x7b-instruct-v0.1.Q2_K.gguf \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "    --prompt_file /content/1.txt \\\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1fA8lHPt5WC",
        "outputId": "6ad75a34-f2d3-4966-ee7e-be42e4c0c74f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:23:01,544 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:23:03.240303: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750378983.260962   48203 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750378983.267484   48203 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:23:03.290175: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 196, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 100, in local_chat\n",
            "    model = custom_models[config.architectures[0]](config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\", line 1341, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1868, in __init__\n",
            "    config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
            "    cls._check_and_enable_flash_attn_2(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2252, in _check_and_enable_flash_attn_2\n",
            "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
            "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x22B-v0.1 \\\n",
        "    --gguf_path ./1 \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "    --cpu_infer=1 \\\n",
        "    --use_cuda_graph=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akKcquAZt5oy",
        "outputId": "e968e31a-63ad-4aed-b92b-a8515833015a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:25:04,685 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:25:06.379224: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750379106.399893   49003 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750379106.406080   49003 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:25:06.427257: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 196, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 100, in local_chat\n",
            "    model = custom_models[config.architectures[0]](config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\", line 1341, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1868, in __init__\n",
            "    config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
            "    cls._check_and_enable_flash_attn_2(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2252, in _check_and_enable_flash_attn_2\n",
            "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
            "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export DISABLE_FLASH_ATTN=1\n",
        "!python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x22B-v0.1 \\\n",
        "    --gguf_path ./1 \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "    --cpu_infer=1 \\\n",
        "    --use_cuda_graph=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4riIxJDluQmD",
        "outputId": "b1e9e404-264a-4caf-cd29-3df73dba02b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:35:58,484 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:36:00.200812: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750379760.220668   53066 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750379760.226705   53066 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:36:00.247560: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 196, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 100, in local_chat\n",
            "    model = custom_models[config.architectures[0]](config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\", line 1341, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1868, in __init__\n",
            "    config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
            "    cls._check_and_enable_flash_attn_2(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2252, in _check_and_enable_flash_attn_2\n",
            "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
            "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"DISABLE_FLASH_ATTN\"] = \"1\"\n"
      ],
      "metadata": {
        "id": "ms3lmyatw9aU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x22B-v0.1 \\\n",
        "    --gguf_path ./1 \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tijbt9EcvbGk",
        "outputId": "e71a3406-b71d-4fc1-f298-c39e770e0ef8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:36:26,054 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:36:27.757235: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750379787.777797   53245 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750379787.784093   53245 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:36:27.804040: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 196, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 100, in local_chat\n",
            "    model = custom_models[config.architectures[0]](config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\", line 1341, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1868, in __init__\n",
            "    config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
            "    cls._check_and_enable_flash_attn_2(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2252, in _check_and_enable_flash_attn_2\n",
            "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
            "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!DISABLE_FLASH_ATTN=1 python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x22B-v0.1 \\\n",
        "    --gguf_path ./1 \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "    --cpu_infer=1 \\\n",
        "    --use_cuda_graph=False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im0J43G5wONG",
        "outputId": "6504e55a-f15c-40dd-d3c4-d07302d3c9b6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:38:32,800 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:38:34.640489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750379914.676855   54024 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750379914.688220   54024 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:38:34.723026: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 196, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 100, in local_chat\n",
            "    model = custom_models[config.architectures[0]](config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\", line 1341, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1868, in __init__\n",
            "    config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
            "    cls._check_and_enable_flash_attn_2(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2252, in _check_and_enable_flash_attn_2\n",
            "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
            "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!DISABLE_FLASH_ATTN=1 python -m ktransformers.local_chat \\\n",
        "  --model_path mistralai/Mixtral-8x22B-v0.1 \\\n",
        "  --gguf_path ./1 \\\n",
        "  --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "  --cpu_infer=1 \\\n",
        "  --use_cuda_graph=False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iuZiuwlxdKD",
        "outputId": "52a0a78d-fc12-4760-e1a9-689227aaa783"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:39:35,798 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:39:37.510988: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750379977.531241   54416 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750379977.537329   54416 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:39:37.557927: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 196, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 100, in local_chat\n",
            "    model = custom_models[config.architectures[0]](config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\", line 1341, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1868, in __init__\n",
            "    config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
            "    cls._check_and_enable_flash_attn_2(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2252, in _check_and_enable_flash_attn_2\n",
            "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
            "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"DISABLE_FLASH_ATTN\"] = \"1\"\n",
        "!python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x22B-v0.1 \\\n",
        "    --gguf_path ./1 \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "    --cpu_infer=1 \\\n",
        "    --use_cuda_graph=False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzSypuNtxoYD",
        "outputId": "568b35f4-edd5-4678-8ea9-a730898df2fd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:41:19,046 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:41:20.741127: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750380080.761431   55064 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750380080.767740   55064 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:41:20.787546: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 196, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 100, in local_chat\n",
            "    model = custom_models[config.architectures[0]](config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\", line 1341, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1868, in __init__\n",
            "    config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
            "    cls._check_and_enable_flash_attn_2(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2252, in _check_and_enable_flash_attn_2\n",
            "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
            "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m ktransformers.local_chat \\\n",
        "    --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "    --gguf_path /path/to/your/SMALLER_mixtral-8x7b.gguf \\\n",
        "    --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "    --cpu_infer=1 \\\n",
        "    --use_cuda_graph=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm640CCwx7bE",
        "outputId": "3d3a2c6b-3d17-487b-f160-1c27f1657ea6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:42:00,688 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:42:02.409431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750380122.429522   55333 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750380122.435650   55333 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:42:02.456031: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 196, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 100, in local_chat\n",
            "    model = custom_models[config.architectures[0]](config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\", line 1341, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1868, in __init__\n",
            "    config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
            "    cls._check_and_enable_flash_attn_2(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2252, in _check_and_enable_flash_attn_2\n",
            "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
            "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ktransformers/run_fixed.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAhroz4LyP7D",
        "outputId": "49ee3f6a-6269-4f56-b325-48c062820021"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ تم إصلاح الملف بنجاح: /usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\n",
            "\n",
            "🚀 سيتم الآن تشغيل الأمر التالي:\n",
            "python -m ktransformers.local_chat --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 --gguf_path /content/ktransformers/1 --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml --cpu_infer=1 --use_cuda_graph=False\n",
            "------------------------------\n",
            "no balance_serve\n",
            "2025-06-20 00:44:39,862 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:44:41.609512: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750380281.633566   56317 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750380281.640310   56317 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:44:41.661700: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 196, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 100, in local_chat\n",
            "    model = custom_models[config.architectures[0]](config)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\", line 1341, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 1868, in __init__\n",
            "    config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
            "    cls._check_and_enable_flash_attn_2(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 2233, in _check_and_enable_flash_attn_2\n",
            "    raise ValueError(\n",
            "ValueError: MixtralForCausalLM does not support Flash Attention 2.0 yet. Please request to add support where the model is hosted, on its model hub page: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/new or in the Transformers GitHub repo: https://github.com/huggingface/transformers/issues/new\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ktransformers/run_fixed.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCiWk-QRyzkc",
        "outputId": "e464bdd3-8c52-4d8e-e9e9-bfffc9ba86f9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔️ [1/2] الملف كان مُصلَحًا بالفعل: /usr/local/lib/python3.11/dist-packages/ktransformers/models/modeling_mixtral.py\n",
            "✅ [2/2] تم إصلاح الملف بنجاح: /usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\n",
            "\n",
            "🚀 سيتم الآن تشغيل الأمر التالي:\n",
            "python -m ktransformers.local_chat --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 --gguf_path /content/ktransformers/1 --optimize_config_path /content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml --cpu_infer=1 --use_cuda_graph=False\n",
            "------------------------------\n",
            "no balance_serve\n",
            "2025-06-20 00:48:58,287 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:49:00.016626: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750380540.037039   57891 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750380540.043264   57891 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:49:00.065239: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "using custom modeling_xxx.py.\n",
            "MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Injecting model as default\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as default\n",
            "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.0.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.0.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as default\n",
            "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.1.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.1.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as default\n",
            "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.2.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.2.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as default\n",
            "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.3.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.3.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as default\n",
            "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.4.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.4.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as default\n",
            "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.5.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.5.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as default\n",
            "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.6.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.6.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as default\n",
            "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.7.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.7.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as default\n",
            "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.8.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.8.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as default\n",
            "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.9.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.9.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as default\n",
            "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.10.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.10.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as default\n",
            "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.11.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.11.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as default\n",
            "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.12.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.12.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as default\n",
            "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.13.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.13.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as default\n",
            "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.14.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.14.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as default\n",
            "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.15.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.15.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as default\n",
            "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.16.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.16.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as default\n",
            "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.17.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.17.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as default\n",
            "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.18.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.18.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as default\n",
            "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.19.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.19.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as default\n",
            "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.20.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.20.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as default\n",
            "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.21.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.21.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as default\n",
            "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.22.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.22.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as default\n",
            "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.23.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.23.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as default\n",
            "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.24.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.24.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as default\n",
            "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.25.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.25.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as default\n",
            "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.26.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.26.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.layers.27 as default\n",
            "Injecting model.layers.27.self_attn as default\n",
            "Injecting model.layers.27.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.27.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.27.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.27.input_layernorm as default\n",
            "Injecting model.layers.27.post_attention_layernorm as default\n",
            "Injecting model.layers.28 as default\n",
            "Injecting model.layers.28.self_attn as default\n",
            "Injecting model.layers.28.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.28.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.28.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.28.input_layernorm as default\n",
            "Injecting model.layers.28.post_attention_layernorm as default\n",
            "Injecting model.layers.29 as default\n",
            "Injecting model.layers.29.self_attn as default\n",
            "Injecting model.layers.29.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.29.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.29.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.29.input_layernorm as default\n",
            "Injecting model.layers.29.post_attention_layernorm as default\n",
            "Injecting model.layers.30 as default\n",
            "Injecting model.layers.30.self_attn as default\n",
            "Injecting model.layers.30.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.30.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.30.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.30.input_layernorm as default\n",
            "Injecting model.layers.30.post_attention_layernorm as default\n",
            "Injecting model.layers.31 as default\n",
            "Injecting model.layers.31.self_attn as default\n",
            "Injecting model.layers.31.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.31.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.31.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.31.input_layernorm as default\n",
            "Injecting model.layers.31.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 196, in <module>\n",
            "    fire.Fire(local_chat)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 135, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 468, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/local_chat.py\", line 123, in local_chat\n",
            "    optimize_and_load_gguf(model, optimize_config_path, gguf_path, config, default_device=device)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/optimize/optimize.py\", line 130, in optimize_and_load_gguf\n",
            "    load_weights(module.lm_head, weights_loader, \"lm_head.\", device=default_device)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/linear.py\", line 937, in load\n",
            "    self.generate_linear.load(w=w)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/linear.py\", line 626, in load\n",
            "    weight = w.view(self.orin_out_features, self.orin_in_features).T\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: shape '[32000, 4096]' is invalid for input of size 196608000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!numactl -N 1 -m 1 python ./ktransformers/local_chat.py --model_path mistralai/Mixtral-8x22B-v0.1 --gguf_path ./1  --prompt_file /content/1.txt  --cpu_infer 33 --max_new_tokens 10\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ly_4_l9z1p0",
        "outputId": "32ad47da-47d5-4715-b5e9-0b43f26470c9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: numactl: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers\n",
        "!python ktransformers/server/main.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4k0UZwU1G69",
        "outputId": "2ab04356-946c-4c61-e92f-df4fb17e80f3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ktransformers\n",
            "no balance_serve\n",
            "2025-06-20 00:54:48,239 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:54:53.885604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750380893.905825   60046 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750380893.912025   60046 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-20 00:54:53.932327: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "usage: kvcache.ai [-h] [--host HOST] [--port PORT] [--api_key API_KEY]\n",
            "                  [--ssl_keyfile SSL_KEYFILE] [--ssl_certfile SSL_CERTFILE]\n",
            "                  [--web WEB] [--model_name MODEL_NAME]\n",
            "                  [--model_dir MODEL_DIR] [--model_path MODEL_PATH]\n",
            "                  [--device DEVICE] [--architectures ARCHITECTURES]\n",
            "                  [--gguf_path GGUF_PATH]\n",
            "                  [--optimize_config_path OPTIMIZE_CONFIG_PATH]\n",
            "                  [--cpu_infer CPU_INFER] [--backend_type BACKEND_TYPE]\n",
            "                  [--chunk_size CHUNK_SIZE] [--max_batch_size MAX_BATCH_SIZE]\n",
            "                  [--max_new_tokens MAX_NEW_TOKENS] [--json_mode JSON_MODE]\n",
            "                  [--healing HEALING] [--ban_strings BAN_STRINGS]\n",
            "                  [--gpu_split GPU_SPLIT] [--length LENGTH]\n",
            "                  [--rope_scale ROPE_SCALE] [--rope_alpha ROPE_ALPHA]\n",
            "                  [--no_flash_attn NO_FLASH_ATTN] [--low_mem LOW_MEM]\n",
            "                  [--experts_per_token EXPERTS_PER_TOKEN] [--load_q4 LOAD_Q4]\n",
            "                  [--fast_safetensors FAST_SAFETENSORS]\n",
            "                  [--draft_model_dir DRAFT_MODEL_DIR]\n",
            "                  [--no_draft_scale NO_DRAFT_SCALE] [--modes MODES]\n",
            "                  [--mode MODE] [--username USERNAME] [--botname BOTNAME]\n",
            "                  [--system_prompt SYSTEM_PROMPT] [--temperature TEMPERATURE]\n",
            "                  [--smoothing_factor SMOOTHING_FACTOR]\n",
            "                  [--dynamic_temperature DYNAMIC_TEMPERATURE] [--top_k TOP_K]\n",
            "                  [--top_p TOP_P] [--top_a TOP_A] [--skew SKEW]\n",
            "                  [--typical TYPICAL]\n",
            "                  [--repetition_penalty REPETITION_PENALTY]\n",
            "                  [--frequency_penalty FREQUENCY_PENALTY]\n",
            "                  [--presence_penalty PRESENCE_PENALTY]\n",
            "                  [--response_chunk RESPONSE_CHUNK]\n",
            "                  [--no_code_formatting NO_CODE_FORMATTING]\n",
            "                  [--cache_8bit CACHE_8BIT] [--cache_q4 CACHE_Q4]\n",
            "                  [--ngram_decoding NGRAM_DECODING]\n",
            "                  [--print_timings PRINT_TIMINGS] [--amnesia AMNESIA]\n",
            "                  [--batch_size BATCH_SIZE] [--cache_lens CACHE_LENS]\n",
            "                  [--kvc2_config_dir KVC2_CONFIG_DIR] [--log_dir LOG_DIR]\n",
            "                  [--log_file LOG_FILE] [--log_level LOG_LEVEL]\n",
            "                  [--backup_count BACKUP_COUNT] [--db_type DB_TYPE]\n",
            "                  [--db_host DB_HOST] [--db_port DB_PORT] [--db_name DB_NAME]\n",
            "                  [--db_pool_size DB_POOL_SIZE] [--db_database DB_DATABASE]\n",
            "                  [--user_secret_key USER_SECRET_KEY]\n",
            "                  [--user_algorithm USER_ALGORITHM]\n",
            "                  [--force_think | --no-force_think]\n",
            "                  [--use_cuda_graph | --no-use_cuda_graph]\n",
            "                  [--web_cross_domain WEB_CROSS_DOMAIN]\n",
            "                  [--file_upload_dir FILE_UPLOAD_DIR]\n",
            "                  [--assistant_store_dir ASSISTANT_STORE_DIR]\n",
            "                  [--prompt_file PROMPT_FILE]\n",
            "                  [--sched_strategy SCHED_STRATEGY] [--page_size PAGE_SIZE]\n",
            "                  [--memory_gpu_only MEMORY_GPU_ONLY]\n",
            "                  [--utilization_percentage UTILIZATION_PERCENTAGE]\n",
            "                  [--cpu_memory_size_GB CPU_MEMORY_SIZE_GB]\n",
            "\n",
            "Ktransformers\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --host HOST\n",
            "  --port PORT\n",
            "  --api_key API_KEY\n",
            "  --ssl_keyfile SSL_KEYFILE\n",
            "  --ssl_certfile SSL_CERTFILE\n",
            "  --web WEB\n",
            "  --model_name MODEL_NAME\n",
            "  --model_dir MODEL_DIR\n",
            "  --model_path MODEL_PATH\n",
            "  --device DEVICE       Warning: Abandoning this parameter\n",
            "  --architectures ARCHITECTURES\n",
            "  --gguf_path GGUF_PATH\n",
            "  --optimize_config_path OPTIMIZE_CONFIG_PATH\n",
            "  --cpu_infer CPU_INFER\n",
            "  --backend_type BACKEND_TYPE\n",
            "  --chunk_size CHUNK_SIZE\n",
            "  --max_batch_size MAX_BATCH_SIZE\n",
            "  --max_new_tokens MAX_NEW_TOKENS\n",
            "  --json_mode JSON_MODE\n",
            "  --healing HEALING\n",
            "  --ban_strings BAN_STRINGS\n",
            "  --gpu_split GPU_SPLIT\n",
            "  --length LENGTH\n",
            "  --rope_scale ROPE_SCALE\n",
            "  --rope_alpha ROPE_ALPHA\n",
            "  --no_flash_attn NO_FLASH_ATTN\n",
            "  --low_mem LOW_MEM\n",
            "  --experts_per_token EXPERTS_PER_TOKEN\n",
            "  --load_q4 LOAD_Q4\n",
            "  --fast_safetensors FAST_SAFETENSORS\n",
            "  --draft_model_dir DRAFT_MODEL_DIR\n",
            "  --no_draft_scale NO_DRAFT_SCALE\n",
            "  --modes MODES\n",
            "  --mode MODE\n",
            "  --username USERNAME\n",
            "  --botname BOTNAME\n",
            "  --system_prompt SYSTEM_PROMPT\n",
            "  --temperature TEMPERATURE\n",
            "  --smoothing_factor SMOOTHING_FACTOR\n",
            "  --dynamic_temperature DYNAMIC_TEMPERATURE\n",
            "  --top_k TOP_K\n",
            "  --top_p TOP_P\n",
            "  --top_a TOP_A\n",
            "  --skew SKEW\n",
            "  --typical TYPICAL\n",
            "  --repetition_penalty REPETITION_PENALTY\n",
            "  --frequency_penalty FREQUENCY_PENALTY\n",
            "  --presence_penalty PRESENCE_PENALTY\n",
            "  --response_chunk RESPONSE_CHUNK\n",
            "  --no_code_formatting NO_CODE_FORMATTING\n",
            "  --cache_8bit CACHE_8BIT\n",
            "  --cache_q4 CACHE_Q4\n",
            "  --ngram_decoding NGRAM_DECODING\n",
            "  --print_timings PRINT_TIMINGS\n",
            "  --amnesia AMNESIA\n",
            "  --batch_size BATCH_SIZE\n",
            "  --cache_lens CACHE_LENS\n",
            "  --kvc2_config_dir KVC2_CONFIG_DIR\n",
            "  --log_dir LOG_DIR\n",
            "  --log_file LOG_FILE\n",
            "  --log_level LOG_LEVEL\n",
            "  --backup_count BACKUP_COUNT\n",
            "  --db_type DB_TYPE\n",
            "  --db_host DB_HOST\n",
            "  --db_port DB_PORT\n",
            "  --db_name DB_NAME\n",
            "  --db_pool_size DB_POOL_SIZE\n",
            "  --db_database DB_DATABASE\n",
            "  --user_secret_key USER_SECRET_KEY\n",
            "  --user_algorithm USER_ALGORITHM\n",
            "  --force_think, --no-force_think\n",
            "  --use_cuda_graph, --no-use_cuda_graph\n",
            "  --web_cross_domain WEB_CROSS_DOMAIN\n",
            "  --file_upload_dir FILE_UPLOAD_DIR\n",
            "  --assistant_store_dir ASSISTANT_STORE_DIR\n",
            "  --prompt_file PROMPT_FILE\n",
            "  --sched_strategy SCHED_STRATEGY\n",
            "  --page_size PAGE_SIZE\n",
            "  --memory_gpu_only MEMORY_GPU_ONLY\n",
            "  --utilization_percentage UTILIZATION_PERCENTAGE\n",
            "  --cpu_memory_size_GB CPU_MEMORY_SIZE_GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ktransformers/server/main.py --model_path /mnt/data/models/DeepSeek-V3 --gguf_path /mnt/data/models/DeepSeek-V3-GGUF/DeepSeek-V3-Q4_K_M/ --cpu_infer 62 --optimize_config_path ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-serve.yaml --port 10002 --chunk_size 256 --max_new_tokens 1024 --max_batch_size 4 --port 10002 --cache_lens 32768 --backend_type balance_serve\n"
      ],
      "metadata": {
        "id": "fjLhY5QX1FY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "--no_flash_attn"
      ],
      "metadata": {
        "id": "akWbl5jv1T7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"DISABLE_FLASH_ATTN\"] = \"1\"\n",
        "\n",
        "# الآن استدعاء سكربت التشغيل أو الكود\n",
        "from ktransformers.local_chat import local_chat\n",
        "local_chat(\n",
        "    model_path=\"mistralai/Mixtral-8x22B-v0.1\",\n",
        "    gguf_path=\"./1\",\n",
        "    optimize_config_path=\"/content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml\",\n",
        "    cpu_infer=True,\n",
        "    use_cuda_graph=False\n",
        "    --no_flash_attn\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "kYdy2qMf1Y19",
        "outputId": "b5b98a49-2143-48b5-bc19-e9d2ff3327b4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-06-20 00:56:01,992 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found flashinfer\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'no_flash_attn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-19-4254372511.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcpu_infer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0muse_cuda_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mno_flash_attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'no_flash_attn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ktransformers\n",
        "!python -m ktransformers.local_chat --model_path mistralai/Mixtral-8x22B-v0.1 --gguf_path ./1 --max_new_tokens 1 --optimize_config_path /content/ktransformers/ktransformers/optimiz --device cpu use_cuda_graph falsee/optimize_rules/Mixtral.yaml"
      ],
      "metadata": {
        "id": "yHXrExs91sm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/ktransformers/ktransformers/optimize/optimize_rules/Mixtral.yaml"
      ],
      "metadata": {
        "id": "qrjljqlt10G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python ktransformers/server/main.py --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 --gguf_path ./1 --cpu_infer 62 --optimize_config_path ktransformers/optimize/optimize_rules/Mixtral.yaml --port 10002 --chunk_size 256 --max_new_tokens 1024 --max_batch_size 4 --port 10002 --cache_lens 32768 --backend_type balance_serve --no_flash_attn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykflo82_1iiG",
        "outputId": "7dc0854f-6821-483e-e52f-73ddb6142ecb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:58:24,069 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:58:27.917335: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750381107.937972   61377 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750381107.944381   61377 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "usage: kvcache.ai [-h] [--host HOST] [--port PORT] [--api_key API_KEY]\n",
            "                  [--ssl_keyfile SSL_KEYFILE] [--ssl_certfile SSL_CERTFILE]\n",
            "                  [--web WEB] [--model_name MODEL_NAME]\n",
            "                  [--model_dir MODEL_DIR] [--model_path MODEL_PATH]\n",
            "                  [--device DEVICE] [--architectures ARCHITECTURES]\n",
            "                  [--gguf_path GGUF_PATH]\n",
            "                  [--optimize_config_path OPTIMIZE_CONFIG_PATH]\n",
            "                  [--cpu_infer CPU_INFER] [--backend_type BACKEND_TYPE]\n",
            "                  [--chunk_size CHUNK_SIZE] [--max_batch_size MAX_BATCH_SIZE]\n",
            "                  [--max_new_tokens MAX_NEW_TOKENS] [--json_mode JSON_MODE]\n",
            "                  [--healing HEALING] [--ban_strings BAN_STRINGS]\n",
            "                  [--gpu_split GPU_SPLIT] [--length LENGTH]\n",
            "                  [--rope_scale ROPE_SCALE] [--rope_alpha ROPE_ALPHA]\n",
            "                  [--no_flash_attn NO_FLASH_ATTN] [--low_mem LOW_MEM]\n",
            "                  [--experts_per_token EXPERTS_PER_TOKEN] [--load_q4 LOAD_Q4]\n",
            "                  [--fast_safetensors FAST_SAFETENSORS]\n",
            "                  [--draft_model_dir DRAFT_MODEL_DIR]\n",
            "                  [--no_draft_scale NO_DRAFT_SCALE] [--modes MODES]\n",
            "                  [--mode MODE] [--username USERNAME] [--botname BOTNAME]\n",
            "                  [--system_prompt SYSTEM_PROMPT] [--temperature TEMPERATURE]\n",
            "                  [--smoothing_factor SMOOTHING_FACTOR]\n",
            "                  [--dynamic_temperature DYNAMIC_TEMPERATURE] [--top_k TOP_K]\n",
            "                  [--top_p TOP_P] [--top_a TOP_A] [--skew SKEW]\n",
            "                  [--typical TYPICAL]\n",
            "                  [--repetition_penalty REPETITION_PENALTY]\n",
            "                  [--frequency_penalty FREQUENCY_PENALTY]\n",
            "                  [--presence_penalty PRESENCE_PENALTY]\n",
            "                  [--response_chunk RESPONSE_CHUNK]\n",
            "                  [--no_code_formatting NO_CODE_FORMATTING]\n",
            "                  [--cache_8bit CACHE_8BIT] [--cache_q4 CACHE_Q4]\n",
            "                  [--ngram_decoding NGRAM_DECODING]\n",
            "                  [--print_timings PRINT_TIMINGS] [--amnesia AMNESIA]\n",
            "                  [--batch_size BATCH_SIZE] [--cache_lens CACHE_LENS]\n",
            "                  [--kvc2_config_dir KVC2_CONFIG_DIR] [--log_dir LOG_DIR]\n",
            "                  [--log_file LOG_FILE] [--log_level LOG_LEVEL]\n",
            "                  [--backup_count BACKUP_COUNT] [--db_type DB_TYPE]\n",
            "                  [--db_host DB_HOST] [--db_port DB_PORT] [--db_name DB_NAME]\n",
            "                  [--db_pool_size DB_POOL_SIZE] [--db_database DB_DATABASE]\n",
            "                  [--user_secret_key USER_SECRET_KEY]\n",
            "                  [--user_algorithm USER_ALGORITHM]\n",
            "                  [--force_think | --no-force_think]\n",
            "                  [--use_cuda_graph | --no-use_cuda_graph]\n",
            "                  [--web_cross_domain WEB_CROSS_DOMAIN]\n",
            "                  [--file_upload_dir FILE_UPLOAD_DIR]\n",
            "                  [--assistant_store_dir ASSISTANT_STORE_DIR]\n",
            "                  [--prompt_file PROMPT_FILE]\n",
            "                  [--sched_strategy SCHED_STRATEGY] [--page_size PAGE_SIZE]\n",
            "                  [--memory_gpu_only MEMORY_GPU_ONLY]\n",
            "                  [--utilization_percentage UTILIZATION_PERCENTAGE]\n",
            "                  [--cpu_memory_size_GB CPU_MEMORY_SIZE_GB]\n",
            "kvcache.ai: error: argument --no_flash_attn: expected one argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "--no_flash_attn NO_FLASH_ATTN"
      ],
      "metadata": {
        "id": "V4LThPqm2LN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python ktransformers/server/main.py --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 --gguf_path ./1 --cpu_infer 62 --optimize_config_path ktransformers/optimize/optimize_rules/Mixtral.yaml --port 10002 --chunk_size 256 --max_new_tokens 1024 --max_batch_size 4 --port 10002 --cache_lens 32768 --backend_type balance_serve --no_flash_attn NO_FLASH_ATTN\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRo81ozN2NRO",
        "outputId": "97197511-a83c-4c32-9894-e676a20cd68c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 00:59:30,947 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 00:59:34.700055: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750381174.721930   61800 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750381174.728062   61800 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ktransformers/ktransformers/server/main.py\", line 122, in <module>\n",
            "    main()\n",
            "  File \"/content/ktransformers/ktransformers/server/main.py\", line 109, in main\n",
            "    create_interface(config=cfg, default_args=cfg)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/utils/create_interface.py\", line 27, in create_interface\n",
            "    from ktransformers.server.backend.interfaces.balance_serve import BalanceServeInterface as BackendInterface\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/backend/interfaces/balance_serve.py\", line 23, in <module>\n",
            "    from ktransformers.models.custom_modeling_deepseek_v3 import KDeepseekV3ForCausalLM\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/custom_modeling_deepseek_v3.py\", line 17, in <module>\n",
            "    from ktransformers.server.balance_serve.inference.forward_batch import ForwardBatchInput, ForwardBatchOutput\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/balance_serve/inference/forward_batch.py\", line 7, in <module>\n",
            "    from ktransformers.server.balance_serve.settings import sched_ext\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/balance_serve/settings.py\", line 11, in <module>\n",
            "    import sched_ext\n",
            "ModuleNotFoundError: No module named 'sched_ext'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ktransformers/server/main.py \\\n",
        "  --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "  --gguf_path ./1 \\\n",
        "  --cpu_infer 62 \\\n",
        "  --optimize_config_path ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "  --port 10002 \\\n",
        "  --chunk_size 256 \\\n",
        "  --max_new_tokens 1024 \\\n",
        "  --max_batch_size 4 \\\n",
        "  --cache_lens 32768 \\\n",
        "  --backend_type balance_serve\n",
        "  --no_flash_attn NO_FLASH_ATTN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "cCVPvPgM2Qh0",
        "outputId": "4e20424c-802c-4954-fc2d-e1bd443805ba"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (ipython-input-27-745902337.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-27-745902337.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    --no_flash_attn NO_FLASH_ATTN\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ktransformers/server/main.py \\\n",
        "  --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "  --gguf_path ./1 \\\n",
        "  --cpu_infer 62 \\\n",
        "  --optimize_config_path ./ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "  --port 10002 \\\n",
        "  --chunk_size 256 \\\n",
        "  --max_new_tokens 1024 \\\n",
        "  --max_batch_size 4 \\\n",
        "  --cache_lens 32768 \\\n",
        "  --backend_type ktransformers\n",
        "  --no_flash_attn True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "_8d4aAo02xKF",
        "outputId": "f9ed4d22-acc6-43ff-e697-8977b7114c9b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (ipython-input-32-1552536185.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-32-1552536185.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    --no_flash_attn True\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import sched_ext\n",
        "except ImportError:\n",
        "    class Dummy:\n",
        "        class InferenceContext: pass\n",
        "    sched_ext = Dummy\n",
        "\n",
        "!python ktransformers/server/main.py \\\n",
        "  --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "  --gguf_path ./1 \\\n",
        "  --cpu_infer 62 \\\n",
        "  --optimize_config_path ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "  --port 10002 \\\n",
        "  --chunk_size 256 \\\n",
        "  --max_new_tokens 1024 \\\n",
        "  --max_batch_size 4 \\\n",
        "  --cache_lens 32768 \\\n",
        "  --backend_type balance_serve \\\n",
        "  --no_flash_attn True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woSLe-df3K0F",
        "outputId": "43d2ee01-4ae2-4bf7-c1c8-cb72705ff588"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 01:07:40,057 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 01:07:44.441225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750381664.476855   64878 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750381664.487520   64878 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ktransformers/ktransformers/server/main.py\", line 122, in <module>\n",
            "    main()\n",
            "  File \"/content/ktransformers/ktransformers/server/main.py\", line 109, in main\n",
            "    create_interface(config=cfg, default_args=cfg)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/utils/create_interface.py\", line 27, in create_interface\n",
            "    from ktransformers.server.backend.interfaces.balance_serve import BalanceServeInterface as BackendInterface\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/backend/interfaces/balance_serve.py\", line 23, in <module>\n",
            "    from ktransformers.models.custom_modeling_deepseek_v3 import KDeepseekV3ForCausalLM\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/models/custom_modeling_deepseek_v3.py\", line 17, in <module>\n",
            "    from ktransformers.server.balance_serve.inference.forward_batch import ForwardBatchInput, ForwardBatchOutput\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/balance_serve/inference/forward_batch.py\", line 7, in <module>\n",
            "    from ktransformers.server.balance_serve.settings import sched_ext\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/balance_serve/settings.py\", line 11, in <module>\n",
            "    import sched_ext\n",
            "ModuleNotFoundError: No module named 'sched_ext'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ktransformers/server/main.py \\\n",
        "  --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "  --gguf_path ./1 \\\n",
        "  --cpu_infer 62 \\\n",
        "  --optimize_config_path ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "  --port 10002 \\\n",
        "  --chunk_size 256 \\\n",
        "  --max_new_tokens 1024 \\\n",
        "  --max_batch_size 4 \\\n",
        "  --cache_lens 32768 \\\n",
        "  --backend_type ktransformers \\\n",
        "  --no_flash_attn True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apLajvB54Qi-",
        "outputId": "662bee83-abbc-402b-d6de-3b4e8ba3362d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 01:08:48,904 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 01:08:52.780121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750381732.799697   65308 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750381732.805594   65308 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Injecting model as default\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as default\n",
            "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.0.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.0.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as default\n",
            "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.1.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.1.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as default\n",
            "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.2.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.2.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as default\n",
            "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.3.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.3.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as default\n",
            "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.4.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.4.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as default\n",
            "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.5.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.5.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as default\n",
            "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.6.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.6.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as default\n",
            "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.7.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.7.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as default\n",
            "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.8.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.8.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as default\n",
            "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.9.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.9.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as default\n",
            "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.10.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.10.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as default\n",
            "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.11.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.11.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as default\n",
            "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.12.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.12.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as default\n",
            "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.13.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.13.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as default\n",
            "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.14.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.14.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as default\n",
            "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.15.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.15.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as default\n",
            "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.16.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.16.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as default\n",
            "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.17.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.17.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as default\n",
            "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.18.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.18.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as default\n",
            "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.19.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.19.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as default\n",
            "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.20.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.20.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as default\n",
            "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.21.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.21.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as default\n",
            "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.22.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.22.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as default\n",
            "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.23.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.23.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as default\n",
            "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.24.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.24.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as default\n",
            "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.25.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.25.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as default\n",
            "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.26.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.26.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.layers.27 as default\n",
            "Injecting model.layers.27.self_attn as default\n",
            "Injecting model.layers.27.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.27.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.27.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.27.input_layernorm as default\n",
            "Injecting model.layers.27.post_attention_layernorm as default\n",
            "Injecting model.layers.28 as default\n",
            "Injecting model.layers.28.self_attn as default\n",
            "Injecting model.layers.28.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.28.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.28.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.28.input_layernorm as default\n",
            "Injecting model.layers.28.post_attention_layernorm as default\n",
            "Injecting model.layers.29 as default\n",
            "Injecting model.layers.29.self_attn as default\n",
            "Injecting model.layers.29.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.29.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.29.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.29.input_layernorm as default\n",
            "Injecting model.layers.29.post_attention_layernorm as default\n",
            "Injecting model.layers.30 as default\n",
            "Injecting model.layers.30.self_attn as default\n",
            "Injecting model.layers.30.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.30.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.30.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.30.input_layernorm as default\n",
            "Injecting model.layers.30.post_attention_layernorm as default\n",
            "Injecting model.layers.31 as default\n",
            "Injecting model.layers.31.self_attn as default\n",
            "Injecting model.layers.31.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.31.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.31.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.31.input_layernorm as default\n",
            "Injecting model.layers.31.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ktransformers/ktransformers/server/main.py\", line 122, in <module>\n",
            "    main()\n",
            "  File \"/content/ktransformers/ktransformers/server/main.py\", line 109, in main\n",
            "    create_interface(config=cfg, default_args=cfg)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/utils/create_interface.py\", line 30, in create_interface\n",
            "    GlobalInterface.interface = BackendInterface(default_args)\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/backend/interfaces/ktransformers.py\", line 63, in __init__\n",
            "    optimize_and_load_gguf(self.model, optimize_config_path, gguf_path, config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/optimize/optimize.py\", line 130, in optimize_and_load_gguf\n",
            "    load_weights(module.lm_head, weights_loader, \"lm_head.\", device=default_device)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/linear.py\", line 937, in load\n",
            "    self.generate_linear.load(w=w)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/linear.py\", line 626, in load\n",
            "    weight = w.view(self.orin_out_features, self.orin_in_features).T\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: shape '[32000, 4096]' is invalid for input of size 196608000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ktransformers/server/main.py \\\n",
        "  --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "  --gguf_path ./1 \\\n",
        "  --cpu_infer 2 \\\n",
        "  --optimize_config_path ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "  --port 10002 \\\n",
        "  --chunk_size 256 \\\n",
        "  --max_new_tokens 10 \\\n",
        "  --max_batch_size 1 \\\n",
        "  --cache_lens 128 \\\n",
        "  --backend_type ktransformers \\\n",
        "  --no_flash_attn True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIDlNntr48K4",
        "outputId": "c76ac6a1-17ef-4b35-dfd2-179499e3de10"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 01:11:35,225 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 01:11:39.320147: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750381899.339875   66381 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750381899.345880   66381 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Injecting model as default\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as default\n",
            "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.0.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.0.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as default\n",
            "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.1.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.1.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as default\n",
            "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.2.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.2.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as default\n",
            "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.3.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.3.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as default\n",
            "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.4.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.4.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as default\n",
            "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.5.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.5.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as default\n",
            "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.6.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.6.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as default\n",
            "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.7.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.7.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as default\n",
            "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.8.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.8.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as default\n",
            "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.9.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.9.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as default\n",
            "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.10.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.10.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as default\n",
            "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.11.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.11.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as default\n",
            "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.12.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.12.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as default\n",
            "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.13.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.13.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as default\n",
            "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.14.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.14.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as default\n",
            "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.15.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.15.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as default\n",
            "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.16.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.16.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as default\n",
            "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.17.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.17.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as default\n",
            "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.18.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.18.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as default\n",
            "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.19.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.19.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as default\n",
            "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.20.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.20.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as default\n",
            "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.21.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.21.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as default\n",
            "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.22.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.22.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as default\n",
            "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.23.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.23.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as default\n",
            "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.24.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.24.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as default\n",
            "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.25.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.25.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as default\n",
            "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.26.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.26.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.layers.27 as default\n",
            "Injecting model.layers.27.self_attn as default\n",
            "Injecting model.layers.27.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.27.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.27.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.27.input_layernorm as default\n",
            "Injecting model.layers.27.post_attention_layernorm as default\n",
            "Injecting model.layers.28 as default\n",
            "Injecting model.layers.28.self_attn as default\n",
            "Injecting model.layers.28.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.28.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.28.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.28.input_layernorm as default\n",
            "Injecting model.layers.28.post_attention_layernorm as default\n",
            "Injecting model.layers.29 as default\n",
            "Injecting model.layers.29.self_attn as default\n",
            "Injecting model.layers.29.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.29.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.29.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.29.input_layernorm as default\n",
            "Injecting model.layers.29.post_attention_layernorm as default\n",
            "Injecting model.layers.30 as default\n",
            "Injecting model.layers.30.self_attn as default\n",
            "Injecting model.layers.30.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.30.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.30.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.30.input_layernorm as default\n",
            "Injecting model.layers.30.post_attention_layernorm as default\n",
            "Injecting model.layers.31 as default\n",
            "Injecting model.layers.31.self_attn as default\n",
            "Injecting model.layers.31.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.31.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.31.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.31.input_layernorm as default\n",
            "Injecting model.layers.31.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ktransformers/ktransformers/server/main.py\", line 122, in <module>\n",
            "    main()\n",
            "  File \"/content/ktransformers/ktransformers/server/main.py\", line 109, in main\n",
            "    create_interface(config=cfg, default_args=cfg)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/utils/create_interface.py\", line 30, in create_interface\n",
            "    GlobalInterface.interface = BackendInterface(default_args)\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/backend/interfaces/ktransformers.py\", line 63, in __init__\n",
            "    optimize_and_load_gguf(self.model, optimize_config_path, gguf_path, config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/optimize/optimize.py\", line 130, in optimize_and_load_gguf\n",
            "    load_weights(module.lm_head, weights_loader, \"lm_head.\", device=default_device)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/linear.py\", line 937, in load\n",
            "    self.generate_linear.load(w=w)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/linear.py\", line 626, in load\n",
            "    weight = w.view(self.orin_out_features, self.orin_in_features).T\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: shape '[32000, 4096]' is invalid for input of size 196608000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!python ktransformers/server/main.py \\\n",
        "  --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "  --gguf_path /content/ktransformers/1/mixtral-8x7b-instruct-v0.1.Q2_K.gguf \\\n",
        "  --cpu_infer 62 \\\n",
        "  --optimize_config_path ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "  --port 10002 \\\n",
        "  --chunk_size 256 \\\n",
        "  --max_new_tokens 1024 \\\n",
        "  --max_batch_size 4 \\\n",
        "  --cache_lens 32768 \\\n",
        "  --backend_type ktransformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8iccAoC5Pjp",
        "outputId": "139f6d9f-df34-4306-846b-b8df4f86b5c7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['no balance_serve',\n",
              " '2025-06-20 01:13:46,217 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend',\n",
              " 'found flashinfer',\n",
              " '2025-06-20 01:13:50.013009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered',\n",
              " 'WARNING: All log messages before absl::InitializeLog() is called are written to STDERR',\n",
              " 'E0000 00:00:1750382030.032699   67263 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered',\n",
              " 'E0000 00:00:1750382030.038838   67263 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered',\n",
              " \"MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\",\n",
              " \"  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\",\n",
              " \"  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\",\n",
              " '  - If you are not the owner of the model architecture class, please contact the model code owner to update it.',\n",
              " 'Injecting model as default',\n",
              " 'Injecting model.embed_tokens as default',\n",
              " 'Injecting model.layers as default',\n",
              " 'Injecting model.layers.0 as default',\n",
              " 'Injecting model.layers.0.self_attn as default',\n",
              " 'Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.0.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.0.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.0.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.0.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.0.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.0.input_layernorm as default',\n",
              " 'Injecting model.layers.0.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.1 as default',\n",
              " 'Injecting model.layers.1.self_attn as default',\n",
              " 'Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.1.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.1.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.1.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.1.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.1.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.1.input_layernorm as default',\n",
              " 'Injecting model.layers.1.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.2 as default',\n",
              " 'Injecting model.layers.2.self_attn as default',\n",
              " 'Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.2.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.2.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.2.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.2.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.2.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.2.input_layernorm as default',\n",
              " 'Injecting model.layers.2.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.3 as default',\n",
              " 'Injecting model.layers.3.self_attn as default',\n",
              " 'Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.3.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.3.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.3.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.3.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.3.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.3.input_layernorm as default',\n",
              " 'Injecting model.layers.3.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.4 as default',\n",
              " 'Injecting model.layers.4.self_attn as default',\n",
              " 'Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.4.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.4.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.4.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.4.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.4.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.4.input_layernorm as default',\n",
              " 'Injecting model.layers.4.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.5 as default',\n",
              " 'Injecting model.layers.5.self_attn as default',\n",
              " 'Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.5.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.5.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.5.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.5.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.5.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.5.input_layernorm as default',\n",
              " 'Injecting model.layers.5.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.6 as default',\n",
              " 'Injecting model.layers.6.self_attn as default',\n",
              " 'Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.6.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.6.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.6.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.6.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.6.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.6.input_layernorm as default',\n",
              " 'Injecting model.layers.6.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.7 as default',\n",
              " 'Injecting model.layers.7.self_attn as default',\n",
              " 'Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.7.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.7.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.7.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.7.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.7.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.7.input_layernorm as default',\n",
              " 'Injecting model.layers.7.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.8 as default',\n",
              " 'Injecting model.layers.8.self_attn as default',\n",
              " 'Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.8.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.8.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.8.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.8.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.8.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.8.input_layernorm as default',\n",
              " 'Injecting model.layers.8.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.9 as default',\n",
              " 'Injecting model.layers.9.self_attn as default',\n",
              " 'Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.9.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.9.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.9.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.9.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.9.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.9.input_layernorm as default',\n",
              " 'Injecting model.layers.9.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.10 as default',\n",
              " 'Injecting model.layers.10.self_attn as default',\n",
              " 'Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.10.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.10.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.10.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.10.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.10.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.10.input_layernorm as default',\n",
              " 'Injecting model.layers.10.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.11 as default',\n",
              " 'Injecting model.layers.11.self_attn as default',\n",
              " 'Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.11.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.11.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.11.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.11.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.11.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.11.input_layernorm as default',\n",
              " 'Injecting model.layers.11.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.12 as default',\n",
              " 'Injecting model.layers.12.self_attn as default',\n",
              " 'Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.12.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.12.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.12.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.12.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.12.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.12.input_layernorm as default',\n",
              " 'Injecting model.layers.12.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.13 as default',\n",
              " 'Injecting model.layers.13.self_attn as default',\n",
              " 'Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.13.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.13.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.13.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.13.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.13.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.13.input_layernorm as default',\n",
              " 'Injecting model.layers.13.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.14 as default',\n",
              " 'Injecting model.layers.14.self_attn as default',\n",
              " 'Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.14.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.14.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.14.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.14.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.14.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.14.input_layernorm as default',\n",
              " 'Injecting model.layers.14.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.15 as default',\n",
              " 'Injecting model.layers.15.self_attn as default',\n",
              " 'Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.15.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.15.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.15.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.15.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.15.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.15.input_layernorm as default',\n",
              " 'Injecting model.layers.15.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.16 as default',\n",
              " 'Injecting model.layers.16.self_attn as default',\n",
              " 'Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.16.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.16.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.16.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.16.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.16.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.16.input_layernorm as default',\n",
              " 'Injecting model.layers.16.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.17 as default',\n",
              " 'Injecting model.layers.17.self_attn as default',\n",
              " 'Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.17.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.17.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.17.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.17.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.17.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.17.input_layernorm as default',\n",
              " 'Injecting model.layers.17.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.18 as default',\n",
              " 'Injecting model.layers.18.self_attn as default',\n",
              " 'Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.18.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.18.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.18.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.18.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.18.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.18.input_layernorm as default',\n",
              " 'Injecting model.layers.18.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.19 as default',\n",
              " 'Injecting model.layers.19.self_attn as default',\n",
              " 'Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.19.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.19.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.19.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.19.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.19.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.19.input_layernorm as default',\n",
              " 'Injecting model.layers.19.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.20 as default',\n",
              " 'Injecting model.layers.20.self_attn as default',\n",
              " 'Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.20.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.20.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.20.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.20.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.20.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.20.input_layernorm as default',\n",
              " 'Injecting model.layers.20.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.21 as default',\n",
              " 'Injecting model.layers.21.self_attn as default',\n",
              " 'Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.21.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.21.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.21.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.21.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.21.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.21.input_layernorm as default',\n",
              " 'Injecting model.layers.21.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.22 as default',\n",
              " 'Injecting model.layers.22.self_attn as default',\n",
              " 'Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.22.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.22.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.22.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.22.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.22.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.22.input_layernorm as default',\n",
              " 'Injecting model.layers.22.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.23 as default',\n",
              " 'Injecting model.layers.23.self_attn as default',\n",
              " 'Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.23.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.23.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.23.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.23.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.23.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.23.input_layernorm as default',\n",
              " 'Injecting model.layers.23.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.24 as default',\n",
              " 'Injecting model.layers.24.self_attn as default',\n",
              " 'Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.24.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.24.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.24.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.24.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.24.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.24.input_layernorm as default',\n",
              " 'Injecting model.layers.24.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.25 as default',\n",
              " 'Injecting model.layers.25.self_attn as default',\n",
              " 'Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.25.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.25.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.25.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.25.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.25.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.25.input_layernorm as default',\n",
              " 'Injecting model.layers.25.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.26 as default',\n",
              " 'Injecting model.layers.26.self_attn as default',\n",
              " 'Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.26.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.26.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.26.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.26.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.26.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.26.input_layernorm as default',\n",
              " 'Injecting model.layers.26.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.27 as default',\n",
              " 'Injecting model.layers.27.self_attn as default',\n",
              " 'Injecting model.layers.27.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.27.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.27.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.27.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.27.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.27.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.27.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.27.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.27.input_layernorm as default',\n",
              " 'Injecting model.layers.27.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.28 as default',\n",
              " 'Injecting model.layers.28.self_attn as default',\n",
              " 'Injecting model.layers.28.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.28.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.28.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.28.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.28.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.28.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.28.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.28.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.28.input_layernorm as default',\n",
              " 'Injecting model.layers.28.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.29 as default',\n",
              " 'Injecting model.layers.29.self_attn as default',\n",
              " 'Injecting model.layers.29.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.29.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.29.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.29.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.29.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.29.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.29.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.29.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.29.input_layernorm as default',\n",
              " 'Injecting model.layers.29.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.30 as default',\n",
              " 'Injecting model.layers.30.self_attn as default',\n",
              " 'Injecting model.layers.30.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.30.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.30.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.30.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.30.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.30.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.30.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.30.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.30.input_layernorm as default',\n",
              " 'Injecting model.layers.30.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.31 as default',\n",
              " 'Injecting model.layers.31.self_attn as default',\n",
              " 'Injecting model.layers.31.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.31.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.31.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.31.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.31.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.31.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.31.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.31.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.31.input_layernorm as default',\n",
              " 'Injecting model.layers.31.post_attention_layernorm as default',\n",
              " 'Injecting model.norm as default',\n",
              " 'Injecting lm_head as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Traceback (most recent call last):',\n",
              " '  File \"/content/ktransformers/ktransformers/server/main.py\", line 122, in <module>',\n",
              " '    main()',\n",
              " '  File \"/content/ktransformers/ktransformers/server/main.py\", line 109, in main',\n",
              " '    create_interface(config=cfg, default_args=cfg)',\n",
              " '  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/utils/create_interface.py\", line 30, in create_interface',\n",
              " '    GlobalInterface.interface = BackendInterface(default_args)',\n",
              " '                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^',\n",
              " '  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/backend/interfaces/ktransformers.py\", line 63, in __init__',\n",
              " '    optimize_and_load_gguf(self.model, optimize_config_path, gguf_path, config)',\n",
              " '  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/optimize/optimize.py\", line 130, in optimize_and_load_gguf',\n",
              " '    load_weights(module.lm_head, weights_loader, \"lm_head.\", device=default_device)',\n",
              " '  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 176, in load_weights',\n",
              " '    module.load()',\n",
              " '  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/linear.py\", line 937, in load',\n",
              " '    self.generate_linear.load(w=w)',\n",
              " '  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/linear.py\", line 626, in load',\n",
              " '    weight = w.view(self.orin_out_features, self.orin_in_features).T',\n",
              " '             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^',\n",
              " \"RuntimeError: shape '[32000, 4096]' is invalid for input of size 196608000\"]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/ktransformers/mixtral-8x7b-instruct-v0.1.Q2_K.gguf"
      ],
      "metadata": {
        "id": "uLEoWF7J5XRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python ktransformers/server/main.py \\\n",
        "  --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "  --gguf_path /content/ktransformers/mixtral-8x7b-instruct-v0.1.Q2_K.gguf \\\n",
        "  --cpu_infer 62 \\\n",
        "  --optimize_config_path ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "  --port 10002 \\\n",
        "  --chunk_size 256 \\\n",
        "  --max_new_tokens 1024 \\\n",
        "  --max_batch_size 4 \\\n",
        "  --cache_lens 32768 \\\n",
        "  --backend_type ktransformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jO2d0t76EL3",
        "outputId": "55c2d13b-9b9c-45d4-9362-bd3d18436e8f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no balance_serve\n",
            "2025-06-20 01:16:21,169 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "found flashinfer\n",
            "2025-06-20 01:16:25.716425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750382185.737293   68271 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750382185.743427   68271 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "/usr/local/lib/python3.11/dist-packages/ktransformers/util/custom_loader.py:297: UserWarning: Version 2 has never been tested, might not work\n",
            "  warnings.warn(f\"Version {version} has never been tested, might not work\")\n",
            "Injecting model as default\n",
            "Injecting model.embed_tokens as default\n",
            "Injecting model.layers as default\n",
            "Injecting model.layers.0 as default\n",
            "Injecting model.layers.0.self_attn as default\n",
            "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.0.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.0.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.0.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.0.input_layernorm as default\n",
            "Injecting model.layers.0.post_attention_layernorm as default\n",
            "Injecting model.layers.1 as default\n",
            "Injecting model.layers.1.self_attn as default\n",
            "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.1.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.1.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.1.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.1.input_layernorm as default\n",
            "Injecting model.layers.1.post_attention_layernorm as default\n",
            "Injecting model.layers.2 as default\n",
            "Injecting model.layers.2.self_attn as default\n",
            "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.2.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.2.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.2.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.2.input_layernorm as default\n",
            "Injecting model.layers.2.post_attention_layernorm as default\n",
            "Injecting model.layers.3 as default\n",
            "Injecting model.layers.3.self_attn as default\n",
            "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.3.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.3.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.3.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.3.input_layernorm as default\n",
            "Injecting model.layers.3.post_attention_layernorm as default\n",
            "Injecting model.layers.4 as default\n",
            "Injecting model.layers.4.self_attn as default\n",
            "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.4.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.4.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.4.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.4.input_layernorm as default\n",
            "Injecting model.layers.4.post_attention_layernorm as default\n",
            "Injecting model.layers.5 as default\n",
            "Injecting model.layers.5.self_attn as default\n",
            "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.5.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.5.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.5.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.5.input_layernorm as default\n",
            "Injecting model.layers.5.post_attention_layernorm as default\n",
            "Injecting model.layers.6 as default\n",
            "Injecting model.layers.6.self_attn as default\n",
            "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.6.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.6.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.6.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.6.input_layernorm as default\n",
            "Injecting model.layers.6.post_attention_layernorm as default\n",
            "Injecting model.layers.7 as default\n",
            "Injecting model.layers.7.self_attn as default\n",
            "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.7.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.7.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.7.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.7.input_layernorm as default\n",
            "Injecting model.layers.7.post_attention_layernorm as default\n",
            "Injecting model.layers.8 as default\n",
            "Injecting model.layers.8.self_attn as default\n",
            "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.8.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.8.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.8.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.8.input_layernorm as default\n",
            "Injecting model.layers.8.post_attention_layernorm as default\n",
            "Injecting model.layers.9 as default\n",
            "Injecting model.layers.9.self_attn as default\n",
            "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.9.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.9.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.9.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.9.input_layernorm as default\n",
            "Injecting model.layers.9.post_attention_layernorm as default\n",
            "Injecting model.layers.10 as default\n",
            "Injecting model.layers.10.self_attn as default\n",
            "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.10.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.10.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.10.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.10.input_layernorm as default\n",
            "Injecting model.layers.10.post_attention_layernorm as default\n",
            "Injecting model.layers.11 as default\n",
            "Injecting model.layers.11.self_attn as default\n",
            "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.11.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.11.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.11.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.11.input_layernorm as default\n",
            "Injecting model.layers.11.post_attention_layernorm as default\n",
            "Injecting model.layers.12 as default\n",
            "Injecting model.layers.12.self_attn as default\n",
            "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.12.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.12.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.12.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.12.input_layernorm as default\n",
            "Injecting model.layers.12.post_attention_layernorm as default\n",
            "Injecting model.layers.13 as default\n",
            "Injecting model.layers.13.self_attn as default\n",
            "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.13.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.13.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.13.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.13.input_layernorm as default\n",
            "Injecting model.layers.13.post_attention_layernorm as default\n",
            "Injecting model.layers.14 as default\n",
            "Injecting model.layers.14.self_attn as default\n",
            "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.14.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.14.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.14.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.14.input_layernorm as default\n",
            "Injecting model.layers.14.post_attention_layernorm as default\n",
            "Injecting model.layers.15 as default\n",
            "Injecting model.layers.15.self_attn as default\n",
            "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.15.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.15.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.15.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.15.input_layernorm as default\n",
            "Injecting model.layers.15.post_attention_layernorm as default\n",
            "Injecting model.layers.16 as default\n",
            "Injecting model.layers.16.self_attn as default\n",
            "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.16.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.16.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.16.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.16.input_layernorm as default\n",
            "Injecting model.layers.16.post_attention_layernorm as default\n",
            "Injecting model.layers.17 as default\n",
            "Injecting model.layers.17.self_attn as default\n",
            "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.17.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.17.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.17.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.17.input_layernorm as default\n",
            "Injecting model.layers.17.post_attention_layernorm as default\n",
            "Injecting model.layers.18 as default\n",
            "Injecting model.layers.18.self_attn as default\n",
            "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.18.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.18.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.18.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.18.input_layernorm as default\n",
            "Injecting model.layers.18.post_attention_layernorm as default\n",
            "Injecting model.layers.19 as default\n",
            "Injecting model.layers.19.self_attn as default\n",
            "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.19.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.19.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.19.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.19.input_layernorm as default\n",
            "Injecting model.layers.19.post_attention_layernorm as default\n",
            "Injecting model.layers.20 as default\n",
            "Injecting model.layers.20.self_attn as default\n",
            "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.20.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.20.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.20.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.20.input_layernorm as default\n",
            "Injecting model.layers.20.post_attention_layernorm as default\n",
            "Injecting model.layers.21 as default\n",
            "Injecting model.layers.21.self_attn as default\n",
            "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.21.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.21.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.21.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.21.input_layernorm as default\n",
            "Injecting model.layers.21.post_attention_layernorm as default\n",
            "Injecting model.layers.22 as default\n",
            "Injecting model.layers.22.self_attn as default\n",
            "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.22.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.22.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.22.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.22.input_layernorm as default\n",
            "Injecting model.layers.22.post_attention_layernorm as default\n",
            "Injecting model.layers.23 as default\n",
            "Injecting model.layers.23.self_attn as default\n",
            "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.23.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.23.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.23.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.23.input_layernorm as default\n",
            "Injecting model.layers.23.post_attention_layernorm as default\n",
            "Injecting model.layers.24 as default\n",
            "Injecting model.layers.24.self_attn as default\n",
            "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.24.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.24.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.24.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.24.input_layernorm as default\n",
            "Injecting model.layers.24.post_attention_layernorm as default\n",
            "Injecting model.layers.25 as default\n",
            "Injecting model.layers.25.self_attn as default\n",
            "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.25.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.25.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.25.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.25.input_layernorm as default\n",
            "Injecting model.layers.25.post_attention_layernorm as default\n",
            "Injecting model.layers.26 as default\n",
            "Injecting model.layers.26.self_attn as default\n",
            "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.26.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.26.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.26.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.26.input_layernorm as default\n",
            "Injecting model.layers.26.post_attention_layernorm as default\n",
            "Injecting model.layers.27 as default\n",
            "Injecting model.layers.27.self_attn as default\n",
            "Injecting model.layers.27.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.27.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.27.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.27.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.27.input_layernorm as default\n",
            "Injecting model.layers.27.post_attention_layernorm as default\n",
            "Injecting model.layers.28 as default\n",
            "Injecting model.layers.28.self_attn as default\n",
            "Injecting model.layers.28.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.28.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.28.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.28.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.28.input_layernorm as default\n",
            "Injecting model.layers.28.post_attention_layernorm as default\n",
            "Injecting model.layers.29 as default\n",
            "Injecting model.layers.29.self_attn as default\n",
            "Injecting model.layers.29.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.29.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.29.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.29.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.29.input_layernorm as default\n",
            "Injecting model.layers.29.post_attention_layernorm as default\n",
            "Injecting model.layers.30 as default\n",
            "Injecting model.layers.30.self_attn as default\n",
            "Injecting model.layers.30.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.30.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.30.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.30.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.30.input_layernorm as default\n",
            "Injecting model.layers.30.post_attention_layernorm as default\n",
            "Injecting model.layers.31 as default\n",
            "Injecting model.layers.31.self_attn as default\n",
            "Injecting model.layers.31.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding\n",
            "Injecting model.layers.31.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock\n",
            "Injecting model.layers.31.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear\n",
            "Injecting model.layers.31.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts\n",
            "Injecting model.layers.31.input_layernorm as default\n",
            "Injecting model.layers.31.post_attention_layernorm as default\n",
            "Injecting model.norm as default\n",
            "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ktransformers/ktransformers/server/main.py\", line 122, in <module>\n",
            "    main()\n",
            "  File \"/content/ktransformers/ktransformers/server/main.py\", line 109, in main\n",
            "    create_interface(config=cfg, default_args=cfg)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/utils/create_interface.py\", line 30, in create_interface\n",
            "    GlobalInterface.interface = BackendInterface(default_args)\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/backend/interfaces/ktransformers.py\", line 63, in __init__\n",
            "    optimize_and_load_gguf(self.model, optimize_config_path, gguf_path, config)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/optimize/optimize.py\", line 130, in optimize_and_load_gguf\n",
            "    load_weights(module.lm_head, weights_loader, \"lm_head.\", device=default_device)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 176, in load_weights\n",
            "    module.load()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/linear.py\", line 937, in load\n",
            "    self.generate_linear.load(w=w)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/linear.py\", line 626, in load\n",
            "    weight = w.view(self.orin_out_features, self.orin_in_features).T\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: shape '[32000, 4096]' is invalid for input of size 196608000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!python ktransformers/server/main.py \\\n",
        "  --model_path mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "  --gguf_path /content/ktransformers/1 \\\n",
        "  --cpu_infer 62 \\\n",
        "  --optimize_config_path ktransformers/optimize/optimize_rules/Mixtral.yaml \\\n",
        "  --port 10002 \\\n",
        "  --chunk_size 256 \\\n",
        "  --max_new_tokens 1024 \\\n",
        "  --max_batch_size 4 \\\n",
        "  --cache_lens 32768 \\\n",
        "  --backend_type ktransformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O7QBxb66Ssu",
        "outputId": "7412c604-d398-41d9-d6d6-711fd0559973"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['no balance_serve',\n",
              " '2025-06-20 01:17:22,386 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend',\n",
              " 'found flashinfer',\n",
              " '2025-06-20 01:17:26.275625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered',\n",
              " 'WARNING: All log messages before absl::InitializeLog() is called are written to STDERR',\n",
              " 'E0000 00:00:1750382246.295577   68707 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered',\n",
              " 'E0000 00:00:1750382246.301570   68707 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered',\n",
              " \"MixtralForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\",\n",
              " \"  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\",\n",
              " \"  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\",\n",
              " '  - If you are not the owner of the model architecture class, please contact the model code owner to update it.',\n",
              " 'Injecting model as default',\n",
              " 'Injecting model.embed_tokens as default',\n",
              " 'Injecting model.layers as default',\n",
              " 'Injecting model.layers.0 as default',\n",
              " 'Injecting model.layers.0.self_attn as default',\n",
              " 'Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.0.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.0.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.0.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.0.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.0.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.0.input_layernorm as default',\n",
              " 'Injecting model.layers.0.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.1 as default',\n",
              " 'Injecting model.layers.1.self_attn as default',\n",
              " 'Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.1.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.1.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.1.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.1.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.1.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.1.input_layernorm as default',\n",
              " 'Injecting model.layers.1.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.2 as default',\n",
              " 'Injecting model.layers.2.self_attn as default',\n",
              " 'Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.2.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.2.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.2.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.2.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.2.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.2.input_layernorm as default',\n",
              " 'Injecting model.layers.2.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.3 as default',\n",
              " 'Injecting model.layers.3.self_attn as default',\n",
              " 'Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.3.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.3.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.3.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.3.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.3.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.3.input_layernorm as default',\n",
              " 'Injecting model.layers.3.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.4 as default',\n",
              " 'Injecting model.layers.4.self_attn as default',\n",
              " 'Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.4.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.4.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.4.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.4.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.4.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.4.input_layernorm as default',\n",
              " 'Injecting model.layers.4.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.5 as default',\n",
              " 'Injecting model.layers.5.self_attn as default',\n",
              " 'Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.5.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.5.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.5.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.5.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.5.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.5.input_layernorm as default',\n",
              " 'Injecting model.layers.5.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.6 as default',\n",
              " 'Injecting model.layers.6.self_attn as default',\n",
              " 'Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.6.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.6.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.6.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.6.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.6.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.6.input_layernorm as default',\n",
              " 'Injecting model.layers.6.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.7 as default',\n",
              " 'Injecting model.layers.7.self_attn as default',\n",
              " 'Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.7.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.7.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.7.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.7.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.7.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.7.input_layernorm as default',\n",
              " 'Injecting model.layers.7.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.8 as default',\n",
              " 'Injecting model.layers.8.self_attn as default',\n",
              " 'Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.8.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.8.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.8.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.8.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.8.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.8.input_layernorm as default',\n",
              " 'Injecting model.layers.8.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.9 as default',\n",
              " 'Injecting model.layers.9.self_attn as default',\n",
              " 'Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.9.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.9.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.9.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.9.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.9.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.9.input_layernorm as default',\n",
              " 'Injecting model.layers.9.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.10 as default',\n",
              " 'Injecting model.layers.10.self_attn as default',\n",
              " 'Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.10.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.10.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.10.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.10.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.10.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.10.input_layernorm as default',\n",
              " 'Injecting model.layers.10.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.11 as default',\n",
              " 'Injecting model.layers.11.self_attn as default',\n",
              " 'Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.11.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.11.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.11.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.11.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.11.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.11.input_layernorm as default',\n",
              " 'Injecting model.layers.11.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.12 as default',\n",
              " 'Injecting model.layers.12.self_attn as default',\n",
              " 'Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.12.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.12.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.12.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.12.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.12.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.12.input_layernorm as default',\n",
              " 'Injecting model.layers.12.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.13 as default',\n",
              " 'Injecting model.layers.13.self_attn as default',\n",
              " 'Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.13.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.13.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.13.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.13.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.13.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.13.input_layernorm as default',\n",
              " 'Injecting model.layers.13.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.14 as default',\n",
              " 'Injecting model.layers.14.self_attn as default',\n",
              " 'Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.14.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.14.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.14.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.14.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.14.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.14.input_layernorm as default',\n",
              " 'Injecting model.layers.14.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.15 as default',\n",
              " 'Injecting model.layers.15.self_attn as default',\n",
              " 'Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.15.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.15.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.15.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.15.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.15.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.15.input_layernorm as default',\n",
              " 'Injecting model.layers.15.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.16 as default',\n",
              " 'Injecting model.layers.16.self_attn as default',\n",
              " 'Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.16.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.16.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.16.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.16.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.16.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.16.input_layernorm as default',\n",
              " 'Injecting model.layers.16.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.17 as default',\n",
              " 'Injecting model.layers.17.self_attn as default',\n",
              " 'Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.17.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.17.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.17.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.17.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.17.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.17.input_layernorm as default',\n",
              " 'Injecting model.layers.17.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.18 as default',\n",
              " 'Injecting model.layers.18.self_attn as default',\n",
              " 'Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.18.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.18.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.18.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.18.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.18.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.18.input_layernorm as default',\n",
              " 'Injecting model.layers.18.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.19 as default',\n",
              " 'Injecting model.layers.19.self_attn as default',\n",
              " 'Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.19.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.19.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.19.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.19.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.19.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.19.input_layernorm as default',\n",
              " 'Injecting model.layers.19.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.20 as default',\n",
              " 'Injecting model.layers.20.self_attn as default',\n",
              " 'Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.20.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.20.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.20.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.20.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.20.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.20.input_layernorm as default',\n",
              " 'Injecting model.layers.20.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.21 as default',\n",
              " 'Injecting model.layers.21.self_attn as default',\n",
              " 'Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.21.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.21.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.21.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.21.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.21.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.21.input_layernorm as default',\n",
              " 'Injecting model.layers.21.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.22 as default',\n",
              " 'Injecting model.layers.22.self_attn as default',\n",
              " 'Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.22.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.22.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.22.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.22.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.22.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.22.input_layernorm as default',\n",
              " 'Injecting model.layers.22.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.23 as default',\n",
              " 'Injecting model.layers.23.self_attn as default',\n",
              " 'Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.23.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.23.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.23.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.23.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.23.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.23.input_layernorm as default',\n",
              " 'Injecting model.layers.23.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.24 as default',\n",
              " 'Injecting model.layers.24.self_attn as default',\n",
              " 'Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.24.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.24.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.24.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.24.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.24.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.24.input_layernorm as default',\n",
              " 'Injecting model.layers.24.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.25 as default',\n",
              " 'Injecting model.layers.25.self_attn as default',\n",
              " 'Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.25.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.25.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.25.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.25.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.25.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.25.input_layernorm as default',\n",
              " 'Injecting model.layers.25.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.26 as default',\n",
              " 'Injecting model.layers.26.self_attn as default',\n",
              " 'Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.26.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.26.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.26.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.26.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.26.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.26.input_layernorm as default',\n",
              " 'Injecting model.layers.26.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.27 as default',\n",
              " 'Injecting model.layers.27.self_attn as default',\n",
              " 'Injecting model.layers.27.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.27.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.27.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.27.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.27.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.27.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.27.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.27.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.27.input_layernorm as default',\n",
              " 'Injecting model.layers.27.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.28 as default',\n",
              " 'Injecting model.layers.28.self_attn as default',\n",
              " 'Injecting model.layers.28.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.28.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.28.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.28.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.28.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.28.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.28.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.28.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.28.input_layernorm as default',\n",
              " 'Injecting model.layers.28.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.29 as default',\n",
              " 'Injecting model.layers.29.self_attn as default',\n",
              " 'Injecting model.layers.29.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.29.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.29.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.29.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.29.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.29.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.29.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.29.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.29.input_layernorm as default',\n",
              " 'Injecting model.layers.29.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.30 as default',\n",
              " 'Injecting model.layers.30.self_attn as default',\n",
              " 'Injecting model.layers.30.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.30.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.30.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.30.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.30.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.30.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.30.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.30.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.30.input_layernorm as default',\n",
              " 'Injecting model.layers.30.post_attention_layernorm as default',\n",
              " 'Injecting model.layers.31 as default',\n",
              " 'Injecting model.layers.31.self_attn as default',\n",
              " 'Injecting model.layers.31.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.31.self_attn.k_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.31.self_attn.v_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.31.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.31.self_attn.rotary_emb as ktransformers.operators.RoPE . RotaryEmbedding',\n",
              " 'Injecting model.layers.31.block_sparse_moe as ktransformers.operators.experts . KMistralSparseMoEBlock',\n",
              " 'Injecting model.layers.31.block_sparse_moe.gate as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Injecting model.layers.31.block_sparse_moe.experts as ktransformers.operators.experts . KTransformersExperts',\n",
              " 'Injecting model.layers.31.input_layernorm as default',\n",
              " 'Injecting model.layers.31.post_attention_layernorm as default',\n",
              " 'Injecting model.norm as default',\n",
              " 'Injecting lm_head as ktransformers.operators.linear . KTransformersLinear',\n",
              " 'Traceback (most recent call last):',\n",
              " '  File \"/content/ktransformers/ktransformers/server/main.py\", line 122, in <module>',\n",
              " '    main()',\n",
              " '  File \"/content/ktransformers/ktransformers/server/main.py\", line 109, in main',\n",
              " '    create_interface(config=cfg, default_args=cfg)',\n",
              " '  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/utils/create_interface.py\", line 30, in create_interface',\n",
              " '    GlobalInterface.interface = BackendInterface(default_args)',\n",
              " '                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^',\n",
              " '  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/server/backend/interfaces/ktransformers.py\", line 63, in __init__',\n",
              " '    optimize_and_load_gguf(self.model, optimize_config_path, gguf_path, config)',\n",
              " '  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/optimize/optimize.py\", line 130, in optimize_and_load_gguf',\n",
              " '    load_weights(module.lm_head, weights_loader, \"lm_head.\", device=default_device)',\n",
              " '  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/util/utils.py\", line 176, in load_weights',\n",
              " '    module.load()',\n",
              " '  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/linear.py\", line 937, in load',\n",
              " '    self.generate_linear.load(w=w)',\n",
              " '  File \"/usr/local/lib/python3.11/dist-packages/ktransformers/operators/linear.py\", line 626, in load',\n",
              " '    weight = w.view(self.orin_out_features, self.orin_in_features).T',\n",
              " '             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^',\n",
              " \"RuntimeError: shape '[32000, 4096]' is invalid for input of size 196608000\"]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}